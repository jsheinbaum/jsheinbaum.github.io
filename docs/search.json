[
  {
    "objectID": "basics_jupyter.html",
    "href": "basics_jupyter.html",
    "title": "Matplotlib Demo",
    "section": "",
    "text": "For a demonstration of a line plot on a polar axis, see Figure 1.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\ntheta = np.linspace(0, 2*np.pi, 1000)\nr = 3* np.sin(18 * theta)\nr1 = 0.5 - 0.5 * np.sin(10*theta)\nr2=0.5+0.5*np.cos(18*theta)\n#plt.polar(theta, r, 'r')\n#r = np.arange(0, 2, 0.01)\n#theta = 2 * np.pi * r\nfig, ax = plt.subplots(subplot_kw={'projection': 'polar'})\nax.plot(theta, r,'b')\nax.plot(theta, r1,'r')\nax.plot(theta, r2,'g')\nax.set_rticks([-1.0,-0.5,0.0,0.5, 1, 1.5, 2, 2.5, 3.0])\nax.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nFigure 1: A line plot on a polar axis"
  },
  {
    "objectID": "basics_jupyter.html#polar-axis",
    "href": "basics_jupyter.html#polar-axis",
    "title": "Matplotlib Demo",
    "section": "",
    "text": "For a demonstration of a line plot on a polar axis, see Figure 1.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\ntheta = np.linspace(0, 2*np.pi, 1000)\nr = 3* np.sin(18 * theta)\nr1 = 0.5 - 0.5 * np.sin(10*theta)\nr2=0.5+0.5*np.cos(18*theta)\n#plt.polar(theta, r, 'r')\n#r = np.arange(0, 2, 0.01)\n#theta = 2 * np.pi * r\nfig, ax = plt.subplots(subplot_kw={'projection': 'polar'})\nax.plot(theta, r,'b')\nax.plot(theta, r1,'r')\nax.plot(theta, r2,'g')\nax.set_rticks([-1.0,-0.5,0.0,0.5, 1, 1.5, 2, 2.5, 3.0])\nax.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nFigure 1: A line plot on a polar axis"
  },
  {
    "objectID": "basics_jupyter.html#widgets",
    "href": "basics_jupyter.html#widgets",
    "title": "Matplotlib Demo",
    "section": "WIDGETS",
    "text": "WIDGETS\n\n\nCode\nfrom ipyleaflet import Map, Marker, basemaps, basemap_to_tiles\nm = Map(\n  basemap=basemap_to_tiles(\n    basemaps.NASAGIBS.ModisTerraTrueColorCR, \"2023-05-27\"\n  ),\n  center=(31.8841, 243.413),\n  zoom=4\n)\nm.add_layer(Marker(location=(31.8841,243.413)))\nm\n\n\n\n\n\n\n\nFigure 2: A widgets demo"
  },
  {
    "objectID": "basics_jupyter.html#xarray",
    "href": "basics_jupyter.html#xarray",
    "title": "Matplotlib Demo",
    "section": "XARRAY",
    "text": "XARRAY\n::: {#cell-xarray plot .cell execution_count=3}\n\nCode\nimport xarray as xr\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport cartopy.crs as ccrs\nfrom matplotlib.animation import FuncAnimation\nvariables=['u-component_of_wind_height_above_ground','v-component_of_wind_height_above_ground']\ndsw=xr.open_dataset('https://thredds.ucar.edu/thredds/dodsC/grib/NCEP/GFS/Global_0p25deg/Best')[variables]\nfrom datetime import datetime, timedelta\nstarttime=datetime.utcnow()\nstarttime\ninittime = datetime.utcnow().date().isoformat()   ### Simulation startime..\nendtime = starttime + timedelta(days=10)\nfinaltime=endtime.date().isoformat()\nprint(inittime)\nprint(finaltime)\nlat_toplot = np.arange(5, 35.25, 0.25) # last number is exclusive\nlon_toplot = np.arange(260, 310.25, 0.25) # last number is exclusive\ndataw= dsw.sel(time1=slice(inittime,finaltime),height_above_ground2=10, lon=lon_toplot, lat=lat_toplot)\nu10=dataw['u-component_of_wind_height_above_ground'].values\nv10=dataw['v-component_of_wind_height_above_ground'].values\nlon=dataw.lon.values\nlat=dataw.lat.values\nl=10\nU10=u10[l,:,:].squeeze()\nV10=v10[l,:,:].squeeze()\nvec_crs = ccrs.RotatedPole(pole_longitude=180.0, pole_latitude=90.0)\n#central_rotated_longitude=0.0)\ndata_crs=ccrs.PlateCarree()\nprint(dataw.time1[l])\nfig = plt.figure(figsize=(20,5))\nax1 = fig.add_subplot(1, 1, 1, projection=ccrs.PlateCarree())\nax1.set_extent([260, 311, 4, 40], crs=ccrs.PlateCarree())\nax1.coastlines()\nmagnitude = (U10 ** 2 + V10 ** 2) ** 0.5\nmagnitude.shape\nax1.streamplot(lon, lat, U10, V10, transform=vec_crs,\n                  linewidth=2, density=2, color=magnitude)\nax1.quiver(lon[::5],lat[::5],U10[::5,::5],V10[::5,::5],scale=200.0,color='b',transform=data_crs)\nplt.show()\n\n\n2024-02-08\n2024-02-18\n&lt;xarray.DataArray 'time1' ()&gt;\narray('2024-02-09T06:00:00.000000000', dtype='datetime64[ns]')\nCoordinates:\n    time1                 datetime64[ns] 2024-02-09T06:00:00\n    reftime1              datetime64[ns] ...\n    height_above_ground2  float32 10.0\nAttributes:\n    standard_name:        time\n    long_name:            GRIB forecast or observation time\n    _CoordinateAxisType:  Time\n\n\n\n\n\nxarray demo\n\n\n\n:::"
  },
  {
    "objectID": "bickley_juliai_jsp2.html",
    "href": "bickley_juliai_jsp2.html",
    "title": "TEST BiCKLEY JET DISTRIBUTED",
    "section": "",
    "text": "---\ntitle: Bickley Jet Distributed Demo\nauthor: JSP\ndate: 12/28/2022\nformat:\n  html:\n    code-fold: true\n---"
  },
  {
    "objectID": "bickley_juliai_jsp2.html#test-bickley-jet-distributed",
    "href": "bickley_juliai_jsp2.html#test-bickley-jet-distributed",
    "title": "TEST BiCKLEY JET DISTRIBUTED",
    "section": "TEST BiCKLEY JET DISTRIBUTED",
    "text": "TEST BiCKLEY JET DISTRIBUTED\n\n\n\n#using Distributed\n#nprocs() == 1 && addprocs()\n#@everywhere begin\n#          import Pkg\n#          Pkg.activate(\"/Users/julios/JULIA/CoherentStructures.jl/\")\n#       end\n\n#@everywhere \nusing CoherentStructures, StreamMacros\n\n\n\n\nFigure 1\n\n\n\n\nconst bickley = @velo_from_stream psi begin\n    psi  = psi₀ + psi₁\n    psi₀ = - U₀ * L₀ * tanh(y / L₀)\n    psi₁ =   U₀ * L₀ * sech(y / L₀)^2 * re_sum_term\n\n    re_sum_term = Σ₁ + Σ₂ + Σ₃\n\n    Σ₁ = ε₁ * cos(k₁*(x - c₁*t))\n    Σ₂ = ε₂ * cos(k₂*(x - c₂*t))\n    Σ₃ = ε₃ * cos(k₃*(x - c₃*t))\n\n    k₁ = 2/r₀    ; k₂ = 4/r₀   ; k₃ = 6/r₀\n    ε₁ = 0.0075  ; ε₂ = 0.15   ; ε₃ = 0.3\n    c₂ = 0.205U₀ ; c₃ = 0.461U₀; c₁ = c₃ + (√5-1)*(c₂-c₃)\n    U₀ = 62.66e-6; L₀ = 1770e-3; r₀ = 6371e-3\nend\n\n(::SciMLBase.ODEFunction{false, SciMLBase.FullSpecialize, var\"#9#10\", LinearAlgebra.UniformScaling{Bool}, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, typeof(SciMLBase.DEFAULT_OBSERVED), Nothing, Nothing}) (generic function with 1 method)\n\n\n\n#@everywhere \nusing OrdinaryDiffEq, Tensors\nq = 81\nconst tspan = range(0., stop=3456000., length=q)\nny = 61\nnx = (22ny) ÷ 6\nxmin, xmax, ymin, ymax = 0.0 - 2.0, 6.371π + 2.0, -3.0, 3.0\nxspan = range(xmin, stop=xmax, length=nx)\nyspan = range(ymin, stop=ymax, length=ny)\nP = tuple.(xspan, yspan')\nconst δ = 1.e-6\nconst D = SymmetricTensor{2,2}((2., 0., 1/2))\nmCG_tensor = u -&gt; av_weighted_CG_tensor(bickley, u, tspan, δ; D=(_ -&gt; D), tolerance=1e-6, solver=Tsit5())\n\n#C̅ = pmap(mCG_tensor, P; batch_size=ceil(Int, length(P)/nprocs()^2))\nC̅ = map(mCG_tensor, P)\np = LCSParameters(2.0)\nvortices, singularities = ellipticLCS(C̅, xspan, yspan, p)\n\n┌ Info: Found 63 singularities...\n└ @ CoherentStructures /Users/julios/.julia/environments/CoherentStructures/src/ellipticLCS.jl:894\n┌ Info: Defined 7 Poincaré sections...\n└ @ CoherentStructures /Users/julios/.julia/environments/CoherentStructures/src/ellipticLCS.jl:896\nDetecting vortices  29%|█████████▍                       |  ETA: 0:00:22\n  num_barriers:  2\nDetecting vortices 100%|█████████████████████████████████| Time: 0:00:11\n  num_barriers:  7\n┌ Info: Found 7 elliptic barriers in total.\n└ @ CoherentStructures /Users/julios/.julia/environments/CoherentStructures/src/ellipticLCS.jl:1194\n\n\n(CoherentStructures.EllipticVortex[CoherentStructures.EllipticVortex([6.9245254985211595, -1.2], CoherentStructures.EllipticBarrier[CoherentStructures.EllipticBarrier(StaticArraysCore.SVector{2, Float64}[[8.179937219365762, -1.2], [8.174550234556019, -1.1864225639817538], [8.158907507304592, -1.149513237617858], [8.151314552511586, -1.1328414223938708], [8.136901415284362, -1.1029727274000647], [8.118681525668242, -1.0677072520006987], [8.107098098345423, -1.046375445491992], [8.066977713963475, -0.9776638610785207], [8.049137922019563, -0.9494182577535918], [8.03269938223394, -0.9246196727399018]  …  [8.242808279441496, -1.4748019756464354], [8.241142455600919, -1.4484852962211603], [8.236214789489916, -1.4050476498332185], [8.22832633126203, -1.3599274926915212], [8.221238073844878, -1.328442799555096], [8.204353367802028, -1.26872035540583], [8.193645186021824, -1.2366673376703643], [8.180083608673206, -1.200087137022632], [8.180049621034497, -1.2000000000000004], [8.180049621034497, -1.2000000000000004]], [6.9245254985211595, -1.2], 1.233048504373692, false)]), CoherentStructures.EllipticVortex([13.090561297499413, -1.2], CoherentStructures.EllipticBarrier[CoherentStructures.EllipticBarrier(StaticArraysCore.SVector{2, Float64}[[14.425681699032562, -1.2], [14.419629574391488, -1.186632957881917], [14.398819252311547, -1.1431514353262542], [14.38019807876337, -1.1072293693076916], [14.369219808509307, -1.0871902661093897], [14.363618222937294, -1.0771787080937454], [14.339170007401691, -1.0350409896707524], [14.322704568050272, -1.0079955651925039], [14.314488150491224, -0.994871834688239], [14.305330117415732, -0.9805185038878036]  …  [14.503722043522757, -1.4648264388613497], [14.498976731320129, -1.4311429437279386], [14.48492346039499, -1.3654583224004841], [14.475044582597194, -1.3302645663396064], [14.46672143402849, -1.3042997128430251], [14.449417243362838, -1.2566357516551476], [14.440607930719947, -1.234572675502949], [14.431745490930927, -1.213531673959329], [14.425795134994164, -1.2000000000000006], [14.425795134994164, -1.2000000000000006]], [13.090561297499413, -1.2], 1.2217857870452369, false)]), CoherentStructures.EllipticVortex([-0.052830800322656316, -0.8999999999999999], CoherentStructures.EllipticBarrier[CoherentStructures.EllipticBarrier(StaticArraysCore.SVector{2, Float64}[[1.1393136076430643, -0.8999999999999999], [1.133783394494916, -0.8870913618829113], [1.128875850076516, -0.8759126131059427], [1.1086889485808942, -0.832302585414342], [1.0943126168930986, -0.8032111292924007], [1.0792536415694631, -0.7739869554934028], [1.072141755469282, -0.7605161185827971], [1.0402184020412715, -0.7028739459590315], [1.0145824021372918, -0.6600916221395909], [0.9927101947957275, -0.6258262502481187]  …  [1.2189175822021618, -1.1325299824885022], [1.2119424391124458, -1.1067088249114165], [1.2056483707298014, -1.08497684138225], [1.2028464658863423, -1.0756379045444406], [1.1797856097023816, -1.0051714300614023], [1.1678671899634574, -0.9722329522663057], [1.1603918369474608, -0.9524299360373579], [1.1419538339973203, -0.9063441149246717], [1.1392882304444607, -0.9000000000000005], [1.1392882304444607, -0.9000000000000005]], [-0.052830800322656316, -0.8999999999999999], 1.2375831789339802, false)]), CoherentStructures.EllipticVortex([20.013829563018856, -0.8999999999999999], CoherentStructures.EllipticBarrier[CoherentStructures.EllipticBarrier(StaticArraysCore.SVector{2, Float64}[[21.14967826283064, -0.8999999999999999], [21.14408545107262, -0.8869186023986599], [21.12576656562869, -0.8463456212154425], [21.110646327579715, -0.8150481969792599], [21.094252414121417, -0.7828340279999625], [21.083897381920103, -0.7631048541844845], [21.052569643987766, -0.7063358561579394], [21.02639523566657, -0.6624743533061815], [21.00636500489063, -0.6309724895205713], [20.971324977422068, -0.5793339172900218]  …  [21.262396807578767, -1.3260897495459978], [21.257087571068663, -1.2770269502393807], [21.249973676826468, -1.2296882076963151], [21.226325038728934, -1.1216068513362418], [21.211974389391774, -1.0714979637216036], [21.19904406713166, -1.0308848897360152], [21.177472954512268, -0.9698348509368971], [21.167155077626894, -0.9427371112844164], [21.149837237059227, -0.9000000000000008], [21.149837237059227, -0.9000000000000008]], [20.013829563018856, -0.8999999999999999], 1.2428512973246524, false)]), CoherentStructures.EllipticVortex([3.516979399085807, 1.0], CoherentStructures.EllipticBarrier[CoherentStructures.EllipticBarrier(StaticArraysCore.SVector{2, Float64}[[4.589909366254956, 1.0], [4.5931448180185495, 1.013838508374781], [4.600171068500731, 1.0475366101334407], [4.603451339313752, 1.0656673402146584], [4.612385574088888, 1.1322629387157679], [4.614768132996646, 1.1631074350759203], [4.61524392325829, 1.251073247658957], [4.611819794739753, 1.2945737688630181], [4.60214068803026, 1.3584631181896805], [4.596844364379431, 1.3830295089469955]  …  [4.481747200115654, 0.7235492580439102], [4.498514505021339, 0.755491855683896], [4.506037002522079, 0.7704758178556254], [4.528077662481855, 0.8174393164406109], [4.537696845108924, 0.8398134154028079], [4.558206008234462, 0.8929950832616457], [4.574393464688156, 0.9426307420716656], [4.584420406216815, 0.9783514229279265], [4.58987242122158, 0.9999999999999994], [4.58987242122158, 0.9999999999999994]], [3.516979399085807, 1.0], 1.282529038396732, true)]), CoherentStructures.EllipticVortex([16.444019363610394, 1.0], CoherentStructures.EllipticBarrier[CoherentStructures.EllipticBarrier(StaticArraysCore.SVector{2, Float64}[[17.559940893250044, 1.0], [17.564527934724236, 1.0135444329467571], [17.57458652277073, 1.0452829992782822], [17.5865960792025, 1.0880376478979004], [17.596543711635476, 1.129066309731338], [17.603194896268953, 1.1609582880265559], [17.615909893673248, 1.243781061516744], [17.620171939319636, 1.2943749749730933], [17.61993166570977, 1.3930920358914032], [17.61287790018642, 1.4580188782837074]  …  [17.41449105065031, 0.6943286183697317], [17.443422591246204, 0.7432952241004384], [17.46022348036495, 0.7735788954370825], [17.48567059991337, 0.8226202748621035], [17.49638894206539, 0.8447640918118674], [17.51845044646951, 0.8934886008708054], [17.540818376681948, 0.9479037254806912], [17.55328400060342, 0.9811833490854039], [17.55992672746429, 0.9999999999999997], [17.55992672746429, 0.9999999999999997]], [16.444019363610394, 1.0], 1.1799954710505167, true)]), CoherentStructures.EllipticVortex([10.007543398010288, 1.25], CoherentStructures.EllipticBarrier[CoherentStructures.EllipticBarrier(StaticArraysCore.SVector{2, Float64}[[11.457985761035246, 1.25], [11.465995995048958, 1.2626408345092017], [11.480135787861322, 1.285655076002317], [11.496433390238481, 1.313561925267759], [11.506623980305987, 1.3319091710405968], [11.51996890431772, 1.3570807334500052], [11.540380768189506, 1.3981315615680356], [11.54974668258743, 1.4182483606683556], [11.55961102081072, 1.4407781601490943], [11.574928265268362, 1.4792222961002006]  …  [11.3005864202349, 1.0366748508842252], [11.324921216155753, 1.0663720515569877], [11.346532742838907, 1.093395011247609], [11.36558787694774, 1.117793375889736], [11.373728246396185, 1.1284573937504714], [11.411593981373125, 1.1804309272247895], [11.435215470700872, 1.2149914134760833], [11.452119661375026, 1.2405919027589183], [11.458185383067358, 1.2499999999999984], [11.458185383067358, 1.2499999999999984]], [10.007543398010288, 1.25], 1.1748930265565731, true)])], Singularity[Singularity([-1.9459119666756293, -2.95], 1//2), Singularity([9.953455364685915, -2.95], -1//2), Singularity([10.602511764578363, -2.85], 1//2), Singularity([15.362258697122982, -2.95], -1//2), Singularity([18.066660363341512, -2.95], 1//2), Singularity([4.003771699005143, -2.85], -1//2), Singularity([11.467920297768295, -2.85], -1//2), Singularity([-1.7295598333781466, -2.45], -1//2), Singularity([8.438990431603537, -2.45], 1//2), Singularity([10.191442711313147, -2.33], -1//2)  …  Singularity([11.792448497714519, 2.65], 1//2), Singularity([12.549680964255707, 2.65], -1//2), Singularity([13.955969830689344, 2.65], 1//2), Singularity([19.689301363072634, 2.65], -1//2), Singularity([13.63144163074312, 2.75], -1//2), Singularity([4.003771699005143, 2.8499999999999996], -1//2), Singularity([5.734588765385005, 2.8499999999999996], 1//2), Singularity([6.708173365223677, 2.8499999999999996], 1//2), Singularity([7.681757965062348, 2.9499999999999997], -1//2), Singularity([17.417603963449064, 2.9499999999999997], -1//2)])\n\n\n\nusing Plots\ntrace = tensor_invariants(C̅)[5]\nfig = plot_vortices(vortices, singularities, (xmin, ymin), (xmax, ymax);\n    bg=trace, xspan=xspan, yspan=yspan, title=\"DBS field and transport barriers\", showlabel=false)\nPlots.plot(fig)"
  },
  {
    "objectID": "makie_julia.html",
    "href": "makie_julia.html",
    "title": "Makie Demo",
    "section": "",
    "text": "Code\n#using Distributed\n#nprocs() == 1 && addprocs()\n@everywhere begin\n          import Pkg\n          Pkg.activate(\"/Users/julios/JULIA/CoherentStructures.jl/\")\n       end\n\n#@everywhere \nusing CoherentStructures, StreamMacros\nconst bickley = @velo_from_stream psi begin\n    psi  = psi₀ + psi₁\n    psi₀ = - U₀ * L₀ * tanh(y / L₀)\n    psi₁ =   U₀ * L₀ * sech(y / L₀)^2 * re_sum_term\n\n    re_sum_term = Σ₁ + Σ₂ + Σ₃\n\n    Σ₁ = ε₁ * cos(k₁*(x - c₁*t))\n    Σ₂ = ε₂ * cos(k₂*(x - c₂*t))\n    Σ₃ = ε₃ * cos(k₃*(x - c₃*t))\n\n    k₁ = 2/r₀    ; k₂ = 4/r₀   ; k₃ = 6/r₀\n    ε₁ = 0.0075  ; ε₂ = 0.15   ; ε₃ = 0.3\n    c₂ = 0.205U₀ ; c₃ = 0.461U₀; c₁ = c₃ + (√5-1)*(c₂-c₃)\n    U₀ = 62.66e-6; L₀ = 1770e-3; r₀ = 6371e-3\nend\n\n#@everywhere \nusing OrdinaryDiffEq, Tensors\nq = 81\nconst tspan = range(0., stop=3456000., length=q)\nny = 61\nnx = (22ny) ÷ 6\nxmin, xmax, ymin, ymax = 0.0 - 2.0, 6.371π + 2.0, -3.0, 3.0\nxspan = range(xmin, stop=xmax, length=nx)\nyspan = range(ymin, stop=ymax, length=ny)\nP = tuple.(xspan, yspan')\nconst δ = 1.e-6\nconst D = SymmetricTensor{2,2}((2., 0., 1/2))\nmCG_tensor = u -&gt; av_weighted_CG_tensor(bickley, u, tspan, δ; D=(_ -&gt; D), tolerance=1e-6, solver=Tsit5())\n\n#C̅ = pmap(mCG_tensor, P; batch_size=ceil(Int, length(P)/nprocs()^2))\nC̅ = map(mCG_tensor, P)\np = LCSParameters(2.0)\nvortices, singularities = ellipticLCS(C̅, xspan, yspan, p)\n\nusing Plots\ntrace = tensor_invariants(C̅)[5]\nfig = plot_vortices(vortices, singularities, (xmin, ymin), (xmax, ymax);\n    bg=trace, xspan=xspan, yspan=yspan, title=\"DBS field and transport barriers\", showlabel=false)\nPlots.plot(fig)\n\n\n\nFigure 1"
  },
  {
    "objectID": "makie_julia.html#test-makie",
    "href": "makie_julia.html#test-makie",
    "title": "Makie Demo",
    "section": "",
    "text": "Code\n#using Distributed\n#nprocs() == 1 && addprocs()\n@everywhere begin\n          import Pkg\n          Pkg.activate(\"/Users/julios/JULIA/CoherentStructures.jl/\")\n       end\n\n#@everywhere \nusing CoherentStructures, StreamMacros\nconst bickley = @velo_from_stream psi begin\n    psi  = psi₀ + psi₁\n    psi₀ = - U₀ * L₀ * tanh(y / L₀)\n    psi₁ =   U₀ * L₀ * sech(y / L₀)^2 * re_sum_term\n\n    re_sum_term = Σ₁ + Σ₂ + Σ₃\n\n    Σ₁ = ε₁ * cos(k₁*(x - c₁*t))\n    Σ₂ = ε₂ * cos(k₂*(x - c₂*t))\n    Σ₃ = ε₃ * cos(k₃*(x - c₃*t))\n\n    k₁ = 2/r₀    ; k₂ = 4/r₀   ; k₃ = 6/r₀\n    ε₁ = 0.0075  ; ε₂ = 0.15   ; ε₃ = 0.3\n    c₂ = 0.205U₀ ; c₃ = 0.461U₀; c₁ = c₃ + (√5-1)*(c₂-c₃)\n    U₀ = 62.66e-6; L₀ = 1770e-3; r₀ = 6371e-3\nend\n\n#@everywhere \nusing OrdinaryDiffEq, Tensors\nq = 81\nconst tspan = range(0., stop=3456000., length=q)\nny = 61\nnx = (22ny) ÷ 6\nxmin, xmax, ymin, ymax = 0.0 - 2.0, 6.371π + 2.0, -3.0, 3.0\nxspan = range(xmin, stop=xmax, length=nx)\nyspan = range(ymin, stop=ymax, length=ny)\nP = tuple.(xspan, yspan')\nconst δ = 1.e-6\nconst D = SymmetricTensor{2,2}((2., 0., 1/2))\nmCG_tensor = u -&gt; av_weighted_CG_tensor(bickley, u, tspan, δ; D=(_ -&gt; D), tolerance=1e-6, solver=Tsit5())\n\n#C̅ = pmap(mCG_tensor, P; batch_size=ceil(Int, length(P)/nprocs()^2))\nC̅ = map(mCG_tensor, P)\np = LCSParameters(2.0)\nvortices, singularities = ellipticLCS(C̅, xspan, yspan, p)\n\nusing Plots\ntrace = tensor_invariants(C̅)[5]\nfig = plot_vortices(vortices, singularities, (xmin, ymin), (xmax, ymax);\n    bg=trace, xspan=xspan, yspan=yspan, title=\"DBS field and transport barriers\", showlabel=false)\nPlots.plot(fig)\n\n\n\nFigure 1"
  },
  {
    "objectID": "about.html#modelos-de-capas",
    "href": "about.html#modelos-de-capas",
    "title": "About",
    "section": "1 Modelos de Capas",
    "text": "1 Modelos de Capas\n\\dfrac {\\partial \\overrightarrow {u}_{i}}{\\partial t}+\\overrightarrow {u}_{i}\\cdot {\\nabla  \\overrightarrow u}_{i}+f  \\overrightarrow {k}\\times \\overrightarrow {u}_{i}=-\\dfrac {1}{\\rho _{0}}\\nabla P_{i}\n\\dfrac {\\partial h_{i}}{\\partial t}+\\nabla \\cdot \\left( h\\overrightarrow {u}_{i}\\right) =0\n\\dfrac {\\partial P}{\\partial z}=-\\rho g"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Julio Sheinbaum",
    "section": "",
    "text": "Modelling the Caribbean Sea and Gulf of Mexico\nForecasting the Loop Current\nOcean Data Assimilation\nObserving the Ocean (Moorings,Gliders,Argos,Drifters)\nAir-Sea Interaction and Climate\n\n\n\n\n1983- 1985 Universidad Nacional Autonoma de Mexico M. Sc. Physics\n1985 - 1989 University of Oxford D.Phil. Physical Oceanography\n\nThis is a Quarto website. To learn more about Quarto websites visit https://quarto.org/docs/websites."
  },
  {
    "objectID": "index.html#reasearch-interests",
    "href": "index.html#reasearch-interests",
    "title": "Julio Sheinbaum",
    "section": "",
    "text": "Modelling the Caribbean Sea and Gulf of Mexico\nForecasting the Loop Current\nOcean Data Assimilation\nObserving the Ocean (Moorings,Gliders,Argos,Drifters)\nAir-Sea Interaction and Climate"
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Julio Sheinbaum",
    "section": "",
    "text": "1983- 1985 Universidad Nacional Autonoma de Mexico M. Sc. Physics\n1985 - 1989 University of Oxford D.Phil. Physical Oceanography\n\nThis is a Quarto website. To learn more about Quarto websites visit https://quarto.org/docs/websites."
  },
  {
    "objectID": "bickley_julia_jsp.html",
    "href": "bickley_julia_jsp.html",
    "title": "TEST BiCKLEY JET DISTRIBUTED",
    "section": "",
    "text": "---\ntitle: Bickley Jet Distributed Demo\nauthor: JSP\ndate: 12/28/2022\nformat:\n  html:\n    code-fold: true\n---"
  },
  {
    "objectID": "bickley_julia_jsp.html#test-bickley-jet-distributed",
    "href": "bickley_julia_jsp.html#test-bickley-jet-distributed",
    "title": "TEST BiCKLEY JET DISTRIBUTED",
    "section": "TEST BiCKLEY JET DISTRIBUTED",
    "text": "TEST BiCKLEY JET DISTRIBUTED\n\n#using Distributed\n#nprocs() == 1 && addprocs()\n#@everywhere begin\n      import Pkg\n         Pkg.activate(\"/Users/julios/JULIA/CoherentStructures.jl/\")\n      #end\n\n#@everywhere \nusing CoherentStructures, StreamMacros\nconst bickley = @velo_from_stream psi begin\n    psi  = psi₀ + psi₁\n    psi₀ = - U₀ * L₀ * tanh(y / L₀)\n    psi₁ =   U₀ * L₀ * sech(y / L₀)^2 * re_sum_term\n\n    re_sum_term = Σ₁ + Σ₂ + Σ₃\n\n    Σ₁ = ε₁ * cos(k₁*(x - c₁*t))\n    Σ₂ = ε₂ * cos(k₂*(x - c₂*t))\n    Σ₃ = ε₃ * cos(k₃*(x - c₃*t))\n\n    k₁ = 2/r₀    ; k₂ = 4/r₀   ; k₃ = 6/r₀\n    ε₁ = 0.0075  ; ε₂ = 0.15   ; ε₃ = 0.3\n    c₂ = 0.205U₀ ; c₃ = 0.461U₀; c₁ = c₃ + (√5-1)*(c₂-c₃)\n    U₀ = 62.66e-6; L₀ = 1770e-3; r₀ = 6371e-3\nend\n\n#@everywhere \nusing OrdinaryDiffEq, Tensors\nq = 81\nconst tspan = range(0., stop=3456000., length=q)\nny = 61\nnx = (22ny) ÷ 6\nxmin, xmax, ymin, ymax = 0.0 - 2.0, 6.371π + 2.0, -3.0, 3.0\nxspan = range(xmin, stop=xmax, length=nx)\nyspan = range(ymin, stop=ymax, length=ny)\nP = tuple.(xspan, yspan')\nconst δ = 1.e-6\nconst D = SymmetricTensor{2,2}((2., 0., 1/2))\nmCG_tensor = u -&gt; av_weighted_CG_tensor(bickley, u, tspan, δ; D=(_ -&gt; D), tolerance=1e-6, solver=Tsit5())\n\n#C̅ = pmap(mCG_tensor, P; batch_size=ceil(Int, length(P)/nprocs()^2))\nC̅ = map(mCG_tensor, P)\np = LCSParameters(2.0)\nvortices, singularities = ellipticLCS(C̅, xspan, yspan, p)\n\nusing Plots\ntrace = tensor_invariants(C̅)[5]\nfig = plot_vortices(vortices, singularities, (xmin, ymin), (xmax, ymax);\n    bg=trace, xspan=xspan, yspan=yspan, title=\"DBS field and transport barriers\", showlabel=false)\nPlots.plot(fig)\n\n  Activating project at `~/JULIA/CoherentStructures.jl`\n┌ Info: Found 63 singularities...\n└ @ CoherentStructures /Users/julios/JULIA/CoherentStructures.jl/src/ellipticLCS.jl:894\n┌ Info: Defined 7 Poincaré sections...\n└ @ CoherentStructures /Users/julios/JULIA/CoherentStructures.jl/src/ellipticLCS.jl:896\nDetecting vortices  29%|█████████▍                       |  ETA: 0:00:21\n  num_barriers:  2\nDetecting vortices 100%|█████████████████████████████████| Time: 0:00:11\n  num_barriers:  7\n┌ Info: Found 7 elliptic barriers in total.\n└ @ CoherentStructures /Users/julios/JULIA/CoherentStructures.jl/src/ellipticLCS.jl:1194\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\nFigure 1: Stream Plot"
  },
  {
    "objectID": "presentation.html#empieza",
    "href": "presentation.html#empieza",
    "title": "Test Reveal Presentations",
    "section": "Empieza",
    "text": "Empieza\n\nTurn on Math\n\\(Ax=b\\)\nCauchy-Shwarz\n\n\\[\\left( \\sum_{k=1}^n a_k b_k \\right)^2 \\leq \\left( \\sum_{k=1}^n a_k^2 \\right) \\left( \\sum_{k=1}^n b_k^2 \\right)\\]"
  },
  {
    "objectID": "presentation.html#more-math",
    "href": "presentation.html#more-math",
    "title": "Test Reveal Presentations",
    "section": "More Math",
    "text": "More Math\n\n\n\nA Cross Product Formula\n\n\\[\\mathbf{V}_1 \\times \\mathbf{V}_2 =  \\begin{vmatrix}\n  \\mathbf{\\hat i} &  \\mathbf{\\hat j} & \\mathbf{\\hat k} \\\\\n  \\frac{\\partial X}{\\partial u} &  \\frac{\\partial Y}{\\partial u} & 0 \\\\\n  \\frac{\\partial X}{\\partial v} &  \\frac{\\partial Y}{\\partial v} & 0\n  \\end{vmatrix}  \\]\n\n\nThe Lorenz Equations \\[\n\\begin{aligned}\n\\dot{x} & = \\sigma(y-x) \\\\\n\\dot{y} & = \\rho x - y - xz \\\\\n\\dot{z} & = -\\beta z + xy\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "presentation.html#julia",
    "href": "presentation.html#julia",
    "title": "Test Reveal Presentations",
    "section": "Julia",
    "text": "Julia\nExample taken from https://docs.makie.org/stable/\n#| echo: true\nusing GLMakie\n\nBase.@kwdef mutable struct Lorenz\n    dt::Float64 = 0.01\n    σ::Float64 = 10\n    ρ::Float64 = 28\n    β::Float64 = 8/3\n    x::Float64 = 1\n    y::Float64 = 1\n    z::Float64 = 1\nend\n\nfunction step!(l::Lorenz)\n    dx = l.σ * (l.y - l.x)\n    dy = l.x * (l.ρ - l.z) - l.y\n    dz = l.x * l.y - l.β * l.z\n    l.x += l.dt * dx\n    l.y += l.dt * dy\n    l.z += l.dt * dz\n    Point3f(l.x, l.y, l.z)\nend\n\nattractor = Lorenz()\n\npoints = Observable(Point3f[])\ncolors = Observable(Int[])\n\nset_theme!(theme_black())\n\nfig, ax, l = lines(points, color = colors,\n    colormap = :inferno, transparency = true,\n    axis = (; type = Axis3, protrusions = (0, 0, 0, 0),\n        viewmode = :fit, limits = (-30, 30, -30, 30, 0, 50)))\n\nrecord(fig, \"lorenz.mp4\", 1:120) do frame\n    for i in 1:50\n        push!(points[], step!(attractor))\n        push!(colors[], frame)\n    end\n    ax.azimuth[] = 1.7pi + 0.3 * sin(2pi * frame / 120)\n    notify.((points, colors))\n    l.colorrange = (0, frame)\nend"
  },
  {
    "objectID": "presentation.html#lorenz-atractor",
    "href": "presentation.html#lorenz-atractor",
    "title": "Test Reveal Presentations",
    "section": "Lorenz Atractor",
    "text": "Lorenz Atractor"
  },
  {
    "objectID": "presentation.html#polar-axis",
    "href": "presentation.html#polar-axis",
    "title": "Test Reveal Presentations",
    "section": "Polar Axis",
    "text": "Polar Axis\nFor a demonstration of a line plot on a polar axis, see Figure 1.\n\n\n\n\n\n\n\n\nFigure 1: A line plot on a polar axis"
  },
  {
    "objectID": "presentation.html#python",
    "href": "presentation.html#python",
    "title": "Test Reveal Presentations",
    "section": "Python",
    "text": "Python\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nr = np.arange(0, 2, 0.01)\ntheta = 2 * np.pi * r\nfig, ax = plt.subplots(subplot_kw={'projection': 'polar'})\nax.plot(theta, r)\nax.set_rticks([0.5, 1, 1.5, 2])\nax.grid(True)\nplt.show()"
  },
  {
    "objectID": "presentation.html#gfs-download-and-plot",
    "href": "presentation.html#gfs-download-and-plot",
    "title": "Test Reveal Presentations",
    "section": "GFS Download and Plot",
    "text": "GFS Download and Plot\n\n\n\n#| label: xarray plot\n#| fig-cap: \"xarray demo\"\nimport xarray as xr\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport cartopy.crs as ccrs\nfrom matplotlib.animation import FuncAnimation\nvariables=['u-component_of_wind_height_above_ground','v-component_of_wind_height_above_ground']\ndsw=xr.open_dataset('https://thredds.ucar.edu/thredds/dodsC/grib/NCEP/GFS/Global_0p25deg/Best')[variables]\nfrom datetime import datetime, timedelta\nstarttime=datetime.utcnow()\nstarttime\ninittime = datetime.utcnow().date().isoformat()   ### Simulation startime..\nendtime = starttime + timedelta(days=10)\nfinaltime=endtime.date().isoformat()\nprint(inittime)\nprint(finaltime)\nlat_toplot = np.arange(5, 35.25, 0.25) # last number is exclusive\nlon_toplot = np.arange(260, 310.25, 0.25) # last number is exclusive\ndataw= dsw.sel(time1=slice(inittime,finaltime),height_above_ground2=10, lon=lon_toplot, lat=lat_toplot)\nu10=dataw['u-component_of_wind_height_above_ground'].values\nv10=dataw['v-component_of_wind_height_above_ground'].values\nlon=dataw.lon.values\nlat=dataw.lat.values\nl=10\nU10=u10[l,:,:].squeeze()\nV10=v10[l,:,:].squeeze()\nvec_crs = ccrs.RotatedPole(pole_longitude=180.0, pole_latitude=90.0)\n#central_rotated_longitude=0.0)\ndata_crs=ccrs.PlateCarree()\n#print(dataw.time[l])\nfig = plt.figure(figsize=(20, 5))\nax1 = fig.add_subplot(1, 1, 1, projection=ccrs.PlateCarree())\nax1.set_extent([260, 311, 4, 40], crs=ccrs.PlateCarree())\nax1.coastlines()\nmagnitude = (U10 ** 2 + V10 ** 2) ** 0.5\n#magnitude.shape\nax1.streamplot(lon, lat, U10, V10, transform=vec_crs,\n                  linewidth=2, density=2, color=magnitude)\nax1.quiver(lon[::5],lat[::5],U10[::5,::5],V10[::5,::5],scale=200.0,color='b',transform=data_crs)\nplt.savefig('foo.png', bbox_inches='tight')\nplt.show()"
  },
  {
    "objectID": "presentation.html#gfs-download-and-plot-output",
    "href": "presentation.html#gfs-download-and-plot-output",
    "title": "Test Reveal Presentations",
    "section": "GFS Download and Plot",
    "text": "GFS Download and Plot\n\n2024-02-08\n2024-02-18"
  },
  {
    "objectID": "presentation.html#plotly",
    "href": "presentation.html#plotly",
    "title": "Test Reveal Presentations",
    "section": "Plotly",
    "text": "Plotly\n\n\n\nimport plotly.express as px\nimport plotly.io as pio\ndf = px.data.iris()\nfig = px.scatter(df, x=\"sepal_width\", y=\"sepal_length\",\n                 color=\"species\",\n                 marginal_y=\"violin\", marginal_x=\"box\",\n                 trendline=\"ols\", template=\"simple_white\")\nfig.show()"
  },
  {
    "objectID": "presentation.html#plotly-output",
    "href": "presentation.html#plotly-output",
    "title": "Test Reveal Presentations",
    "section": "Plotly",
    "text": "Plotly"
  },
  {
    "objectID": "presentation.html#instrmentación-1-y-latex",
    "href": "presentation.html#instrmentación-1-y-latex",
    "title": "Test Reveal Presentations",
    "section": "INSTRMENTACIÓN 1 y LaTeX",
    "text": "INSTRMENTACIÓN 1 y LaTeX\n\n\\[\\frac{\\partial{\\mathbf {u}_h}}{\\partial{t}} + \\mathbf {u}_h \\cdot {\\nabla}_h \\mathbf {{u}_h}\\]\n\n\n\n\n\n\nEstaciones Hidrográficas\n\n\n\n\n\n\n\nPerfiladores Anclajes2"
  },
  {
    "objectID": "presentation.html#instrumentacion2",
    "href": "presentation.html#instrumentacion2",
    "title": "Test Reveal Presentations",
    "section": "INSTRUMENTACION2",
    "text": "INSTRUMENTACION2\n\n\n\n\n\n\n\n\nRevolución Lagrangiana\n\n\n\n\n\n\n\nPercepción Remota\n\n\n\n\n\n\n\n\n\n\nhttps://quarto.org"
  },
  {
    "objectID": "cvprod.html",
    "href": "cvprod.html",
    "title": "CV-PRODUCTS",
    "section": "",
    "text": "0.1 Tania Reyes-Jiménez, Gabriela Athié, Cecilia Enriquez, Julio Sheinbaum, Ismael Mariño-Tapia, Mark Marín-Hernández, David Salas-Monreal, Julio Candela. (2023).Triggering mechanisms of the Yucatan upwelling, Continental Shelf Research, Volume 255,104910,ISSN 0278-4343, https://doi.org/10.1016/j.csr.2022.104910.\n0.2 Amon, R. M. W., Ochoa, J., Candela, J., Herzka, S. Z., Pérez-Brunius, P., Sheinbaum, J., … Molodtsov, S. (2023). Ventilation of the deep Gulf of Mexico and potential insights to the Atlantic Meridional Overturning Circulation. Science Advances, 9(11). https://doi.org/10.1126/sciadv.ade1685\n0.3 Vazquez, H. J., G. Gopalakrishnan, and J. Sheinbaum, 2023: Impact of Yucatan Channel Subsurface Velocity Observations on the Gulf of Mexico State Estimates. J. Phys. Oceanogr., 53, 361–385, https://doi.org/10.1175/JPO-D-21-0213.1.\n1 A.E. Romo-Curiel,Z. Ramírez-Mendoza,A. Fajardo-Yamamoto, M.R. Ramírez-León, M.C. García-Aguilar, S.Z. Herzka,P. Pérez-Brunius,L.E. Saldaña-Ruiz, J. Sheinbaum, K. Kotzakoulakis, J. Rodríguez-Outerelo, F. Medrano and O. Sosa-Nishizaki, (2022). Assessing the exposure risk of large pelagic fish to oil spills scenarios in the deep waters of the Gulf of Mexico. Marine Pollution Bulletin, 176, p.113434, https://doi.org/10.1016/j.marpolbul.2022.113434\n2 Damien, P., Sheinbaum Pardo, J., Pasqueron De Fommervault, O., Jouanno, J., Linacre Rojas , L., & Duteil, O. (2021). Do Loop Current eddies stimulate productivity in the Gulf of Mexico?. Biogeosciences, 18(14), 4281-4303. doi: 10.5194/bg-18-4281-2021\n3 Jouanno, J., Benshila, R., Berline, L., Soulié, A., Radenac, R. H., Morvan, G., Diaz, F., Sheinbaum Pardo, J., Chevalier, C., Thibaut, T., Changeux, T., Menard, F., Berthet, S., Aumont, O., Ethé, C., Nabat, P., & Mallet, M. (2021). A NEMO-based model of Sargassum distribution in the tropical Atlantic: Description of the model and sensitivity analysis (NEMO-Sarg1.0). Geoscientific Model Development, 14(6), 4069-4086. doi: 10.5194/gmd-14-4069-2021.\n4 Ursella, L., Pensieri, S., Pallás Sanz, E., Herzka Llona, S. Z., Bozzano, R., Costa de Almeida Tenreiro, M. J., Cardin, V., Candela Pérez, J., & Sheinbaum Pardo, J. (2021). Diel, lunar and seasonal vertical migration in the deep western Gulf of Mexico evidenced from a long-term data series of acoustic backscatter. Progress in Oceanography, 195. doi: 10.1016/j.pocean.2021.102562.\n5 Ochoa de la Torre, J. L., Ferreira Bartrina, V., Candela Pérez, J., Sheinbaum Pardo, J., López Mariscal, J. M., Pérez Brunius, P., Herzka Llona, S. Z., & Amon, R. (2021). Deep-Water Warming in the Gulf of Mexico from 2003 to 2019. Journal of Physical Oceanography, 51(4), 1021-1035. doi: 10.1175/JPO-D-19-0295.1.\n6 Jouanno, J., Moquet, J. S., Berline, L., Radenac, R. H., Santini, W., Changeux, T., Thibaut, T., Podlejski, W., Menard, F., Martinez, J. M., Aumont, O., Sheinbaum Pardo, J., Filizola, N., & Moukandi N’Kaya, G. (2021). Evolution of the riverine nutrient export to the Tropical Atlantic over the last 15 years: is there a link with Sargassum proliferation?. Environmental Research Letters, 16(3), 034042 1-14. doi: 10.1088/1748-9326/abe11a.\n7 Guerrero Moreno, L., Sheinbaum Pardo, J., Mariño Tapia, I., González Rejón, J. J., & Pérez Brunius, P. (2020). Influence of mesoscale eddies on cross-shelf exchange in the western Gulf of Mexico. Continental Shelf Research, 209(12), 104243. doi: 10.1016/j.csr.2020.104243.\n8 Ochoa de la Torre, J. L., Badan Dangon, A. R. F., Sheinbaum Pardo, J., & Castro López, J. (2020). `Preferred Trajectories¿ defined by mass and potential vorticity conservation. Geofísica Internacional, 59(3), 195-207. doi: 10.22201/igeof.00167169p.2020.59.3.2094.\n8.5 Maslo, A., Azevedo Correia de Souza, J. M., & Sheinbaum Pardo, J. (2020). Energetics of the Deep Gulf of Mexico. Journal of Physical Oceanography, 50(6), 1655-1675. doi: 10.1175/JPO-D-19-0308.1.\n9\nSheila N Estrada-Allis, Julio Sheinbaum Pardo, Joao M Azevedo Correia de Souza, Cecilia Elizabeth Enríquez Ortiz, Ismael Mariño Tapia, and Jorge A Herrera-Silveira. Dissolved inorganic nitrogen and particulate organic nitrogen budget in the Yucatán shelf: driving mechanisms through a physical–biogeochemical coupled model. Biogeosciences, 17(4):1087–1111, February 2020. URL: [https://www.biogeosciences.net/17/1087/2020/ 2020 Estrada-Allis.pdf:doi:10.5194/bg-17-1087-2020.\n10 Thomas Meunier, julio sheinbaum, Enric Pallàs Sanz, Miguel Tenreiro, José Ochoa, Angel Ruiz Angulo, Xavier Carton, and Charly de Marez. Heat Content Anomaly and Decay of Warm-Core Rings: the Case of the Gulf of Mexico. Geophysical research letters, 47(3):21, February 2020. URL: https://agupubs.onlinelibrary.wiley.com/doi/full/10.1029/2019GL085600, doi:10.1029/2019GL085600.\n11\nGabriela Athié, julio sheinbaum, Julio Candela, José Ochoa, Paula Pérez-Brunius, and Angélica Romero-Arteaga. Seasonal Variability of the Transport through the Yucatan Channel from Observations. J Phys Oceanogr, 50(2):343–360, February 2020. URL: http://journals.ametsoc.org/doi/10.1175/JPO-D-18-0269.1, doi:10.1175/JPO-D-18-0269.1.\n12 Olaf Duteil, Pierre Damien, julio sheinbaum, and Marlene Spinner. Ocean currents and coastal exposure to offshore releases of passively transported material in the Gulf of Mexico. Environmental Research Communications, 1(8):081006, September 2019. URL: https://iopscience.iop.org/article/10.1088/2515-7620/ab3aad, doi:10.1088/2515-7620/ab3aad.\n13 J Candela, J Ochoa, J Sheinbaum, M López, P Pérez-Brunius, M Tenreiro, E Pallàs-Sanz, G Athie, and L Arriaza-Oliveros. The Flow through the Gulf of Mexico. J Phys Oceanogr, 49(6):1381–1401, June 2019. URL: http://journals.ametsoc.org/doi/10.1175/JPO-D-18-0189.1, doi:10.1175/JPO-D-18-0189.1.\n14\nMatt K Gough, Francisco J Beron-Vera, María J Olascoaga, julio sheinbaum, Julien Jouanno, and Rodrigo Duran. Persistent Lagrangian Transport Patterns in the Northwestern Gulf of Mexico. J Phys Oceanogr, 49(2):353–367, December 2018. URL: https://doi.org/10.1175/JPO-D-17-0207.1, doi:10.1175/JPO-D-17-0207.1.\n15\nP Miron, F J Beron-Vera, M J Olascoaga, G Froyland, P Pérez-Brunius, and J Sheinbaum. Lagrangian Geography of the Deep Gulf of Mexico. J Phys Oceanogr, 49(1):269–290, November 2018. URL: https://doi.org/10.1175/JPO-D-18-0073.1, doi:10.1175/JPO-D-18-0073.1.\n16 Pierre Damien, Orens Pasqueron de Fommervault, julio sheinbaum, Julien Jouanno, Victor F Camacho-Ibar, and Olaf Duteil. Partitioning of the Open Waters of the Gulf of Mexico Based on the Seasonal and Interannual Variability of Chlorophyll Concentration. Journal of Geophysical Research-Oceans, 115(1):85, April 2018. URL: http://doi.wiley.com/10.1002/2017JC013456, doi:10.1002/2017JC013456.\n17 M Tenreiro, Julio Candela, julio sheinbaum, Enric Pallàs Sanz, and José Ochoa. Near-Surface and Deep Circulation Coupling in the Western Gulf of Mexico. Journal of Physical Oceanography, 48(1):145–161, 2018. URL: https://journals.ametsoc.org/doi/abs/10.1175/JPO-D-17-0018.1, doi:10.1175/JPO-D-17-0018.1.\n18\nJouanno J, Pallàs Sanz E, and Sheinbaum J. Variability and Dynamics of the Yucatan Upwelling: High-Resolution Simulations. Journal of Geophysical Research-Oceans, 123(2):1251–1262, 2018. URL: https://agupubs.onlinelibrary.wiley.com/doi/abs/10.1002/2017JC013535, doi:10.1002/2017JC013535.\n19\nOrens Pasqueron de Fommervault, Paula Pérez-Brunius, Pierre Damien, Victor F Camacho-Ibar, and julio sheinbaum. Temporal variability of chlorophyll distribution in the Gulf of Mexico: bio-optical data from profiling floats. Biogeosciences, 14(24):5647–5662, December 2017. URL: https://www.biogeosciences.net/14/5647/2017/, doi:10.5194/bg-14-5647-2017.\n20\nP Miron, F J Beron-Vera, M J Olascoaga, J Sheinbaum, P Pérez-Brunius, and G Froyland. Lagrangian dynamical geography of the Gulf of Mexico. Scientific Reports, 7(1):7021, August 2017. URL: http://www.nature.com/articles/s41598-017-07177-w, doi:10.1038/s41598-017-07177-w.\n21 P Miron, F J Beron-Vera, M J Olascoaga, J Sheinbaum, P Pérez-Brunius, and G Froyland. Lagrangian dynamical geography of the Gulf of Mexico. Scientific Reports, 7(1):7021, August 2017. URL: http://www.nature.com/articles/s41598-017-07177-w, doi:10.1038/s41598-017-07177-w.\n22\nLuis Zavala Sansón, Paula Pérez-Brunius, and julio sheinbaum. Surface Relative Dispersion in the Southwestern Gulf of Mexico. J Phys Oceanogr, 47(2):387–403, February 2017. URL: http://journals.ametsoc.org/doi/10.1175/JPO-D-16-0105.1, doi:10.1175/JPO-D-16-0105.1.\n23\nLuis Zavala Sansón, Paula Pérez-Brunius, and julio sheinbaum. Surface Relative Dispersion in the Southwestern Gulf of Mexico. J Phys Oceanogr, 47(2):387–403, February 2017. URL: http://journals.ametsoc.org/doi/10.1175/JPO-D-16-0105.1, doi:10.1175/JPO-D-16-0105.1.\n24\nL Zavala Sansón, P Pérez-Brunius, and J Sheinbaum. Point source dispersion of surface drifters in the southern Gulf of Mexico. Environmental Research Letters, 12(2):024006, 2017. URL: http://stacks.iop.org/1748-9326/12/i=2/a=024006.\n25 Enric Pallàs-Sanz, Julio Candela, julio sheinbaum, and José Ochoa. Mooring observations of the near-inertial wave wake of Hurricane Ida (2009). Dynamics of Atmospheres and Oceans VL - IS - SP - EP - PY - T2 -, 76, Part 2 IS -:325–344, December 2016. URL: http://www.sciencedirect.com/science/article/pii/S0377026516300355, doi:10.1016/j.dynatmoce.2016.05.003.\n26\njulio sheinbaum, Gabriela Athié, Julio Candela, José Ochoa, and Angélica Romero-Arteaga. Structure and variability of the Yucatan and loop currents along the slope and shelf break of the Yucatan channel and Campeche bank. Dynamics of Atmospheres and Oceans VL - IS - SP - EP - PY - T2 -, 76, Part 2 IS -:217–239, December 2016. URL: http://www.sciencedirect.com/science/article/pii/S0377026516300501.\n27\nAlejandro Adem, Omar Antolín Camarena, Gordon W Semenoff, and Daniel Sheinbaum. Topology of Fermi surfaces and anomaly inflows. Journal of High Energy Physics, 2016(11):83, November 2016. URL: http://link.springer.com/10.1007/JHEP11(2016)083, doi:10.1007/JHEP11(2016)083.\n28\nJulien Jouanno, José Ochoa, Enric Pallàs-Sanz, julio sheinbaum, Fernando Andrade-Canto, Julio Candela, and Jean-Marc Molines. Loop Current Frontal Eddies: Formation along the Campeche Bank and Impact of Coastally Trapped Waves. Journal of Physical Oceanography, 46(11):3339–3363, November 2016. URL: http://journals.ametsoc.org/doi/10.1175/JPO-D-16-0052.1, doi:10.1175/JPO-D-16-0052.1.\n29\nMaximo Garcia-Jove Navarro, julio sheinbaum, and Julien Jouanno. Sensitivity of Loop Current metrics and eddy detachments to different model configurations: The impact of topography and Caribbean perturbations. ATMOSFERA, 29(3):235–265, June 2016. URL: http://www.revistascca.unam.mx/atm/index.php/atm/article/view/ATM.2016.29.03.05/46565.\n30\nSabrina M Parra, Arnoldo Valle-Levinson, Ismael Mariño Tapia, Cecilia Enriquez, Julio Candela, and julio sheinbaum. Seasonal variability of saltwater intrusion at a point-source submarine groundwater discharge. Limnology and Oceanography, 61(4):1245–1258, May 2016. URL: http://doi.wiley.com/10.1002/lno.10286, doi:10.1002/lno.10286.\n31\nEugenio Ruiz-Castillo, Jose Gomez-Valdes, julio sheinbaum, and Rodolfo Rioja-Nieto. Wind-driven coastal upwelling and westward circulation in the Yucatan shelf. Continental Shelf Research VL - IS - SP - EP - PY - T2 -, 118 IS -:63–76, April 2016. URL: http://www.sciencedirect.com/science/article/pii/S0278434316300644, doi:10.1016/j.csr.2016.02.010.\n32\nPeter Hamilton, Alexis Lugo-Fernández, and julio sheinbaum. A Loop Current experiment: Field and remote measurements. Dynamics of Atmospheres and Oceans VL - IS - SP - EP - PY - T2 -, 76 IS -:156–173, 2016. URL: http://www.sciencedirect.com/science/article/pii/S0377026516300069, doi:10.1016/j.dynatmoce.2016.01.005.\n33\nG A Passalacqua, J Sheinbaum, and J A Martinez. Sea surface temperature influence on a winter cold front position and propagation: air–sea interactions of the \\textquoteleft Nortes\\textquoteright  winds in the Gulf of Mexico. Atmospheric Science Letters, 17(5):302–307, 2016. URL: http://dx.doi.org/10.1002/asl.655, doi:10.1002/asl.655.\n34 Sabrina M Parra, Arnoldo Valle-Levinson, Ismael Mariño Tapia, Cecilia Enriquez, Julio Candela, and julio sheinbaum. Seasonal variability of saltwater intrusion at a point-source submarine groundwater discharge. Limnology and Oceanography, 61(4):1245–1258, 2016. URL: http://dx.doi.org/10.1002/lno.10286, doi:10.1002/lno.10286.\n35\nGabriela Athié, julio sheinbaum, Robert Leben, José Ochoa, Michael R Shannon, and Julio Candela. Trapping of the near-inertial wave wakes of two consecutive hurricanes in the Loop Current. Journal of Geophysical Research-Oceans, 121(10):7431–7454, 2016. URL: http://dx.doi.org/10.1002/2015JC011592, doi:10.1002/2015JC011592.\n36 José Ochoa, H Maske, J Sheinbaum, and J Candela. Diel and lunar cycles of vertical migration extending to below 1000 m in the ocean and the vertical connectivity of depth-tiered populations. Limnology and Oceanography, 58(4):1207–1214, June 2013. URL: http://adsabs.harvard.edu/cgi-bin/nph-data_query?bibcode=2013LimOc..58.1207O&link_type=EJOURNAL, doi:10.4319/lo.2013.58.4.1207.\n37\nJulien Jouanno and julio sheinbaum. Heat Balance and Eddies in the Caribbean Upwelling System. Journal of Physical Oceanography, 43(5):1004–1014, May 2013. URL: http://journals.ametsoc.org/doi/abs/10.1175/JPO-D-12-0140.1, doi:10.1175/JPO-D-12-0140.1.\n38\nF Andrade-Canto, J Sheinbaum, and L Zavala Sansón. A Lagrangian approach to the Loop Current eddy separation. Nonlinear Processes in Geophysics, 20(1):85–96, 2013. URL: http://www.nonlin-processes-geophys.net/20/85/2013/, doi:10.5194/npg-20-85-2013.\n39\nPaula Pérez-Brunius, Paula García-Carrillo, Jean Dubranna, julio sheinbaum, and Julio Candela. Direct observations of the upper layer circulation in the southern Gulf of Mexico. Deep Sea Research Part II: Topical Studies in Oceanography, 85(0 SP - EP - PY - T2 -):182–194, 2013. URL: http://www.sciencedirect.com/science/article/pii/S0967064512001063, doi:doi: 10.1016/j.dsr2.2012.07.020.\n40\nJulien Jouanno, julio sheinbaum, Bernard Barnier, Jean-Marc Molines, and Julio Candela. Seasonal and Interannual Modulation of the Eddy Kinetic Energy in the Caribbean Sea. Journal of Physical Oceanography, 42(11):2041–2055, November 2012. URL: http://journals.ametsoc.org/doi/abs/10.1175/JPO-D-12-048.1, doi:10.1175/JPO-D-12-048.1.\n41\nNicolas Kolodziejczyk, José Ochoa, Julio Candela, and julio sheinbaum. Observations of intermittent deep currents and eddies in the Gulf of Mexico. Journal of Geophysical Research, 117(C9):n/a–n/a, September 2012. URL: http://doi.wiley.com/10.1029/2012JC007890, doi:10.1029/2012JC007890.\n42\nGabriela Athié, Julio Candela, José Ochoa, and julio sheinbaum. Impact of Caribbean cyclones on the detachment of Loop Current anticyclones. Journal of Geophysical Research: Oceans (1978–2012), 117(C3):C03018, 2012. URL: http://dx.doi.org/10.1029/2011JC007090.\n43\nNicolas Kolodziejczyk, José Ochoa, Julio Candela, and julio sheinbaum. Deep Currents in the Bay of Campeche. Journal of Physical Oceanography, 41(10):1902–1920, October 2011. URL: http://journals.ametsoc.org/doi/abs/10.1175/2011JPO4526.1, doi:10.1175/2011JPO4526.1.\n44 José Ochoa, julio sheinbaum, and Aleph Jiménez. Lateral Friction in Reduced-Gravity Models: Parameterizations Consistent with Energy Dissipation and Conservation of Angular Momentum. Journal of Physical Oceanography, 41(10):1894–1901, October 2011. URL: http://journals.ametsoc.org/doi/abs/10.1175/2011JPO4599.1, doi:10.1175/2011JPO4599.1.\n45 Julien Jouanno, Frédéric Marin, Yves du Penhoat, julio sheinbaum, and Jean-Marc Molines. Seasonal heat balance in the upper 100 m of the equatorial Atlantic Ocean. Journal of Geophysical Research, 116(C9):C09003–, September 2011. URL: http://www.agu.org/pubs/crossref/2011/2010JC006912.shtml, doi:10.1029/2010JC006912.\n46\nJulien Jouanno, Frédéric Marin, Yves du Penhoat, Jean-Marc Molines, and julio sheinbaum. Seasonal Modes of Surface Cooling in the Gulf of Guinea. J Phys Oceanogr, 41(7):1408–1416, April 2011. URL: http://dx.doi.org/10.1175/JPO-D-11-031.1, doi:10.1175/JPO-D-11-031.1.\n47\nG Athie, Julio Candela, J Sheinbaum, A Badan, and José Ochoa. Yucatan Current variability through the Cozumel and Yucatan channels. Ciencias Marinas, 37(4A):471–492, 2011. URL: http://rcmarinas.ens.uabc.mx/index.php/cmarinas/article/view/1794/1340, doi:10.7773/cm.v37i4a.1794.\n48\nJulien Jouanno, julio sheinbaum, Bernard Barnier, and Jean-Marc Molines. The mesoscale variability in the Caribbean Sea. Part II: Energy sources. ocean modelling, 26(3–4):226–239, January 2009. URL: http://www.sciencedirect.com/science/article/pii/S1463500308001601.\n49\nDavid Rivas, Antoine Badan, julio sheinbaum, José Ochoa, and Julio Candela. Vertical Velocity and Vertical Heat Flux Observed within Loop Current Eddies in the Central Gulf of Mexico. Journal of Physical Oceanography, 38(11):2461–2481, November 2008. URL: http://journals.ametsoc.org/doi/abs/10.1175/2008JPO3755.1, doi:10.1175/2008JPO3755.1.\n50\nL Zavala Sansón and J Sheinbaum. Elementary properties of the enstrophy and strain fields in confined two-dimensional flows. European Journal of Mechanics-B/Fluids, 2008. URL: http://linkinghub.elsevier.com/retrieve/pii/S0997754607000441.\n51\nM Marín, J Candela, J Sheinbaum, J Ochoa, and A Badan. On the near surface momentum balance in the Yucatán Channel. Geofísica internacional, 47(1):57–75, 2008. URL: http://www.scielo.org.mx/scielo.php?script=sci_arttext&pid=S0016-71692008000100005.\n52\nJulien Jouanno, julio sheinbaum, Bernard Barnier, Jean-Marc Molines, Laurent Debreu, and Florian Lemarié. The mesoscale variability in the Caribbean Sea. Part I: Simulations and characteristics with an embedded model. ocean modelling, 23(3-4):82–101, 2008. URL: http://gateway.webofknowledge.com/gateway/Gateway.cgi?GWVersion=2&SrcAuth=mekentosj&SrcApp=Papers&DestLinkType=FullRecord&DestApp=WOS&KeyUT=000258612800002, doi:10.1016/j.ocemod.2008.04.002.\n53\nC Coronado, J Candela, R Iglesias-Prieto, J Sheinbaum, M López, and F J Ocampo-Torres. On the circulation in the Puerto Morelos fringing reef lagoon. Coral Reefs, 26(1):149–163, February 2007. URL: https://link-springer-com/article/10.1007/s00338-006-0175-9, doi:10.1007/s00338-006-0175-9.\n54 P Cetina, J Candela, J Sheinbaum, J Ochoa, and A Badan. Circulation along the Mexican Caribbean coast. Journal of Geophysical Research, 111(C8):C08021, 2006. URL: http://www.agu.org/pubs/crossref/2006/2005JC003056.shtml, doi:10.1029/2005JC003056.\n55\nI Polo, B R De Fonseca, and J Sheinbaum. Northwest Africa upwelling and the Atlantic climate variability. Geophysical research letters, 32(23):L23702, January 2005. URL: https://agupubs.onlinelibrary.wiley.com/doi/full/10.1029/2005GL023883\n56 José Ochoa, Julio Candela, Antonio Badan, and julio sheinbaum. Ageostrophic fluctuations in Cozumel Channel. Journal of Geophysical Research, 110(C2):C02004, 2005. URL: http://dx.doi.org/10.1029/2004JC002408, doi:10.1029/2004JC002408.\n57 A J Abascal, J Sheinbaum, J Candela, J Ochoa, and A Badan. Analysis of flow variability in the Yucatan Channel. Journal of Geophysical Research, 108(C12):3381, 2003. URL: http://dx.doi.org/10.1029/2003JC001922.\n58 J Sheinbaum. Current theories on El Niño-Southern Oscillation: A review. GEOFISICA INTERNACIONAL-MEXICO-, 42(3):291–306, 2003. URL:http://areas.geofisica.unam.mx/geofisica_internacional/anteriores/2003/03/sheinbaum.pdf\n59\nJulio Candela, Sorayda Tanahara, Michel Crepon, Bernard Barnier, and julio sheinbaum. Yucatan Channel flow: Observations versus CLIPPER ATL6 and MERCATOR PAM models. Journal of Geophysical Research: Oceans (1978–2012), 108(C12):3385, 2003. URL: http://dx.doi.org/10.1029/2003JC001961.\n60 Julio Candela, julio sheinbaum, José Ochoa, Antoine Badan, and Robert Leben. The potential vorticity flux through the Yucatan Channel and the Loop Current in the Gulf of Mexico. Geophysical research letters, 29(22):16–1–16–4, November 2002. URL: https://agupubs-onlinelibrary-wiley-com/doi/full/10.1029/2002GL015587, doi:10.1029/2002GL015587.\n61\nJ Sheinbaum, J Candela, A Badan, and J Ochoa. Flow structure and transport in the Yucatan Channel. Geophysical research letters, 29(3):10–11, January 2002. URL:https://doi.org/10.1029/2001GL013990\n62 L Bunge, J Ochoa, A Badan, J Candela, and J Sheinbaum. Deep flows in the Yucatan Channel and their relation to changes in the Loop Current extension. Journal of Geophysical Research: Oceans (1978–2012), 107(C12):3233, 2002. URL: http://dx.doi.org/10.1029/2001JC001256, doi:10.1029/2001JC001256.\n63\nJosé Ochoa, julio sheinbaum, and Edgar G Pavía. Inhomogeneous rodons. Journal of Geophysical Research, 103(C11):24869–24,880, January 1998. URL: http://www.agu.org/pubs/crossref/1998/98JC02159.shtml, doi:doi:10.1029/98JC02159.\n64 DLT Anderson, J Sheinbaum, and K Haines. Data assimilation in ocean models. Reports on Progress in Physics, 59(10):1209, 1996. URL: http://iopscience.iop.org/0034-4885/59/10/001.\n65\nJ Sheinbaum. Variational assimilation of simulated acoustic tomography data and point observations: A comparative study. Journal of Geophysical Research, 100(C10):20745–20,761, January 1995.\n66\njulio sheinbaum. Variational assimilation of simulated acoustic tomography data and point observations: A comparative study. Journal of Geophysical Research, 100(C10):20745, 1995. URL: http://doi.wiley.com/10.1029/95JC02113, doi:10.1029/95JC02113.\n67\njulio sheinbaum and David L T Anderson. Variational Assimilation of XBT Data. Part 1. J Phys Oceanogr, 20(5):672–688, May 1990. URL: http://journals.ametsoc.org/doi/abs/10.1175/1520-0485%281990%29020%3C0672%3AVAOXDP%3E2.0.CO%3B2, doi:10.1175/1520-0485(1990)020&lt;0672:VAOXDP&gt;2.0.CO;2.\n68 julio sheinbaum and David L T Anderson. Variational Assimilation of XBT Data. Part 1. Journal of Physical Oceanography, 20(5):672–688, May 1990. URL: http://journals.ametsoc.org/doi/abs/10.1175/1520-0485%281990%29020%3C0672%3AVAOXDP%3E2.0.CO%3B2, doi:10.1175/1520-0485(1990)020&lt;0672:VAOXDP&gt;2.0.CO;2.\n69\njulio sheinbaum and David L T Anderson. Variational Assimilation of XBT Data. Part II. Sensitivity Studies and Use of Smoothing Constraints. Journal of Physical Oceanography, 20(5):689–704, May 1990. URL: http://journals.ametsoc.org/doi/abs/10.1175/1520-0485%281990%29020%3C0689%3AVAOXDP%3E2.0.CO%3B2, doi:10.1175/1520-0485(1990)020&lt;0689:VAOXDP&gt;2.0.CO;2.\n70\nRoberto Hojman, Sergio Hojman, and julio sheinbaum. Shortcut for constructing any Lagrangian from its equations of motion. Physical Review D, 28(6):1333–1336, September 1983. URL: http://link.aps.org/doi/10.1103/PhysRevD.28.1333, doi:10.1103/PhysRevD.28.1333.\n\n\n\n\n2004 Nonlinear Process in Geophysics\n2009 The Ocean, the Wine and the Valley"
  },
  {
    "objectID": "cvprod.html#publications",
    "href": "cvprod.html#publications",
    "title": "CV-PRODUCTS",
    "section": "",
    "text": "0.1 Tania Reyes-Jiménez, Gabriela Athié, Cecilia Enriquez, Julio Sheinbaum, Ismael Mariño-Tapia, Mark Marín-Hernández, David Salas-Monreal, Julio Candela. (2023).Triggering mechanisms of the Yucatan upwelling, Continental Shelf Research, Volume 255,104910,ISSN 0278-4343, https://doi.org/10.1016/j.csr.2022.104910.\n0.2 Amon, R. M. W., Ochoa, J., Candela, J., Herzka, S. Z., Pérez-Brunius, P., Sheinbaum, J., … Molodtsov, S. (2023). Ventilation of the deep Gulf of Mexico and potential insights to the Atlantic Meridional Overturning Circulation. Science Advances, 9(11). https://doi.org/10.1126/sciadv.ade1685\n0.3 Vazquez, H. J., G. Gopalakrishnan, and J. Sheinbaum, 2023: Impact of Yucatan Channel Subsurface Velocity Observations on the Gulf of Mexico State Estimates. J. Phys. Oceanogr., 53, 361–385, https://doi.org/10.1175/JPO-D-21-0213.1.\n1 A.E. Romo-Curiel,Z. Ramírez-Mendoza,A. Fajardo-Yamamoto, M.R. Ramírez-León, M.C. García-Aguilar, S.Z. Herzka,P. Pérez-Brunius,L.E. Saldaña-Ruiz, J. Sheinbaum, K. Kotzakoulakis, J. Rodríguez-Outerelo, F. Medrano and O. Sosa-Nishizaki, (2022). Assessing the exposure risk of large pelagic fish to oil spills scenarios in the deep waters of the Gulf of Mexico. Marine Pollution Bulletin, 176, p.113434, https://doi.org/10.1016/j.marpolbul.2022.113434\n2 Damien, P., Sheinbaum Pardo, J., Pasqueron De Fommervault, O., Jouanno, J., Linacre Rojas , L., & Duteil, O. (2021). Do Loop Current eddies stimulate productivity in the Gulf of Mexico?. Biogeosciences, 18(14), 4281-4303. doi: 10.5194/bg-18-4281-2021\n3 Jouanno, J., Benshila, R., Berline, L., Soulié, A., Radenac, R. H., Morvan, G., Diaz, F., Sheinbaum Pardo, J., Chevalier, C., Thibaut, T., Changeux, T., Menard, F., Berthet, S., Aumont, O., Ethé, C., Nabat, P., & Mallet, M. (2021). A NEMO-based model of Sargassum distribution in the tropical Atlantic: Description of the model and sensitivity analysis (NEMO-Sarg1.0). Geoscientific Model Development, 14(6), 4069-4086. doi: 10.5194/gmd-14-4069-2021.\n4 Ursella, L., Pensieri, S., Pallás Sanz, E., Herzka Llona, S. Z., Bozzano, R., Costa de Almeida Tenreiro, M. J., Cardin, V., Candela Pérez, J., & Sheinbaum Pardo, J. (2021). Diel, lunar and seasonal vertical migration in the deep western Gulf of Mexico evidenced from a long-term data series of acoustic backscatter. Progress in Oceanography, 195. doi: 10.1016/j.pocean.2021.102562.\n5 Ochoa de la Torre, J. L., Ferreira Bartrina, V., Candela Pérez, J., Sheinbaum Pardo, J., López Mariscal, J. M., Pérez Brunius, P., Herzka Llona, S. Z., & Amon, R. (2021). Deep-Water Warming in the Gulf of Mexico from 2003 to 2019. Journal of Physical Oceanography, 51(4), 1021-1035. doi: 10.1175/JPO-D-19-0295.1.\n6 Jouanno, J., Moquet, J. S., Berline, L., Radenac, R. H., Santini, W., Changeux, T., Thibaut, T., Podlejski, W., Menard, F., Martinez, J. M., Aumont, O., Sheinbaum Pardo, J., Filizola, N., & Moukandi N’Kaya, G. (2021). Evolution of the riverine nutrient export to the Tropical Atlantic over the last 15 years: is there a link with Sargassum proliferation?. Environmental Research Letters, 16(3), 034042 1-14. doi: 10.1088/1748-9326/abe11a.\n7 Guerrero Moreno, L., Sheinbaum Pardo, J., Mariño Tapia, I., González Rejón, J. J., & Pérez Brunius, P. (2020). Influence of mesoscale eddies on cross-shelf exchange in the western Gulf of Mexico. Continental Shelf Research, 209(12), 104243. doi: 10.1016/j.csr.2020.104243.\n8 Ochoa de la Torre, J. L., Badan Dangon, A. R. F., Sheinbaum Pardo, J., & Castro López, J. (2020). `Preferred Trajectories¿ defined by mass and potential vorticity conservation. Geofísica Internacional, 59(3), 195-207. doi: 10.22201/igeof.00167169p.2020.59.3.2094.\n8.5 Maslo, A., Azevedo Correia de Souza, J. M., & Sheinbaum Pardo, J. (2020). Energetics of the Deep Gulf of Mexico. Journal of Physical Oceanography, 50(6), 1655-1675. doi: 10.1175/JPO-D-19-0308.1.\n9\nSheila N Estrada-Allis, Julio Sheinbaum Pardo, Joao M Azevedo Correia de Souza, Cecilia Elizabeth Enríquez Ortiz, Ismael Mariño Tapia, and Jorge A Herrera-Silveira. Dissolved inorganic nitrogen and particulate organic nitrogen budget in the Yucatán shelf: driving mechanisms through a physical–biogeochemical coupled model. Biogeosciences, 17(4):1087–1111, February 2020. URL: [https://www.biogeosciences.net/17/1087/2020/ 2020 Estrada-Allis.pdf:doi:10.5194/bg-17-1087-2020.\n10 Thomas Meunier, julio sheinbaum, Enric Pallàs Sanz, Miguel Tenreiro, José Ochoa, Angel Ruiz Angulo, Xavier Carton, and Charly de Marez. Heat Content Anomaly and Decay of Warm-Core Rings: the Case of the Gulf of Mexico. Geophysical research letters, 47(3):21, February 2020. URL: https://agupubs.onlinelibrary.wiley.com/doi/full/10.1029/2019GL085600, doi:10.1029/2019GL085600.\n11\nGabriela Athié, julio sheinbaum, Julio Candela, José Ochoa, Paula Pérez-Brunius, and Angélica Romero-Arteaga. Seasonal Variability of the Transport through the Yucatan Channel from Observations. J Phys Oceanogr, 50(2):343–360, February 2020. URL: http://journals.ametsoc.org/doi/10.1175/JPO-D-18-0269.1, doi:10.1175/JPO-D-18-0269.1.\n12 Olaf Duteil, Pierre Damien, julio sheinbaum, and Marlene Spinner. Ocean currents and coastal exposure to offshore releases of passively transported material in the Gulf of Mexico. Environmental Research Communications, 1(8):081006, September 2019. URL: https://iopscience.iop.org/article/10.1088/2515-7620/ab3aad, doi:10.1088/2515-7620/ab3aad.\n13 J Candela, J Ochoa, J Sheinbaum, M López, P Pérez-Brunius, M Tenreiro, E Pallàs-Sanz, G Athie, and L Arriaza-Oliveros. The Flow through the Gulf of Mexico. J Phys Oceanogr, 49(6):1381–1401, June 2019. URL: http://journals.ametsoc.org/doi/10.1175/JPO-D-18-0189.1, doi:10.1175/JPO-D-18-0189.1.\n14\nMatt K Gough, Francisco J Beron-Vera, María J Olascoaga, julio sheinbaum, Julien Jouanno, and Rodrigo Duran. Persistent Lagrangian Transport Patterns in the Northwestern Gulf of Mexico. J Phys Oceanogr, 49(2):353–367, December 2018. URL: https://doi.org/10.1175/JPO-D-17-0207.1, doi:10.1175/JPO-D-17-0207.1.\n15\nP Miron, F J Beron-Vera, M J Olascoaga, G Froyland, P Pérez-Brunius, and J Sheinbaum. Lagrangian Geography of the Deep Gulf of Mexico. J Phys Oceanogr, 49(1):269–290, November 2018. URL: https://doi.org/10.1175/JPO-D-18-0073.1, doi:10.1175/JPO-D-18-0073.1.\n16 Pierre Damien, Orens Pasqueron de Fommervault, julio sheinbaum, Julien Jouanno, Victor F Camacho-Ibar, and Olaf Duteil. Partitioning of the Open Waters of the Gulf of Mexico Based on the Seasonal and Interannual Variability of Chlorophyll Concentration. Journal of Geophysical Research-Oceans, 115(1):85, April 2018. URL: http://doi.wiley.com/10.1002/2017JC013456, doi:10.1002/2017JC013456.\n17 M Tenreiro, Julio Candela, julio sheinbaum, Enric Pallàs Sanz, and José Ochoa. Near-Surface and Deep Circulation Coupling in the Western Gulf of Mexico. Journal of Physical Oceanography, 48(1):145–161, 2018. URL: https://journals.ametsoc.org/doi/abs/10.1175/JPO-D-17-0018.1, doi:10.1175/JPO-D-17-0018.1.\n18\nJouanno J, Pallàs Sanz E, and Sheinbaum J. Variability and Dynamics of the Yucatan Upwelling: High-Resolution Simulations. Journal of Geophysical Research-Oceans, 123(2):1251–1262, 2018. URL: https://agupubs.onlinelibrary.wiley.com/doi/abs/10.1002/2017JC013535, doi:10.1002/2017JC013535.\n19\nOrens Pasqueron de Fommervault, Paula Pérez-Brunius, Pierre Damien, Victor F Camacho-Ibar, and julio sheinbaum. Temporal variability of chlorophyll distribution in the Gulf of Mexico: bio-optical data from profiling floats. Biogeosciences, 14(24):5647–5662, December 2017. URL: https://www.biogeosciences.net/14/5647/2017/, doi:10.5194/bg-14-5647-2017.\n20\nP Miron, F J Beron-Vera, M J Olascoaga, J Sheinbaum, P Pérez-Brunius, and G Froyland. Lagrangian dynamical geography of the Gulf of Mexico. Scientific Reports, 7(1):7021, August 2017. URL: http://www.nature.com/articles/s41598-017-07177-w, doi:10.1038/s41598-017-07177-w.\n21 P Miron, F J Beron-Vera, M J Olascoaga, J Sheinbaum, P Pérez-Brunius, and G Froyland. Lagrangian dynamical geography of the Gulf of Mexico. Scientific Reports, 7(1):7021, August 2017. URL: http://www.nature.com/articles/s41598-017-07177-w, doi:10.1038/s41598-017-07177-w.\n22\nLuis Zavala Sansón, Paula Pérez-Brunius, and julio sheinbaum. Surface Relative Dispersion in the Southwestern Gulf of Mexico. J Phys Oceanogr, 47(2):387–403, February 2017. URL: http://journals.ametsoc.org/doi/10.1175/JPO-D-16-0105.1, doi:10.1175/JPO-D-16-0105.1.\n23\nLuis Zavala Sansón, Paula Pérez-Brunius, and julio sheinbaum. Surface Relative Dispersion in the Southwestern Gulf of Mexico. J Phys Oceanogr, 47(2):387–403, February 2017. URL: http://journals.ametsoc.org/doi/10.1175/JPO-D-16-0105.1, doi:10.1175/JPO-D-16-0105.1.\n24\nL Zavala Sansón, P Pérez-Brunius, and J Sheinbaum. Point source dispersion of surface drifters in the southern Gulf of Mexico. Environmental Research Letters, 12(2):024006, 2017. URL: http://stacks.iop.org/1748-9326/12/i=2/a=024006.\n25 Enric Pallàs-Sanz, Julio Candela, julio sheinbaum, and José Ochoa. Mooring observations of the near-inertial wave wake of Hurricane Ida (2009). Dynamics of Atmospheres and Oceans VL - IS - SP - EP - PY - T2 -, 76, Part 2 IS -:325–344, December 2016. URL: http://www.sciencedirect.com/science/article/pii/S0377026516300355, doi:10.1016/j.dynatmoce.2016.05.003.\n26\njulio sheinbaum, Gabriela Athié, Julio Candela, José Ochoa, and Angélica Romero-Arteaga. Structure and variability of the Yucatan and loop currents along the slope and shelf break of the Yucatan channel and Campeche bank. Dynamics of Atmospheres and Oceans VL - IS - SP - EP - PY - T2 -, 76, Part 2 IS -:217–239, December 2016. URL: http://www.sciencedirect.com/science/article/pii/S0377026516300501.\n27\nAlejandro Adem, Omar Antolín Camarena, Gordon W Semenoff, and Daniel Sheinbaum. Topology of Fermi surfaces and anomaly inflows. Journal of High Energy Physics, 2016(11):83, November 2016. URL: http://link.springer.com/10.1007/JHEP11(2016)083, doi:10.1007/JHEP11(2016)083.\n28\nJulien Jouanno, José Ochoa, Enric Pallàs-Sanz, julio sheinbaum, Fernando Andrade-Canto, Julio Candela, and Jean-Marc Molines. Loop Current Frontal Eddies: Formation along the Campeche Bank and Impact of Coastally Trapped Waves. Journal of Physical Oceanography, 46(11):3339–3363, November 2016. URL: http://journals.ametsoc.org/doi/10.1175/JPO-D-16-0052.1, doi:10.1175/JPO-D-16-0052.1.\n29\nMaximo Garcia-Jove Navarro, julio sheinbaum, and Julien Jouanno. Sensitivity of Loop Current metrics and eddy detachments to different model configurations: The impact of topography and Caribbean perturbations. ATMOSFERA, 29(3):235–265, June 2016. URL: http://www.revistascca.unam.mx/atm/index.php/atm/article/view/ATM.2016.29.03.05/46565.\n30\nSabrina M Parra, Arnoldo Valle-Levinson, Ismael Mariño Tapia, Cecilia Enriquez, Julio Candela, and julio sheinbaum. Seasonal variability of saltwater intrusion at a point-source submarine groundwater discharge. Limnology and Oceanography, 61(4):1245–1258, May 2016. URL: http://doi.wiley.com/10.1002/lno.10286, doi:10.1002/lno.10286.\n31\nEugenio Ruiz-Castillo, Jose Gomez-Valdes, julio sheinbaum, and Rodolfo Rioja-Nieto. Wind-driven coastal upwelling and westward circulation in the Yucatan shelf. Continental Shelf Research VL - IS - SP - EP - PY - T2 -, 118 IS -:63–76, April 2016. URL: http://www.sciencedirect.com/science/article/pii/S0278434316300644, doi:10.1016/j.csr.2016.02.010.\n32\nPeter Hamilton, Alexis Lugo-Fernández, and julio sheinbaum. A Loop Current experiment: Field and remote measurements. Dynamics of Atmospheres and Oceans VL - IS - SP - EP - PY - T2 -, 76 IS -:156–173, 2016. URL: http://www.sciencedirect.com/science/article/pii/S0377026516300069, doi:10.1016/j.dynatmoce.2016.01.005.\n33\nG A Passalacqua, J Sheinbaum, and J A Martinez. Sea surface temperature influence on a winter cold front position and propagation: air–sea interactions of the \\textquoteleft Nortes\\textquoteright  winds in the Gulf of Mexico. Atmospheric Science Letters, 17(5):302–307, 2016. URL: http://dx.doi.org/10.1002/asl.655, doi:10.1002/asl.655.\n34 Sabrina M Parra, Arnoldo Valle-Levinson, Ismael Mariño Tapia, Cecilia Enriquez, Julio Candela, and julio sheinbaum. Seasonal variability of saltwater intrusion at a point-source submarine groundwater discharge. Limnology and Oceanography, 61(4):1245–1258, 2016. URL: http://dx.doi.org/10.1002/lno.10286, doi:10.1002/lno.10286.\n35\nGabriela Athié, julio sheinbaum, Robert Leben, José Ochoa, Michael R Shannon, and Julio Candela. Trapping of the near-inertial wave wakes of two consecutive hurricanes in the Loop Current. Journal of Geophysical Research-Oceans, 121(10):7431–7454, 2016. URL: http://dx.doi.org/10.1002/2015JC011592, doi:10.1002/2015JC011592.\n36 José Ochoa, H Maske, J Sheinbaum, and J Candela. Diel and lunar cycles of vertical migration extending to below 1000 m in the ocean and the vertical connectivity of depth-tiered populations. Limnology and Oceanography, 58(4):1207–1214, June 2013. URL: http://adsabs.harvard.edu/cgi-bin/nph-data_query?bibcode=2013LimOc..58.1207O&link_type=EJOURNAL, doi:10.4319/lo.2013.58.4.1207.\n37\nJulien Jouanno and julio sheinbaum. Heat Balance and Eddies in the Caribbean Upwelling System. Journal of Physical Oceanography, 43(5):1004–1014, May 2013. URL: http://journals.ametsoc.org/doi/abs/10.1175/JPO-D-12-0140.1, doi:10.1175/JPO-D-12-0140.1.\n38\nF Andrade-Canto, J Sheinbaum, and L Zavala Sansón. A Lagrangian approach to the Loop Current eddy separation. Nonlinear Processes in Geophysics, 20(1):85–96, 2013. URL: http://www.nonlin-processes-geophys.net/20/85/2013/, doi:10.5194/npg-20-85-2013.\n39\nPaula Pérez-Brunius, Paula García-Carrillo, Jean Dubranna, julio sheinbaum, and Julio Candela. Direct observations of the upper layer circulation in the southern Gulf of Mexico. Deep Sea Research Part II: Topical Studies in Oceanography, 85(0 SP - EP - PY - T2 -):182–194, 2013. URL: http://www.sciencedirect.com/science/article/pii/S0967064512001063, doi:doi: 10.1016/j.dsr2.2012.07.020.\n40\nJulien Jouanno, julio sheinbaum, Bernard Barnier, Jean-Marc Molines, and Julio Candela. Seasonal and Interannual Modulation of the Eddy Kinetic Energy in the Caribbean Sea. Journal of Physical Oceanography, 42(11):2041–2055, November 2012. URL: http://journals.ametsoc.org/doi/abs/10.1175/JPO-D-12-048.1, doi:10.1175/JPO-D-12-048.1.\n41\nNicolas Kolodziejczyk, José Ochoa, Julio Candela, and julio sheinbaum. Observations of intermittent deep currents and eddies in the Gulf of Mexico. Journal of Geophysical Research, 117(C9):n/a–n/a, September 2012. URL: http://doi.wiley.com/10.1029/2012JC007890, doi:10.1029/2012JC007890.\n42\nGabriela Athié, Julio Candela, José Ochoa, and julio sheinbaum. Impact of Caribbean cyclones on the detachment of Loop Current anticyclones. Journal of Geophysical Research: Oceans (1978–2012), 117(C3):C03018, 2012. URL: http://dx.doi.org/10.1029/2011JC007090.\n43\nNicolas Kolodziejczyk, José Ochoa, Julio Candela, and julio sheinbaum. Deep Currents in the Bay of Campeche. Journal of Physical Oceanography, 41(10):1902–1920, October 2011. URL: http://journals.ametsoc.org/doi/abs/10.1175/2011JPO4526.1, doi:10.1175/2011JPO4526.1.\n44 José Ochoa, julio sheinbaum, and Aleph Jiménez. Lateral Friction in Reduced-Gravity Models: Parameterizations Consistent with Energy Dissipation and Conservation of Angular Momentum. Journal of Physical Oceanography, 41(10):1894–1901, October 2011. URL: http://journals.ametsoc.org/doi/abs/10.1175/2011JPO4599.1, doi:10.1175/2011JPO4599.1.\n45 Julien Jouanno, Frédéric Marin, Yves du Penhoat, julio sheinbaum, and Jean-Marc Molines. Seasonal heat balance in the upper 100 m of the equatorial Atlantic Ocean. Journal of Geophysical Research, 116(C9):C09003–, September 2011. URL: http://www.agu.org/pubs/crossref/2011/2010JC006912.shtml, doi:10.1029/2010JC006912.\n46\nJulien Jouanno, Frédéric Marin, Yves du Penhoat, Jean-Marc Molines, and julio sheinbaum. Seasonal Modes of Surface Cooling in the Gulf of Guinea. J Phys Oceanogr, 41(7):1408–1416, April 2011. URL: http://dx.doi.org/10.1175/JPO-D-11-031.1, doi:10.1175/JPO-D-11-031.1.\n47\nG Athie, Julio Candela, J Sheinbaum, A Badan, and José Ochoa. Yucatan Current variability through the Cozumel and Yucatan channels. Ciencias Marinas, 37(4A):471–492, 2011. URL: http://rcmarinas.ens.uabc.mx/index.php/cmarinas/article/view/1794/1340, doi:10.7773/cm.v37i4a.1794.\n48\nJulien Jouanno, julio sheinbaum, Bernard Barnier, and Jean-Marc Molines. The mesoscale variability in the Caribbean Sea. Part II: Energy sources. ocean modelling, 26(3–4):226–239, January 2009. URL: http://www.sciencedirect.com/science/article/pii/S1463500308001601.\n49\nDavid Rivas, Antoine Badan, julio sheinbaum, José Ochoa, and Julio Candela. Vertical Velocity and Vertical Heat Flux Observed within Loop Current Eddies in the Central Gulf of Mexico. Journal of Physical Oceanography, 38(11):2461–2481, November 2008. URL: http://journals.ametsoc.org/doi/abs/10.1175/2008JPO3755.1, doi:10.1175/2008JPO3755.1.\n50\nL Zavala Sansón and J Sheinbaum. Elementary properties of the enstrophy and strain fields in confined two-dimensional flows. European Journal of Mechanics-B/Fluids, 2008. URL: http://linkinghub.elsevier.com/retrieve/pii/S0997754607000441.\n51\nM Marín, J Candela, J Sheinbaum, J Ochoa, and A Badan. On the near surface momentum balance in the Yucatán Channel. Geofísica internacional, 47(1):57–75, 2008. URL: http://www.scielo.org.mx/scielo.php?script=sci_arttext&pid=S0016-71692008000100005.\n52\nJulien Jouanno, julio sheinbaum, Bernard Barnier, Jean-Marc Molines, Laurent Debreu, and Florian Lemarié. The mesoscale variability in the Caribbean Sea. Part I: Simulations and characteristics with an embedded model. ocean modelling, 23(3-4):82–101, 2008. URL: http://gateway.webofknowledge.com/gateway/Gateway.cgi?GWVersion=2&SrcAuth=mekentosj&SrcApp=Papers&DestLinkType=FullRecord&DestApp=WOS&KeyUT=000258612800002, doi:10.1016/j.ocemod.2008.04.002.\n53\nC Coronado, J Candela, R Iglesias-Prieto, J Sheinbaum, M López, and F J Ocampo-Torres. On the circulation in the Puerto Morelos fringing reef lagoon. Coral Reefs, 26(1):149–163, February 2007. URL: https://link-springer-com/article/10.1007/s00338-006-0175-9, doi:10.1007/s00338-006-0175-9.\n54 P Cetina, J Candela, J Sheinbaum, J Ochoa, and A Badan. Circulation along the Mexican Caribbean coast. Journal of Geophysical Research, 111(C8):C08021, 2006. URL: http://www.agu.org/pubs/crossref/2006/2005JC003056.shtml, doi:10.1029/2005JC003056.\n55\nI Polo, B R De Fonseca, and J Sheinbaum. Northwest Africa upwelling and the Atlantic climate variability. Geophysical research letters, 32(23):L23702, January 2005. URL: https://agupubs.onlinelibrary.wiley.com/doi/full/10.1029/2005GL023883\n56 José Ochoa, Julio Candela, Antonio Badan, and julio sheinbaum. Ageostrophic fluctuations in Cozumel Channel. Journal of Geophysical Research, 110(C2):C02004, 2005. URL: http://dx.doi.org/10.1029/2004JC002408, doi:10.1029/2004JC002408.\n57 A J Abascal, J Sheinbaum, J Candela, J Ochoa, and A Badan. Analysis of flow variability in the Yucatan Channel. Journal of Geophysical Research, 108(C12):3381, 2003. URL: http://dx.doi.org/10.1029/2003JC001922.\n58 J Sheinbaum. Current theories on El Niño-Southern Oscillation: A review. GEOFISICA INTERNACIONAL-MEXICO-, 42(3):291–306, 2003. URL:http://areas.geofisica.unam.mx/geofisica_internacional/anteriores/2003/03/sheinbaum.pdf\n59\nJulio Candela, Sorayda Tanahara, Michel Crepon, Bernard Barnier, and julio sheinbaum. Yucatan Channel flow: Observations versus CLIPPER ATL6 and MERCATOR PAM models. Journal of Geophysical Research: Oceans (1978–2012), 108(C12):3385, 2003. URL: http://dx.doi.org/10.1029/2003JC001961.\n60 Julio Candela, julio sheinbaum, José Ochoa, Antoine Badan, and Robert Leben. The potential vorticity flux through the Yucatan Channel and the Loop Current in the Gulf of Mexico. Geophysical research letters, 29(22):16–1–16–4, November 2002. URL: https://agupubs-onlinelibrary-wiley-com/doi/full/10.1029/2002GL015587, doi:10.1029/2002GL015587.\n61\nJ Sheinbaum, J Candela, A Badan, and J Ochoa. Flow structure and transport in the Yucatan Channel. Geophysical research letters, 29(3):10–11, January 2002. URL:https://doi.org/10.1029/2001GL013990\n62 L Bunge, J Ochoa, A Badan, J Candela, and J Sheinbaum. Deep flows in the Yucatan Channel and their relation to changes in the Loop Current extension. Journal of Geophysical Research: Oceans (1978–2012), 107(C12):3233, 2002. URL: http://dx.doi.org/10.1029/2001JC001256, doi:10.1029/2001JC001256.\n63\nJosé Ochoa, julio sheinbaum, and Edgar G Pavía. Inhomogeneous rodons. Journal of Geophysical Research, 103(C11):24869–24,880, January 1998. URL: http://www.agu.org/pubs/crossref/1998/98JC02159.shtml, doi:doi:10.1029/98JC02159.\n64 DLT Anderson, J Sheinbaum, and K Haines. Data assimilation in ocean models. Reports on Progress in Physics, 59(10):1209, 1996. URL: http://iopscience.iop.org/0034-4885/59/10/001.\n65\nJ Sheinbaum. Variational assimilation of simulated acoustic tomography data and point observations: A comparative study. Journal of Geophysical Research, 100(C10):20745–20,761, January 1995.\n66\njulio sheinbaum. Variational assimilation of simulated acoustic tomography data and point observations: A comparative study. Journal of Geophysical Research, 100(C10):20745, 1995. URL: http://doi.wiley.com/10.1029/95JC02113, doi:10.1029/95JC02113.\n67\njulio sheinbaum and David L T Anderson. Variational Assimilation of XBT Data. Part 1. J Phys Oceanogr, 20(5):672–688, May 1990. URL: http://journals.ametsoc.org/doi/abs/10.1175/1520-0485%281990%29020%3C0672%3AVAOXDP%3E2.0.CO%3B2, doi:10.1175/1520-0485(1990)020&lt;0672:VAOXDP&gt;2.0.CO;2.\n68 julio sheinbaum and David L T Anderson. Variational Assimilation of XBT Data. Part 1. Journal of Physical Oceanography, 20(5):672–688, May 1990. URL: http://journals.ametsoc.org/doi/abs/10.1175/1520-0485%281990%29020%3C0672%3AVAOXDP%3E2.0.CO%3B2, doi:10.1175/1520-0485(1990)020&lt;0672:VAOXDP&gt;2.0.CO;2.\n69\njulio sheinbaum and David L T Anderson. Variational Assimilation of XBT Data. Part II. Sensitivity Studies and Use of Smoothing Constraints. Journal of Physical Oceanography, 20(5):689–704, May 1990. URL: http://journals.ametsoc.org/doi/abs/10.1175/1520-0485%281990%29020%3C0689%3AVAOXDP%3E2.0.CO%3B2, doi:10.1175/1520-0485(1990)020&lt;0689:VAOXDP&gt;2.0.CO;2.\n70\nRoberto Hojman, Sergio Hojman, and julio sheinbaum. Shortcut for constructing any Lagrangian from its equations of motion. Physical Review D, 28(6):1333–1336, September 1983. URL: http://link.aps.org/doi/10.1103/PhysRevD.28.1333, doi:10.1103/PhysRevD.28.1333.\n\n\n\n\n2004 Nonlinear Process in Geophysics\n2009 The Ocean, the Wine and the Valley"
  },
  {
    "objectID": "bickley_julia.html",
    "href": "bickley_julia.html",
    "title": "Bickley Jet from CoherentStructures.jl",
    "section": "",
    "text": "Example taken from https://github.com/CoherentStructures/CoherentStructures.jl\n\n\nCode\n#using Distributed\n#nprocs() == 1 && addprocs()\n#@everywhere begin\n          import Pkg\n          Pkg.activate(\"/Users/julios/JULIA/CoherentStructures_julia1.9.jl/\")\n#       end\n\n#@everywhere \nusing CoherentStructures, StreamMacros\nconst bickley = @velo_from_stream psi begin\n    psi  = psi₀ + psi₁\n    psi₀ = - U₀ * L₀ * tanh(y / L₀)\n    psi₁ =   U₀ * L₀ * sech(y / L₀)^2 * re_sum_term\n\n    re_sum_term = Σ₁ + Σ₂ + Σ₃\n\n    Σ₁ = ε₁ * cos(k₁*(x - c₁*t))\n    Σ₂ = ε₂ * cos(k₂*(x - c₂*t))\n    Σ₃ = ε₃ * cos(k₃*(x - c₃*t))\n\n    k₁ = 2/r₀    ; k₂ = 4/r₀   ; k₃ = 6/r₀\n    ε₁ = 0.0075  ; ε₂ = 0.15   ; ε₃ = 0.3\n    c₂ = 0.205U₀ ; c₃ = 0.461U₀; c₁ = c₃ + (√5-1)*(c₂-c₃)\n    U₀ = 62.66e-6; L₀ = 1770e-3; r₀ = 6371e-3\nend\n\n#@everywhere \nusing OrdinaryDiffEq, Tensors\nq = 81\nconst tspan = range(0., stop=3456000., length=q)\nny = 61\nnx = (22ny) ÷ 6\nxmin, xmax, ymin, ymax = 0.0 - 2.0, 6.371π + 2.0, -3.0, 3.0\nxspan = range(xmin, stop=xmax, length=nx)\nyspan = range(ymin, stop=ymax, length=ny)\nP = tuple.(xspan, yspan')\nconst δ = 1.e-6\nconst D = SymmetricTensor{2,2}((2., 0., 1/2))\nmCG_tensor = u -&gt; av_weighted_CG_tensor(bickley, u, tspan, δ; D=(_ -&gt; D), tolerance=1e-6, solver=Tsit5())\n\n#C̅ = pmap(mCG_tensor, P; batch_size=ceil(Int, length(P)/nprocs()^2))\nC̅ = map(mCG_tensor, P)\np = LCSParameters(2.0)\nvortices, singularities = ellipticLCS(C̅, xspan, yspan, p)\n\nusing Plots\ntrace = tensor_invariants(C̅)[5]\nfig = plot_vortices(vortices, singularities, (xmin, ymin), (xmax, ymax);\n    bg=trace, xspan=xspan, yspan=yspan, title=\"DBS field and transport barriers\", showlabel=false)\nPlots.plot(fig)\n\n\nDetecting vortices 100%|█████████████████████████████████| Time: 0:00:17\n  num_barriers:  7\n[ Info: Found 7 elliptic barriers in total.\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\nFigure 1: Stream Plot"
  },
  {
    "objectID": "bickley_julia.html#test-bickley-jet-not-distributed",
    "href": "bickley_julia.html#test-bickley-jet-not-distributed",
    "title": "Bickley Jet from CoherentStructures.jl",
    "section": "",
    "text": "Example taken from https://github.com/CoherentStructures/CoherentStructures.jl\n\n\nCode\n#using Distributed\n#nprocs() == 1 && addprocs()\n#@everywhere begin\n          import Pkg\n          Pkg.activate(\"/Users/julios/JULIA/CoherentStructures_julia1.9.jl/\")\n#       end\n\n#@everywhere \nusing CoherentStructures, StreamMacros\nconst bickley = @velo_from_stream psi begin\n    psi  = psi₀ + psi₁\n    psi₀ = - U₀ * L₀ * tanh(y / L₀)\n    psi₁ =   U₀ * L₀ * sech(y / L₀)^2 * re_sum_term\n\n    re_sum_term = Σ₁ + Σ₂ + Σ₃\n\n    Σ₁ = ε₁ * cos(k₁*(x - c₁*t))\n    Σ₂ = ε₂ * cos(k₂*(x - c₂*t))\n    Σ₃ = ε₃ * cos(k₃*(x - c₃*t))\n\n    k₁ = 2/r₀    ; k₂ = 4/r₀   ; k₃ = 6/r₀\n    ε₁ = 0.0075  ; ε₂ = 0.15   ; ε₃ = 0.3\n    c₂ = 0.205U₀ ; c₃ = 0.461U₀; c₁ = c₃ + (√5-1)*(c₂-c₃)\n    U₀ = 62.66e-6; L₀ = 1770e-3; r₀ = 6371e-3\nend\n\n#@everywhere \nusing OrdinaryDiffEq, Tensors\nq = 81\nconst tspan = range(0., stop=3456000., length=q)\nny = 61\nnx = (22ny) ÷ 6\nxmin, xmax, ymin, ymax = 0.0 - 2.0, 6.371π + 2.0, -3.0, 3.0\nxspan = range(xmin, stop=xmax, length=nx)\nyspan = range(ymin, stop=ymax, length=ny)\nP = tuple.(xspan, yspan')\nconst δ = 1.e-6\nconst D = SymmetricTensor{2,2}((2., 0., 1/2))\nmCG_tensor = u -&gt; av_weighted_CG_tensor(bickley, u, tspan, δ; D=(_ -&gt; D), tolerance=1e-6, solver=Tsit5())\n\n#C̅ = pmap(mCG_tensor, P; batch_size=ceil(Int, length(P)/nprocs()^2))\nC̅ = map(mCG_tensor, P)\np = LCSParameters(2.0)\nvortices, singularities = ellipticLCS(C̅, xspan, yspan, p)\n\nusing Plots\ntrace = tensor_invariants(C̅)[5]\nfig = plot_vortices(vortices, singularities, (xmin, ymin), (xmax, ymax);\n    bg=trace, xspan=xspan, yspan=yspan, title=\"DBS field and transport barriers\", showlabel=false)\nPlots.plot(fig)\n\n\nDetecting vortices 100%|█████████████████████████████████| Time: 0:00:17\n  num_barriers:  7\n[ Info: Found 7 elliptic barriers in total.\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\nFigure 1: Stream Plot"
  },
  {
    "objectID": "article2_new.html",
    "href": "article2_new.html",
    "title": "Julio Sheinbaum",
    "section": "",
    "text": "m– title: “Notas Curso Análisis de datos” format: pdf: agu-pdf: keep-tex: true agu-html: default author: - name: Enric Pallas affiliations: - name: Centro de Investigación Científica y de Educación Superior de Ensenada, CICESE department: Physical Oceanography, address: Carretera Ensenada-Tijuana 3018 city: Ensenada region: Baja California country: MEXICO postal-code: 22860 orcid: 0000-0001-000-000 email: epallas@cicese.mx url: https://cicese.edu.mx/~epallas - name: Julio Sheinbaum affiliations: - name: Centro de Investigación Científica y de Educación Superior de Ensenada, CICESE department: Physical Oceanography, address: Carretera Ensenada-Tijuana 3018 city: Ensenada region: Baja California country: MEXICO postal-code: 22860 orcid: 0000-0001-7031-5225 email: julios@cicese.mx url: https://jsheinbaum.github.io acknowledgements: Translated template to Quarto.\nabstract: | En el océano conviven una gran cantidad de corrientes de diferentes escalas espaciales y temporales. Las escalas espaciales típicas de la circulación oceánica son la larga escala, la mesoescala, submesoescala, y microescala. La larga escala es del \\({\\cal O}(1000\\,km)\\) y esta determinada por la circulación general en el océano como la termohalina y los grandes giros anticiclónicos de las grandes cuencas oceánicas; las escalas temporales de la larga escala varia entre meses y años. La mesoescala esta definida por corrientes del \\({\\cal O}(100\\,km)\\) como remolinos, corrientes costeras, filamentos, frentes, etc. Son corrientes mas regionales pero pueden tener gran influencia sobre la circulación general o de larga escala. Sus escalas temporales son de semanas a meses. La submesoescala corresponde a corrientes del \\({\\cal O}(10\\,km)\\) de caracter local remolinos, filamentos, frentes, corrientes en playas, puertos, y estuarios. La submesoscala varía temporalmente con rapidez en tiempos que varían de horas a días. Finalmente podemos hablar de la microescala que son remolinos del orden de centímetros a metros y generalmente es la escala característica de la turbulencia que transfiere energía desde la submesoescala hacia la disipación molecular. Aquí podemos hablar de fenómenos del orden de segundos y minutos.\nkeywords: [] key-points: - Probar interactividad y teoría - Jupyter,Julia,Pluto. - Vamos viendo. bibliography: bibliography.bib\ncitation: container-title: Geophysical Research Letters keep-tex: true date: last-modified"
  },
  {
    "objectID": "article2_new.html#estadística-y-conceptos-de-probabilidad",
    "href": "article2_new.html#estadística-y-conceptos-de-probabilidad",
    "title": "Julio Sheinbaum",
    "section": "Estadística y conceptos de probabilidad",
    "text": "Estadística y conceptos de probabilidad\nA pesar de nuestra formación determinista a la hora de resolver problemas matemáticos y aunque consideremos que las ecuaciones de Navier-Stokes que describen el movimiento del océano son deterministas, la estadística es ampliamente utilizada en oceanografía debido a diferentes razones:\n\nPara una descripción completa del océano es necesario especificar una gran cantidad de variables, muchas de las cuales son desconocidas. Un ejemplo de ello son las parametrizaciones que se hacen en oceanografía para describir variables que no pueden medirse directamente. Una parametrización no es nadammas que un modelo estadístico que explica la evolución de una variable dependiente de otras variables independientes. Por ejemplo, parametrización del esfuerzo del viento en función del corte vertical o parametrización del coeficiente de arrastre en función de la velocidad del viento a 10m de la superfície del océano.\nEl océano es altamente no lineal. La evolución de una cierta variable no se puede estudiar de forma aislada.\n\nEjemplo:\nSupongamos el término de aceleración horizontal en las ecuaciones de Navier Stokes para fluidos incompresibles,\n\\[\\begin{equation}\n\\frac{\\partial{\\mathbf {u}_h}}{\\partial{t}} + \\mathbf {u}_h \\cdot {\\nabla}_h \\mathbf {{u}_h}\n\\end{equation}\\]\nComo ya sabemos por el curso de Mecánica de Fluidos, la aceleración de un fluido es una derivada material y consta de un término local (aceleración local) y de un término advectivo o aceleración advectiva. En general, esta ecuación no se aplica a partículas de agua individuales.\nEn oceanografía hablamos de continuo. No estamos interesados en las características cinemáticas de las partículas individuales sino en la manifestación promedia del movimiento molecular, es decir, del fluido como un conjunto o contínuo. Es decir, asumimos que el fluido es uniforme en el espacio que ocupa sin considerar la estructura molecular.\nPor ello debemos de promediar de alguna forma para explicar el comportamiento conjunto del fluído y no de una partícula de agua específica? Y como se realiza tal promedio? En general, el promediado se realiza de tal forma que nos permite separar la larga escala que trataremos como determinística, de la pequeña escala que consideramos un proceso aleatorio (turbulento). Supongamos entonces la separación de la velocidad horizontal en una velocidad promedio y una velocidad fluctuante alrededor de la media\n\\[{\\mathbf u_h} = &lt;{\\mathbf u_h}&gt; + {\\mathbf u'_h} \\]\ndonde $ &lt; &gt; $ denotan promedio. Si aplicamos esta descomposición a la componente \\(x\\) de la aceleración obtenemos:\n\\[\\begin{equation}\n\\frac{\\partial &lt;\\mathbf {u}&gt;}{\\partial t} +  &lt;{\\mathbf u}&gt; \\cdot  {\\nabla} &lt;{\\mathbf u} &gt;  + &lt; {\\mathbf u'} \\cdot {\\nabla} \\mathbf {u}'&gt;\n\\end{equation}\\]\n\\[\\begin{equation}\n\\frac{\\partial \\mathbf {&lt; u &gt;}}{\\partial t} + \\nabla \\cdot (&lt;{\\mathbf u}&gt; \\textbf {&lt; u &gt;}) + \\nabla \\cdot&lt; \\mathbf {u'} \\mathbf {u}' &gt;\n\\end{equation}\\]\nDonde usamos la ecuación de continuidad\n\\[ \\nabla \\cdot (&lt; \\mathbf {u} &gt; + \\mathbf {u}') =0 \\]\npara pasar de la primera a la segunda expresión.\n\n\nInevitablemente, las pequeñas escalas o fluctuaciones respecto a la larga escala aparecen en la expresión de la aceleración de larga escala. De forma que la separación que deseamos no es tan simple ya que debemos de conocer la estadística de la pequeña escala para poder describir la circulación media.\nEl término \\(&lt;\\textbf {u}'_h u'&gt;\\) se denomina esfuerzo de Reynolds y nos informa de la correlación entre las componentes fluctuantes (alta frecuencia) de la velocidad. Por ejemplo, \\(&lt;u'v'&gt; = 0\\) significa que no existe i correlación y hablamos de isotropía. Si \\(&lt;u'v'&gt; &lt; 0\\), significa que las fluctuaciones están inversamente correlacionadas, i.e., anisotropía.\nEste es un gran problema no resuelto en la oceanografía física. El esfuerzo de Reynolds aparece porque la advección es no-lineal de tal forma que no podemos estudiar la larga escala sin conocer información de la pequeña escala que es un proceso aleatorio. Por similitud con el flujo laminar, los términos de esfuerzo de Reynolds se parametrizan estadísticamente como proporcionales a los gradientes de velocidad. El factor de proporcionalidad es el coeficiente de viscosidad, en este caso, turbulento. Es aqui donde utilizar herramientas estadísticas tiene sentido.\n\nNo podemos controlar las variables oceanográficas; estan en constante cambio a medida que el sistema observado evoluciona.\n\nEjemplo:\nEn el océano coexisten mareas, ondas internas, remolinos, turbulencia de pequeña escala,…las cuales enmascaran el fenómeno oceanográfico que estamos interesados en estudiar. Estos procesos incontrolables por el oceanógrafo en ocasiones es útil considerarlos aleatorios y utilizar herramientas estadísticas para caracterizarlos.\nImaginemos que queremos conocer cual es la temperatura superficial promedio en la bahía de Todos Santos. Una forma de proceder sería promediar todos los datos de temperatura superficial que disponemos de los últimos 100 años y promediarlos? Pero, ¿es realmente lo que deseamos? ¿Deberíamos de considerar la estaciones del año y obtener un pormedio para cada estación? ¿Qué sucede en años Niño, el cual sabemos que afecta la temperatura del océano? En definitiva, debemos de definir sobre que conjunto de datos vamos a promediar, y dichos promedio va a reflejar efectivamente esa elección.\n\nEste ejemplo precisa de la distinción entre lo que consideramos nuestra señal (temperatura media) de los procesos que son ruido (Estaciones del año, los años Niño, ondas internas, etc.). De esta forma, al definir el promedio estamos haciendo explícita la separación entre señal y ruido. Finalmente, una vez definido sobre que promediar, existen en literatura una gran cantidad de herramientas estadísticas que podemos utilizar. Definir señal y ruido, y determinar sobre que conjunto de datos vamos a calcular el promedio, es una tarea difícil. Conocer como debemos muestrear el océano también debe hacerse cuidadosamente.\n\nEn oceanografía física se muestrea el océano de forma discontínua, es decir, se obtienen medidas puntuales en el espacio y en el tiempo. Como dijimos anteriormente, el océano contiene procesos de diferentes escalas espaciales y temporales, nolineales, y aleatorios. Es por ello que es sumamente importante saber escojer el intervalo de muestreo \\(\\Delta{t}\\) dependiendo del fenómeno que se quiere muestrear. Debemos de tener en mente que la frecuencia mas alta que podemos resolver es la frecuencia de Nyquist:\n\\[f_N=1/(2\\Delta{t})\\,.\\]\nPor ejemplo, si medimos a intervalos de \\(\\Delta{t}=5\\,{ h}\\) podremos como máximo resolver procesos que ocurren con frecuencia \\(f_N\\le1/10\\,{ cph}\\). La frecuencia mas baja que podemos resolver va a depender de la longitud del registro. A esa frecuencia le llamamos frecuencia fundamental\n\\[f_0=1/(\\Delta{t}N)\\,,\\]\n{}donde \\(T=\\Delta{t} N\\) es la duración del muestreo y N es el número de muestras o datos. En general, debemos de medir suficiente tiempo para registrar varios ciclos del fenómeno de estudio para tener significancia estadística. Por lo tanto, nuestra resolución frecuencial va a depender del intervalo y duración del muestreo. El cociente \\(f_N/f_0=(1/2\\Delta{t})/(1/N\\Delta{t})=N/2\\) indica el número máximo de componentes de Fourier que podemos estimar. Una señal periódica se puede descomponer en la suma de un conjunto (infinito) de funciones oscilatorias de senos y cosenos o componentes de Fourier. Esto lo veremos en el capítulo~7. A cada muestreo de un fenómeno le denominamos realización, y a un conjunto de realizaciones se les denomina ensamble.\n\nEstadística básica\nLa estadística trata de describir las características de una población continua a partir de muestras discretas de la misma. Hablamos de población y de muestra de una población. Si calculamos, por ejemplo, la media de una población, estamos calculando un {parámetro}. Cuando calculamos la media de una muestra le llamamos un estadístico de la población.\n\nLa estadística nos ayuda a organizar, analizar, presentar datos, y nos da información de cómo planear la recolección de los mismos, i.e. a muestrear.\n\nLa media:\n\n\nLa media de una muestra de N valores \\(x_i=x_1,x_2,...,x_N\\) es\n\\[\\begin{equation}\n\\bar{x}=\\frac{1}{N}\\sum^N_{i=1}x_i=&lt;x&gt;\\,.\n\\end{equation}\\]\nLa media debe de diferenciarse de la mediana. La media es el momento de orden cero. La mediana de una población es aquel valor numérico que separa el 50% de valores mas altos del 50% de valores mas bajos. Se puede calcular ordenando de menor a mayor el conjunto de valores y escojer el valor central si el conjunto de datos es impar o el promedio de los dos centrales si es par.\n\n\nLa varianza:\n\n\nLa varianza de un una muestra de N valores \\(x_i\\) es\n\\[\\begin{equation}\ns^2=\\frac{1}{N-1}\\sum^N_{i=1}(x_i-\\bar{x})^2=&lt;x'^2&gt;\\,,\n\\end{equation}\\]\ndonde las primas indican fluctuaciones alrededor de la media. La varianza es una medida de cuán lejos estan los diferentes puntos de la muestra de la media de la población. La varianza es el segundo momento alrededor de la media. Al dividir por \\(N\\) estamos subestimando la verdaderavarianza de la población. Al dividir por \\(N-1\\) obtenemos un estimador insesgado.\n\nNOTA: el sesgo de un estimador se refiere a la diferencia entre su esperanza matemática y el valor numérico (real) del parámetro que se estima. Un estimador que no tiene sesgo se dice insesgado. Por ejemplo, para la media:\n\\[E[x]-\\mu \\rightarrow {0}\\]\n\\[\\bar{x}-\\mu \\rightarrow {0}\\]\nEJERCICIO: Demostrar porqué hay que dividir por \\(N-1\\) en lugar de \\(N\\) para que la definición de varianza sea un estimador insesgado.\n\n\nLa desviación típica:\n\n\nEs la raíz cuadrada de la varianza. Se suele escribir como \\(\\sigma\\) para referirse a la población o como \\(s\\) en estadística\n\\[\\begin{equation}\ns=\\sqrt{s^2}\\,.\n\\end{equation}\\]\n\nMomentos de orden superior:\n\n\nPodemos definir un momento alrededor de la media como:\n\\[\\begin{equation}\nm_p=\\frac{1}{N}\\sum^N_{i=1}(x_i-\\bar{x})^p=&lt;x'^p&gt;\\,.\n\\end{equation}\\]\nDe esta forma \\(m_2\\) es la varianza, \\(m_3\\) es la asimetría, y \\(m_4\\) la curtosis. El momento \\(m_3\\) indica la asimetría de la muestra alrededor de la media (\\(m_3&gt;0\\) implica distribución con cola larga en la parte positiva y viceversa). \\(m_4\\) indica el grado de esparcimiento de las muestras alrededor de la media. Una mayor curtosis indica mayor concentración de puntos alrededor de la media. Los momentos de orden superior (\\(&gt;2\\)) se suelen adimensionalizar dividiendo por la desviación estándar:\n\\[\\begin{equation}\n    m_3=\\frac{1}{N}\\sum^N_{i=1}\\left[\\frac{x_i-\\bar{x}}{\\sigma}\\right]^3=&lt;(x/\\sigma)'^3&gt;\n\\end{equation}\\]\n\\[\\begin{equation}\n    m_4=\\frac{1}{N}\\sum^N_{i=1}\\left[\\frac{x_i-\\bar{x}}{\\sigma}\\right]^4-3=&lt;(x/\\sigma)'^4&gt;-3\n\\end{equation}\\]\ndonde el factor \\(-3\\) hace que la curtosis tome el valor cero para una distribución Normal.\n\nCovarianza y correlación:\n\n\nLa covarianza entre dos variables \\(x\\) e \\(y\\) puede definirse como un estadístico que relaciona \\(x\\) e \\(y\\) de la siguiente forma\n\\[C_{xy}=&lt;x'y'&gt;=&lt;(x-\\bar{x})(y-\\bar{y})&gt;=\\frac{1}{N-1}\\sum\\limits^N_{i=1} (x_i-\\bar{x})(y_i-\\bar{y})\\,.\\]\nLa correlación es la covarianza normalizada\n\\[\\rho_{x y}=\\frac{C_{x y}}{s_x s_y}=\\frac{&lt;x' y'&gt;}{\\sqrt{&lt;x'^2&gt;&lt;y'^2&gt;}}\\,. \\]\nConsideremos el modelo estadístico lineal de media cero (es una recta que pasa por \\((\\overline{x},\\overline{y})=(0,0)\\))\n\\[\\hat{y}=\\alpha x\\,,\\]\ndonde \\(\\alpha\\) es una constante. El error cometido por este estimador se define como el error cuadratico medio\n\\[\\epsilon=&lt;(\\hat{y}-y)^2&gt;=\\alpha^2&lt;x^2&gt;+&lt;y^2&gt;-2\\alpha&lt;xy&gt;\\]\ny si queremos minimizar dicho error entonces tenemos que encontrar que \\(\\alpha\\) es el que provoca que la derivada \\(\\partial{\\epsilon}/\\partial{\\alpha}\\rightarrow{0}\\). Es decir\n\\[\\partial{\\epsilon}/\\partial{\\alpha}=2\\alpha&lt;x^2&gt;-2&lt;xy&gt;=0\\,,\\]\ny el \\(\\alpha\\) es\n\\[\\alpha=\\frac{&lt;xy&gt;}{&lt;x^2&gt;}\\,.\\]\nEl error cuadrático mínimo se encuentra substituyendo el valor de \\(\\alpha\\) en la expresión del error \\(\\epsilon\\) de arriba\n\\[\\epsilon=\\frac{&lt;xy&gt;^2}{&lt;x^2&gt;} + &lt;y^2&gt; - 2\\frac{&lt;xy&gt;^2}{&lt;x^2&gt;}=\n           &lt;y^2&gt;\\left(\\frac{&lt;xy&gt;^2}{&lt;x^2&gt;&lt;y^2&gt;}+1-2\\frac{&lt;xy&gt;^2}{&lt;x^2&gt;&lt;y^2&gt;}\\right)=\\]\n\\[=&lt;y^2&gt;(1-\\rho^2_{xy})\\,.\\]\nSi \\(\\rho^2_{xy}=1\\) entonces el error es cero, es decir, mínimo error. Opuestamente, si \\(\\rho^2_{xy}=0\\) entonces el error es igual a la varianza, es decir, máximo error. Si \\(\\rho\\) toma valores intermedios, i.e., \\(\\rho^2_{xy}=0.5\\), entonces el error es \\(\\epsilon=0.5&lt;y^2&gt;\\), es decir, el error del modelo lineal es un \\(50\\%\\) de la varianza. Por lo tanto, la correlación al cuadrado puede definirse también ciomo la eficiencia relativa del estimador \\(\\hat{y}^2\\) o la fracción de varianza explicada por el modelo lineal\n\\[\\rho^2_{xy}=\\frac{&lt;\\hat{y}^2&gt;}{&lt;y^2&gt;}=\\frac{{varianza\\,\\,\\,explicada}}{{ varianza\\,\\,\\,total}}\\,.\\]\nA este parámetro se le puede encontrar en literatura inglesa como skill del modelo lineal."
  },
  {
    "objectID": "article2_new.html#probabilidad",
    "href": "article2_new.html#probabilidad",
    "title": "Julio Sheinbaum",
    "section": "Probabilidad",
    "text": "Probabilidad\n\nDistribuciones de probabilidad:\nLa {función de distribución acumulativa} \\(D_x(r)\\) se define como la probabilidad que una variable aleatoria \\(x\\) sea menor o igual a \\(r\\), es decir, \\(P(x\\le r)\\). Matemáticamente:\n\\[D(x)=\\int^r_{-\\infty}F(x)dx\\,,\\]\ndonde\n\\[F(x)=\\frac{d}{dx} D(x)\\]\nes la función de densidad de probabilidad (PDF, por su siglas en inglés). La PDF nos informa de la probabilidad que \\(x\\) sea igual a un cierto valor \\(r\\), \\(P(x=r)\\).\n\n{} Algunas propiedades de \\(D(x)\\):\n\n* \\(D(r)\\le D(s)\\,\\,\\,{ if}\\,\\,\\,r\\le s\\)\n* \\(D(-\\infty)=0\\)\n* \\(D(\\infty)=1\\)\n\nAlgunas propiedades de \\(F(x)\\):\n\n\n\\(F(x)\\ge0\\)\n\n\\(\\int^{\\infty}_{-\\infty} F(x) dx=1\\)\n\n\nLa probabilidad que una variable aleatoria \\(x\\) este contenida en el intervalo \\([r,r+dr]\\) es la integral de la función de densidad de probabilidad \\[P(r\\le x\\le r+dr)=\\int^{r+dr}_r F(x)dx\\,.\\]\nAmbas definiciones son parecidas aunque no son lo mismo. Para ello veamos el ejemplo de la suma del lanzamiento de dos dados al aire.\n\nLa densidad de probabilidad de que la suma de los dos dados sea 7 es máxima y que sea 2 o 12 es mínima. Este ejemplo describe dos propiedades fundamentales de funciones de probabilidad discretas: (i) \\(P(X=x) \\ge 0\\) y (ii)\\(\\sum{P(x)}=1\\). La distribución de probabilidad acumulativa y la función de densidad de probabilidad tienen las siguientes distribuciones\n\n{} Momentos estadísticos de una función de densidad de probabilidad:\n\nLos momentos centrados (o alrededor de la media) de una distribución de probabilidad se definen como\n\\[m_r=E[(x-E[x])^r]=\\int^{\\infty}_{-\\infty} (x-\\mu)^rF(x)dx\\,.\\]\nComo caso particular, los momentos alrededor del origen (i.e., \\(\\mu=0\\)) son: \\[m^0_r=E[x^r]=\\int^{\\infty}_{-\\infty} x^rF(x)dx\\,.\\]\nEntonces, los primeros tres momentos centrados se definen como\n\\[m_0=E[(x-E[x])^0]=E[1]=\\int^{\\infty}_{-\\infty}F(x)dx=1\\,,\\] \\[m_1=E[(x-E[x])^1]=E[x]-\\mu=\\int^{\\infty}_{-\\infty} (x-\\mu)^1 F(x)dx=0\\,,\\] \\[m_2=E[(x-E[x])^2]=E[x^2 + E[x]^2 -2xE[x]]=\n      E[x^2]+E[x]^2-2E[x]E[x]=\\] \\[E[x^2]-E[x]^2=\\underbrace{E[x^2]}_{\\sigma^2}-\\mu^2=\\int^{\\infty}_{-\\infty} (x-\\mu)^2 F(x)dx=\\sigma^2-\\mu^2\\,.\\]\nLos momentos alrededor de cero (\\(\\mu=0\\)) tambien pueden ser estandarizados: \\[m^*_r=m_r/\\sigma^r=\\frac{E[(x-E[x])^r]}{(\\underbrace{E[(x-E[x])^2]}_{\\sigma^2})^{r/2}}\\,.\\]\nLos cuatro primeros momentos estadísticos alrededor de cero estandarizados son: \\[m^*_1=m_1/\\sigma^1=\\frac{E[(x-\\mu)^1]}{(E[(x-\\mu)^2])^{1/2}}=\\frac{\\mu-\\mu}{\\sqrt{E[(x-\\mu)^2]}}=0\\,,\\] \\[m^*_2=m_2/\\sigma^2=\\frac{E[(x-\\mu)^2]}{(E[(x-\\mu)^2])^{2/2}}=1\\,,\\] \\[m^*_3=m_3/\\sigma^3=\\frac{E[(x-\\mu)^3]}{(E[(x-\\mu)^2])^{3/2}}\\,,\\] \\[m^*_4=m_4/\\sigma^4=\\frac{E[(x-\\mu)^4]}{(E[(x-\\mu)^2])^{4/2}}\\,.\\]\n\nDistribución uniforme:\nLa distribución de probabilidad uniforme viene dada por \\[F(x)=\\frac{1}{b-a}\\,\\,\\,, a \\le x \\le b\\] \\[=0,\\,\\,\\,\\,fuera\\,\\,del\\,\\,intervalo\\]\n\nSe deduce de la expresión de área de un cuadrado:\n\\[Area=base*altura=(b-a)F(x)=1\\]\nLa función de distribución acumulativa es \\[D(x)=0,\\,\\,\\,x&lt;a\\] \\[D(x)=\\frac{x-a}{b-a},\\,\\,\\,a \\le x \\le b\\] \\[D(x)=1,\\,\\,\\,x \\ge b\\]\n\nLa media es \\(\\mu=(b+a)/2\\) y la varianza es \\(\\sigma^2=1/3(a^2 + b^2 +ab)\\). Demostración:\n\n{} Los momentos estadísticos alrededor del origen de la distribución uniforme son \\[m^0_r=E[x^r]=\\int^{\\infty}_{-\\infty} x^rF(x)dx=\\int^{b}_{a} \\frac{x^r}{b-a}dx=\\]\n\\[\\frac{1}{b-a}\\int^{b}_{a}x^r dx=\\frac{1}{b-a}\\left[\\frac{x^{r+1}}{r+1}\\right]^b_a=\n      \\frac{1}{b-a}\\left[\\frac{b^{r+1}}{r+1}-\\frac{a^{r+1}}{r+1}\\right]=\\frac{b^{r+1}-a^{r+1}}{(b-a)(r+1)}\\]\ny por lo tanto la media es\n\\[m^0_1=E(x)=\\frac{b^2-a^2}{2(b-a)}=\\frac{(b-a)(b+a)}{2(b-a)}=\\frac{b+a}{2}\\,,\\]\ny la varianza\n\\[m^0_2=E(x^2)=\\frac{b^3-a^3}{3(b-a)}=\\frac{(b-a)(a^2+b^2+ab)}{3(b-a)}=\\frac{1}{3}(a^2 + b^2 +ab)\\,.\\]\nEjemplo de distribución uniforme: La ruleta rusa. Supongamos que puede tomar 360 valores, es decir, \\(0 \\le x \\le 360\\). Entonces\n\\[F(x)=\\frac{1}{360},\\,\\,\\,0 \\le x \\le 360\\,,\\]\ny, por ejemplo, la probabilidad de que la bola caiga entre el 50 y el 360 es\n\\[P(50\\le x \\le 360)=\\int^{360}_{50}\\frac{1}{360}dx=\\frac{1}{360}\\left[x\\right]^{360}_{50}=\\frac{310}{360}=0.8611\\,(\\sim86\\%).\\]\nLa función de distribución acumulativa es\n\\[D(x)=0,\\,\\,\\,x&lt;0\\] \\[D(x)=\\frac{x}{360},\\,\\,\\,0 \\le x \\le 360\\] \\[D(x)=1,\\,\\,\\,x \\ge 360\\]\n\nDistribución normal o Gaussiana:\n\n\nLa distribución normal es una de las distribuciones mas recurrente en la naturaleza. En general cualquier variable aleatoria medida, especialmente aquellas que son suma de otras variables aleatorias, tiene una distribución normal alrededor de la media\n\\[F(x)=\\frac{1}{\\sigma\\sqrt{2\\pi}}{exp}\n   \\left\\{ -\\frac{(x-\\bar{x})^2}{2\\sigma^2} \\right\\}\\,.\\]\nLa distribución de función acumulativa normal se obtiene integrando la expresión de arriba. Para ello vamos a realizar el cambio de variable (que no es nada mas que estandarizar la variable aleatoria x)\n\\[z=\\frac{x-\\bar{x}}{\\sigma\\sqrt{2}}\\] y \\[dz=\\frac{dx}{\\sigma\\sqrt{2}}\\,,\\]\nde lo que se deduce\n\\[D(z)=\\frac{\\sigma\\sqrt{2}}{\\sigma\\sqrt{2\\pi}}\\int^z_{-\\infty} {exp}\n  \\left\\{ -z^2 \\right\\}dz =\n  \\frac{1}{\\sqrt{\\pi}}\\int^z_{-\\infty} {exp}\n  \\left\\{ -z^2 \\right\\} dz\\,,\\]\ndonde \\(\\frac{2}{\\sqrt{\\pi}}\\int^z_{0}{exp}\\left\\{ -t^2 \\right\\}dt={erf}(z)\\,.\\)\nLos momentos estadísticos alrededor del origen de la función de distribución Normal son\n\\[m^0_r=E[x^r]=\\int^{\\infty}_{-\\infty} x^rF(x)dx=\\frac{1}{\\sigma\\sqrt{2\\pi}}\\int^{\\infty}_{-\\infty}x^r{exp}\n   \\left\\{ -\\frac{(x-\\bar{x})^2}{2\\sigma^2} \\right\\} dx\\]\nHagamos el cambio de variable\n\\[u=\\frac{x-\\bar{x}}{\\sigma\\sqrt{2}}\\] \\[du=\\frac{dx}{\\sigma\\sqrt{2}}\\]\nSi substituimos en la expresión de \\(m_r\\) obtenemos\n\\[m^0_r=\\frac{\\sigma\\sqrt{2}}{\\sigma\\sqrt{2\\pi}}\\int^{\\infty}_{-\\infty}\n  \\left( \\sigma \\sqrt{2}u+\\bar{x}\\right)^r{e}^{-u^2}du=\n  \\frac{1}{\\sqrt{\\pi}}\\int^{\\infty}_{-\\infty}\\left( \\sigma \\sqrt{2}u+\\bar{x}\\right)^r{e}\n   ^{-u^2}du\\,.\\]\nEjercicio: Deducir los momentos estadísticos de orden 1 y 2 de la distribución Normal, es decir, la media y la varianza. Integrales útiles:\n\\[\\int e^{-ax^2}dx=\\frac{\\sqrt{\\pi}}{2\\sqrt{a}}{erf}(x\\sqrt{a})\\] \\[\\int xe^{-ax^2}dx=-\\frac{1}{2a}e^{-ax^2}\\,,\\]\ndonde la función de error se define cómo:\n\\[erf(z)=\\frac{2}{\\sqrt{\\pi}}\\int_0^z e^{-t^2} dt\\,.\\]\nLa función de error cumple las siguientes identidades: \\[{erf}(0)=\\frac{2}{\\sqrt{\\pi}}\\int_0^0 e^{-t^2} dt=0\\], \\[{erf}(\\infty)=\\frac{2}{\\sqrt{\\pi}}\\underbrace{\\int_0^\\infty e^{-t^2} dt}_{\\frac{\\sqrt{\\pi}}{2}}=1\\]. \\[{erf}(-\\infty)=\\frac{2}{\\sqrt{\\pi}}\\underbrace{\\int_0^{-\\infty} e^{-t^2} dt}_{-\\frac{\\sqrt{\\pi}}{2}}=-1\\].\nRespuesta: La media y la varianza son\n\\[m^0_1=E(x)=\\mu\\,\\,\\,\\,\\,\\,;\\,\\,\\,\\,\\,\\,m_1=0\\] \\[m^0_2=Var(x)=\\mu^2 + \\sigma^2\\,\\,\\,\\,\\,\\,;\\,\\,\\,\\,\\,\\,m_2=\\sigma^2\\]\nDeducción de la media: El momento de orden 1 centrado es:\n\\[m_1=\\frac{\\sigma\\sqrt{2}}{\\sqrt{\\pi}}\\int^{\\infty}_{-\\infty}u{e}\n   ^{-u^2}du=-\\frac{\\sigma\\sqrt{2}}{2\\sqrt{\\pi}}{e}\n   ^{-u^2}\\Big|^{\\infty}_{-\\infty}=0\\,.\\]\ny alrededor de cero (momentos crudos):\n\\[m^0_1=\\frac{1}{\\sqrt{\\pi}}\\int^{\\infty}_{-\\infty}(\\sigma \\sqrt{2} u + \\mu){e}\n   ^{-u^2}du=\\frac{\\sigma\\sqrt{2}}{\\sqrt{\\pi}}\\int^{\\infty}_{-\\infty}u{e}\n   ^{-u^2}du + \\frac{1}{\\sqrt{\\pi}}\\int^{\\infty}_{-\\infty}\\mu{ e}\n   ^{-u^2}du=\\]\n\\[=\\underbrace{\\frac{\\sigma\\sqrt{2}}{\\sqrt{\\pi}}\\left( -\\frac{1}{2}{ e}\n   ^{-u^2}\\right)}_{0} \\Big|^{\\infty}_{-\\infty}+\n   \\frac{\\mu}{\\sqrt{\\pi}}\\left( \\frac{\\sqrt{\\pi}}{2}{ erf}(u)\\right) \\Big|^{\\infty}_{-\\infty}=\n   \\frac{\\mu}{2}[1-(-1)]=\\mu\\,,\\]\nDeducción de la varianza:\nEl momento de orden 2 centrado es:\n\n\\[m_2=\\frac{(\\sigma\\sqrt{2})^2}{\\sqrt{\\pi}}\\int^{\\infty}_{-\\infty}u^2{ e}^{-u^2}du\\]\n\\[x=u \\rightarrow dx=du\\] \\[dy=u{ e}^{-u^2} \\rightarrow y=\\frac{1}{2}{ e}^{-u^2}\\]\n\\[m_2=\\frac{2\\sigma^2}{\\sqrt{\\pi}}\\left[ -\\frac{1}{2}{ e}^{-u^2}\n      \\Big|^{\\infty}_{-\\infty} +\\frac{\\sqrt{\\pi}}{4} { erf}(u)\\Big|^{\\infty}_{-\\infty}\\right]=\n  \\frac{2\\sigma^2}{\\sqrt{\\pi}}\\left[0 + \\frac{\\pi}{4}\\left({ erf}(\\infty) - { erf}(-\\infty) \\right)\n  \\right]=\\] \\[=\\frac{2\\sigma^2}{\\sqrt{\\pi}}\\left[ \\frac{\\pi}{4}+\\frac{\\pi}{4}\\right]=\\sigma^2\\]\n\ny alrededor de cero (momentos crudos):\n\\[m^0_2=\\frac{1}{\\sqrt{\\pi}}\\int^{\\infty}_{-\\infty}(\\sigma \\sqrt{2} u + \\mu)^2{ e}^{-u^2}du=\n   \\frac{1}{\\sqrt{\\pi}}\\int^{\\infty}_{-\\infty}2\\sigma^2 u^2{ e}^{-u^2}du +\\] \\[ +\\frac{\\mu^2}{\\sqrt{\\pi}}\\int^{\\infty}_{-\\infty}{ e}^{-u^2}du\n   +\\frac{2\\sigma\\sqrt{2}\\mu}{\\sqrt{\\pi}}\\int^{\\infty}_{-\\infty}u{ e}^{-u^2}du=\\]\n\\[\n   =\\underbrace{\\frac{2\\sigma^2}{\\sqrt{\\pi}}\\int^{\\infty}_{-\\infty}u^2{ e}^{-u^2}du}_{m_2=\\sigma^2}\n   +\\mu \\underbrace{\\frac{1}{\\sqrt{\\pi}}\\int^{\\infty}_{-\\infty}\\mu{ e}^{-u^2}du}_{m_1^0=\\mu}+\n   +2\\mu \\underbrace{\\frac{\\sigma\\sqrt{2}}{\\sqrt{\\pi}}\\int^{\\infty}_{-\\infty}u{ e}^{-u^2}du}_{m_1=0}= \\sigma^2+\\mu^2\\,,\\]\n\nLa probabilidad de que una variable normalmentedistribuida caiga en una desviación estándar de su valor medio viene dada por \\[P(-1\\le z \\le 1)=\\int^{+1}_{-1} F(z) dz=\\frac{1}{\\sqrt{2\\pi}}\\int^{+1}_{-1}e^{-\\frac{1}{2}z^2}dz=\\] \\[=\\frac{1}{\\sqrt{2\\pi}}\\frac{\\sqrt{\\pi}}{2\\sqrt{1/2}}\\left[{ erf}({z\\sqrt{1/2}})\\right]^1_{-1}=\n   \\frac{1}{2}\\left[{ erf}({1/\\sqrt{2}})-{ erf}({-1/\\sqrt{2}}) \\right]=\\] \\[=\\frac{1}{2}\\left[0.6827-(-0.6827)\\right]=0.6827\\,(~68.27\\%)\\,,\\]\n\ny similarmente para 2 y 3 desviaciones estándar\n\n\\[P(-2\\le z \\le 2)=\\int^{+2}_{-2} F_x(z) dz=95.45\\%\\] \\[P(-3\\le z \\le 3)=\\int^{+3}_{-3} F_x(z) dz=99.73\\%\\,.\\]\nEntonces solo hay un \\(4.55\\%\\) de probabilidad de que una variable normalmente distribuida caiga fuera de dos desviaciones estándar respecto de la media. Puesto que es una probabilidad con 2 colas, la probabilidad de que una variable normal exceda su media por mas de \\(2\\sigma\\) es la mitad de esto, es decir \\(2.275\\%\\), ya que la distribución normal es simétrica.\nEn la práctica una PDF se calcula como un histograma escalado. Es por ello que necesitamos escojer el tamaño y localización de los bins en el histograma. La demo muestra las consecuencias de esta elección (pdf_demo.m).\n\nDistribución de Poisson:\n\n\nLa distribución de Poisson expresa la probabilidad de que ocurra un determinado número de eventos durante un cierto intervalo en el tiempo o distancia en el espacio. Se usa generalmente para la ocurrencia de sucesos con muy poca probabilidad o muy ``raros’’. La expresión para la función acumulativa es:\n\\[D(x)=P(x \\le r)=e^{-\\lambda}\\sum^{|r|}_{k=0}\\frac{\\lambda^k}{k!}\\,,\\]\ndonde \\(\\lambda\\) es el valor promedio\ny la función de densidad de probabilidad\n\\[F(x)=P(x=k)=\\frac{{\\lambda}^k e^{-\\lambda}}{k!}\\]\nLos momentos estadísticos alrededor del origen de la función de distribución de Poisson se pueden calcular directamente con sumatorios y expansion de Taylor:\n\\[m^0_r=E[x^r]=\\sum k^r P(X=k)=\\sum_{k\\ge0} k^rF(x)= \\sum_{k\\ge0} k^r\\frac{{\\lambda}^k e^{-\\lambda}}{k!}=\\] \\[=\\lambda e^{-\\lambda}\\sum_{k \\ge 0}\\frac{k^r}{k!}\\lambda^{k-1}\\]\n{}Veamos el momento de orden 1 alrededor del orígen:\n\\[m^0_1=E[x]=\\lambda e^{-\\lambda}\\sum_{k \\ge 0}\\frac{k}{k!}\\lambda^{k-1}=\n\\lambda e^{-\\lambda}\\sum_{k \\ge 1}\\frac{k}{(k-1)!k}\\lambda^{k-1}=\n\\lambda e^{-\\lambda}\\sum_{k \\ge 1}\\frac{1}{(k-1)!}\\lambda^{k-1}\\] \\[=\\lambda e^{-\\lambda}\\sum_{j \\ge 0}\\frac{\\lambda^{j}}{j!}\\,,\\] para \\(j=k-1\\). Finalmente expandiendo en series de Taylor la función exponencial\n\\[e^{\\lambda}=\\sum_{j \\ge 0}\\frac{1}{j!}\\lambda^{j}\\], obtenemos:\n\\[m^0_1=\\lambda e^{-\\lambda}e^{\\lambda}=\\lambda\\]\nEjercicio: Demostrar que el momento estadístico alrededor del origen de orden 2 de la distribución de Poisson es igual a \\(m^0_2=\\lambda+\\lambda^2\\).\n\\[m^0_2=E[x^2]=\\lambda e^{-\\lambda}\\sum_{k \\ge 0}\\frac{k^2}{k!}\\lambda^{k-1}=\n               \\lambda e^{-\\lambda}\\left[ \\sum_{k \\ge 1}(k-1)\\frac{1}{(k-1)!}\\lambda^{k-1} +\n                                      \\sum_{k \\ge 1} \\frac{1} {(k-1)!} \\lambda^{k-1} \\right]=\\]\n\\[= \\lambda e^{-\\lambda}\\left[ \\lambda \\sum_{k \\ge 2}\\frac{1}{(k-2)!}\\lambda^{k-2} + \\sum_{k \\ge 1}\\frac{1}{(k-1)!}\\lambda^{k-1}\\right]=\\] \\[= \\lambda e^{-\\lambda}\\left[ \\lambda \\sum_{j \\ge 0}\\frac{1}{j!}\\lambda^{j} + \\sum_{i \\ge 0}\\frac{1}{(i)!}\\lambda^{i}\\right]=\n\\lambda e^{-\\lambda} \\left[\\lambda e^{\\lambda} + e^{\\lambda}\\right]=\\]\n\\[=\\lambda (\\lambda +1)=\\lambda^2 + \\lambda\\,.\\]\nEn el casso que fuera el momento de orden 2 centrado se escribiría:\n\\[E[(x-E[x])^2]=E[x^2]-(E[x])^2=\\lambda^2 + \\lambda - (\\lambda)^2=\\lambda\\,.\\]\nEjemplo: En los últimos 160 años, han sucedido 680 tormentas intensas en el Golfo de México, incluyendo depresiones, tormentas tropicales, y huracanes. Asumimos que la frecuencia de ocurrencia de una tormenta intensa en el Golfo de México sigue una distribución de Poisson (eventos “raros”, poco frecuentes). Calcula la probabilidad de que ocurran 2 huracanes en 1 año:\n\n\nEl número promedio de tormentas intensas por año es: \\(\\mu=680/160=4.25\\) huracanes/año.\nLa probabilidad de que ocurran 2 huracanes en 1 año es:\n\n\\[P(x=2)=\\frac{{\\lambda}^2 e^{-\\lambda}}{2!}=\\frac{{4.25}^2 e^{-4.25}}{2!}=0.1288\\,(\\sim12\\%)\\]\nLa probabilidad es muy baja debido a que exigimos que sean exactamente 2 huracanes en un año y no, por ejemplo, \\(&gt;2\\). En el segundo caso, la probabilidad aumentaría considerablemente\n\\[P(x&gt;2)=P(x=3)+P(x=4)+....=1-P(x\\le 2)=1-[P(x=0)+P(x=1)+P(x=2)]=\\] \\[=1-[0.0143+0.0606+0.1288]=1-0.2037=0.7963\\,(\\sim80\\%)\\]\n\nDistribución Binomial:\n\nSupongamos que tenemos un conjunto de \\(n\\) tiradas en los cuales pueden suceder únicamente dos cosas: acierto' ofallo’. La probabilidad de acertar en una tirada es p=P. Si \\(X\\) es el número total de aciertos en \\(n\\) tiradas, entonces la probabilidad de que el número de aciertos sea \\(k\\) es:\n\\[P(X=k)=\\left( \\begin{array}{c}\nn \\\\ k\n       \\end{array} \\right)\np^k (1-p)^{n-k}, \\, k=0,1,2,3,....,n\\,,\\]\n}donde la expresión\n\\[\\left( \\begin{array}{c}\nn \\\\ k\n       \\end{array} \\right)=C(n,k)\\equiv\\frac{n!}{(n-k)!k!}\\,,\\]\n\nes el número de diferentes combinaciones de grupos de k objetos que pueden ser elegidos de un conjunto total de n objetos. Estos números se denominan coeficientes binomiales. La probabilidad de que el número de aciertos caiga en un rango de valores es\n\\[P(a\\le X\\le b)=\\sum^b_a P(X)\\]\nEjemplo 1: ¿Cual es la probabilidad de obtener exactamente 6 caras de 10 lanzamientos de moneda? Respuesta:\n\\[P(x=6) = C(10,6)0.5^6(1-0.5)^{10-6} = \\frac{10!}{(10-6)!6!}0.5^6(1-0.5)^{10-6} \\simeq 0.205\\]\n{Ejemplo 2:} ¿Cual es la probabilidad de obtener mas de 15 caras de 20 lanzamientos de moneda? Respuesta:\n\\[\\sum^{20}_{k=16} \\left( \\begin{array}{c} 20 \\\\ k\n       \\end{array} \\right) 0.5^k(1-0.5)^{20-k}=0.006\\,.\\]\nSi realizas esta operación a mano se vuelve muy tediosa. Es por ello que se utiliza la aproximación Normal a la distribución Binomial (DeMoivre-Laplace).\n{} Teorema de DeMoivere-Laplace (aproximación de Binomial a Normal)\n\nLa distribución binomial de una variable X definida por n tiradas independientes cada una de las cuales tienen una probabilidad \\(p\\) de acertar, es aproximadamente una distribución Normal de media \\(np\\) y desviación típica \\(\\sqrt{np(1-p)}\\), cuando n es suficientemente grande. Entonces se deduce que para cualquier número a y b,\n\\[lim_{n\\rightarrow\\infty} P \\left( a&lt;\\frac{X-np}{\\sqrt{np(1-p)}}&lt;b\\right)= \\frac{1}{\\sqrt{2\\pi np(1-p)}}\\int^b_a exp-\\left[\\frac{(x-np)^2}{2np(1-p)}\\right]dx\\,.\\]\n\nEsto significa que el estadístico, \\(\\frac{X-np}{\\sqrt{np(1-p)}}\\) , tiene una distribución Normal. Este teorema es un caso particular del teorema del límite central y nos permite de simplificar la solución de un problema binomial.\n{} Ejemplo de la aproximación Normal a la distribución Binomial:\n\nEl 2% de los XBTs fabricados por una empresa presentan defectos. Si hemos adquirido 2000 XBTs, ¿Cual es la probabilidad de que haya menos de 50 defectuosos?\n\nRespuesta: Se trata de una distribución binomial ya que solo pueden ser defectuosos o no defectuosos. La probabilidad que sea defectuoso es \\(p=0.02\\) (2%) y \\(n=2000\\), lo que nos da una distribución Binomial \\(B(2000,0.02)\\). Puesto que la \\(n\\) es grande podemos hacer una aproximación a la distribución Normal. Calculamos la media y desviación estándar de la distribución Normal \\(\\mu=np=200*0.02=40\\) y \\(\\sigma=\\sqrt{np(1-p)}=\\sqrt{2000*0.02*(1-0.02)}=6.26\\) \\(x\\) es \\(B(2000,0.02)\\) y \\(x_N\\) es \\(N(40,6.26)\\).\nLa probabilidad que \\(x&lt;50\\) es \\[p(x&lt;50)=p(x_N\\le 49)\\,,\\] y si estandarizamos \\[p(x_N\\le 49)=p\\left(z\\le \\frac{49-40}{6.26} \\right)=p(z\\le 1.44)=0.9251\\,.\\]\n{} EJERCICIOS de estadística y probabilidad:\n\nEjercicio 1: Calcule E[x] si x tiene la función de densidad de probabilidad\n\\[ f(x)=\\Bigg(\\begin{array}{c}\n\\frac{1}{4}xe^{-\\frac{x}{2}}\\,\\,\\,\\,\\,,x&gt;0 \\\\ 0 \\,\\,\\,\\,\\,,otherwise\n\\end{array})\\,.\\]\nLa esperanza E[x] de la función \\(f(x)\\) es entonces\n\\[E[x]=\\int^\\infty_0 x\\left(\\frac{1}{4}xe^{-\\frac{x}{2}}\\right)dx\n      =\\frac{1}{4}\\int^\\infty_0 x^2e^{-\\frac{x}{2}}dx\\,.\\]\nDefinamos \\(y=x/2\\); entonces \\(x=2y\\) y \\(dx=2dy\\) y obtenemos\n\\[E[x]=\\frac{1}{4}\\int^\\infty_0 x^2e^{-\\frac{x}{2}}dx\n        =\\frac{1}{4}\\int^\\infty_0 (2y)^2e^{-y}2dy\n    =2\\int^\\infty_0y^2e^{-y}dy\\,.\\] Vamos ahora a resolver la integral por partes. Hacemos la siguiente sustitución: \\(u=y^2\\), \\(dv=e^{-y}\\) y por ende \\(du=2ydy\\) y \\(v=-e^{-y}\\). La integral se puede reescribir usando la expresión general de integración por partes \\(h(x)=uv-\\int vdv\\):\n\\[E[x]=2\\int^\\infty_0y^2e^{-y}dy=2\\left[ -y^2e^{-y}-\\int-e^{-y}(2y)dy\\right]\n      =2\\left[ -y^2e^{-y}+2\\int ye^{-y}dy\\right]\\,.\\]\nIntegramos de nuevo por partes. Usa \\(u=y\\), \\(dv=e^{-y}\\) y entonces \\(du=dy\\) y \\(v=-e^{-y}\\)\n\\[E[x]=2\\left[ -y^2 e^{-y} + 2 \\Big\\{ -y e^{-y} - \\int -e^{-y}dy \\Big\\} \\right]=\\] \\[=2\\left[ -y^2 e^{-y} + 2 \\Big\\{ -y e^{-y} - e^{-y}         \\Big\\} \\right]=\\]\n\\[=-2y^2e^{-y}-4ye^{-y}-4e^{-y}=\\] \\[=\\left[ -2e^{-y}(y^2+2y+2) \\right]^{\\infty}_0=\\] \\[=\\lim_{n \\to\\infty}\\left[ -2e^{-y}(y^2+2y+2) \\right]-\\left[ -2e^{-y}(y^2+2y+2) \\right]_{y=0}\\,.\\]\nEl límite es ahora del tipo \\(\\infty/\\infty\\) y entonces usamos la regla de l’Hopital \\[E[x]=-2\\lim_{y \\to\\infty}\\frac{2y+2}{e^y}+2\\left[e^{-y}(y^2+2y+2) \\right]_{y=0}\\] Usamos la regla de l’Hopital de nuevo\n\\[E[x]=-2\\lim_{y \\to\\infty}\\frac{2}{e^y}+2\\left[e^{-y}(y^2+2y+2) \\right]_{y=0}=0+2\\left[e^{-y}(y^2+2y+2) \\right]_{y=0}=4\\]\n{} Ejercicio 2: Calcule E[x] si x tiene la función de densidad de probabilidad\n\\[f(x)=\\Bigg\\{\\begin{array}{c}\nc(1-x^2)\\,\\,\\,\\,\\,,-1&lt;x&lt;1 \\\\ 0 \\,\\,\\,\\,\\,,otherwise\n       \\end{array} \\,.\\]\n\\[E[x]=\\int^1_{-1} x[c(1-x^2)]dx=c \\int^1_{-1} x[(1-x^2)]dx=\\] \\[=c \\int^1_{-1} x-x^3dx=c\\left[\\frac{x^2}{2}+\\frac{x^4}{4}\\right]^{1}_{-1}=0\\]\n{} Ejercicio 3: Calcule E[x] si x tiene la función de densidad de probabilidad\n\\[f(x)=\\Bigg\\{\\begin{array}{c}\n\\frac{5}{x^2}\\,\\,\\,\\,\\,,x&gt;5 \\\\ 0 \\,\\,\\,\\,\\,,x\\le5\n       \\end{array} \\,.\\]\n\\[E[x]=\\int^{\\infty}_{5}x\\frac{5}{x^2}dx=\\int^{\\infty}_{5}\\frac{5}{x}dx=5\\int^{\\infty}_{5}\\frac{1}{x}dx\\] \\[=5[lnx]^{\\infty}_5=5\\left[\\Big(\\lim_{x \\to\\infty} ln{x}\\Big)-ln{5}\\right]\\rightarrow \\infty\\]\n La variable aleatoria \\(x\\) tiene la siguiente función de densidad de probabilidad\n\\[f(x)=\\Bigg\\{\\begin{array}{c}\n          k(2x+3)\\,\\,\\,\\,\\,-1\\le x \\le 2 \\\\ 0 \\,\\,\\,\\,\\,otherwise\n         \\end{array} \\,.\\]\n Sea la función \\(g(x)\\) dada por\n\\[g(x)=\\Bigg\\{\\begin{array}{c}\nx+2\\alpha\\,\\,\\,\\,\\,,x\\le-\\alpha \\\\\nx \\,\\,\\,\\,\\,,-\\alpha \\le x \\le \\alpha \\\\\nx-2 \\alpha \\,\\,\\,\\,\\,,x&gt;\\alpha\n       \\end{array} \\,,\\]\ndonde asumimos que x esta normalmente distribuida. Calcula la media de \\(g(x)\\).\n\\[E[g(x)]=\\int^{\\infty}_{-\\infty} g(x) F(x) dx=\\int^{-\\alpha}_{-\\infty} (x+2\\alpha) F(x) dx +\n\\int^{\\alpha}_{-\\alpha} x F(x) dx + \\int^{\\infty}_{\\alpha} (x-2\\alpha) F(x) dx=\\] \\[=\\int^{-\\alpha}_{-\\infty} x F(x) dx + \\int^{-\\alpha}_{-\\infty} 2 \\alpha F(x) dx +\n   \\int^{\\alpha}_{-\\alpha} x F(x) dx + \\int^{\\infty}_{\\alpha} x F(x) - \\int^{\\infty}_{\\alpha} 2\\alpha F(x)dx=\\] \\[=\\int^{-\\infty}_{-\\infty} x F(x) dx +  2\\alpha \\left[ \\int^{-\\alpha}_{-\\infty} F(x) dx - \\int^{\\infty}_{\\alpha}  F(x)\\right]=\\] \\[=\\int^{-\\infty}_{-\\infty} x F(x) dx +  2\\alpha \\left[ D(x=-\\alpha) - \\Bigg(1-\\int^{-\\alpha}_{-\\infty} F(x) dx\\Bigg)\\right]=\\] \\[=\\int^{-\\infty}_{-\\infty} x F(x) dx +  2\\alpha \\left[ D(x=-\\alpha) - \\Bigg(1-D(x=\\alpha)\\Bigg)\\right]=\\] \\[=\\mu + 2 \\alpha \\left[ D(-\\alpha) - 1 + D(\\alpha) \\right]\\,.\\]\ndonde la media de la distribución Normal es\n\\[E[x]=\\int^{-\\infty}_{-\\infty} x F(x) dx=\\int^{-\\infty}_{-\\infty} x \\frac{1}{\\sigma \\sqrt{2\\pi}}e^{-\\frac{1}{2} \\big( \\frac{x-\\mu}{\\sigma} \\big)^2 } dx=\\mu\\,.\\]\n{} Teorema del límite central:\nDefinición 1: Sea \\(X_1,\\,X_2,\\,X_3,...,X_n\\) un conjunto de variables aleatorias, independientes e idénticamente distribuidas con media \\(\\mu\\) y varianza \\(\\sigma^2\\) distinta de cero. Sea \\[S_n=X_1+X_2+....+X_n\\,,\\] entonces \\[\\lim_{n \\to\\infty} Pr (Z_n\\le z)= \\Phi(z)\\,,\\] donde \\(\\Phi(z)\\) es una distribución Normal estándar y \\(Z_n=\\frac{S_n - n\\mu} {\\sigma\\sqrt{n}} = \\frac{\\bar{X} - \\mu} {\\sigma/\\sqrt{n}}\\) es una estandarización del sumatorio \\(S_n\\) de tal forma que la media de la nueva variable \\(Z_n\\) sea cero y su desviación estándard sea igual a 1. De esta forma, las variables \\(Z_n\\) convergerán a una distribución normal estándar \\(N(0,1)\\), cuando \\(n\\) tienda a infinito.\n\nDefinición 2: Sea \\(X_1,\\,X_2,\\,X_3,...,X_n\\) un conjunto de variables aleatorias, independientes e idénticamente distribuidas con media \\(\\mu\\) y varianza \\(\\sigma^2\\) distinta de cero. Entonces, si \\(n\\) es suficientemente grande, la variable aleatoria\n\\[\\bar{X}=\\frac{1}{n}\\sum^n_{i=1}{X_i}\\]\ntiene aproximadamente una distribución normal con media \\(\\mu(\\bar{X})=\\mu\\) y desviación típica \\(\\sigma(\\bar{X})=\\sigma/\\sqrt{n}\\).\n\nNOTA: Es importante remarcar que el teorema del límite central no dice nada acerca de la distribución de \\(X_i\\), solo de la distibución de su media muestral \\(\\bar{X}\\).\n\nAplicación 1: Calculo de probabilidades sobre la media muestral.\n\nEjemplo: La recolección de muestras de agua con una roseta es una variable aleatoria con media \\(\\mu=150\\,{ ml}\\) y varianza de \\(\\sigma^2=120\\,{ ml}^2\\). Si tomamos \\(n=40\\) muestras aleatorias de agua. (a) ?`Cual es la media y la desviación estándar de la media muestral?, (b) ¿Cual es la probabilidad de que la media muestral contenga entre \\(145\\) y \\(153\\,{ ml}\\) de agua?\n\n\n\\(\\mu(\\bar{X})=150\\,{ ml}\\) y \\(\\sigma(\\bar{X})=\\sigma/\\sqrt{n}=\\sqrt{120/40}= \\sqrt{3}\\,{ ml}\\)\n\nQueremos calcular \\(Pr(145 \\le \\bar{X} \\le 153)\\). Si escribimos la probabilidad en forma estandarizada, entonces:\n\n\\[Pr(145 \\le \\bar{X} \\le 153) =\n      Pr\\left( \\frac{145-150}{\\sqrt{3}} \\le Z \\le \\frac{153-150}{\\sqrt{3}}\\right)\n       \\simeq Pr(-2.89 \\le Z \\le 1.73)=\\]\n\\[=Pr(Z \\le 1.73)- Pr(Z \\le -2.89)=0.9582-(1-0.9981)=0.9582-0.0019=0.9563\\]\n{} Función de densidad de probabilidad conjunta\n\nLa probabilidad que dos variables aleatorias \\((x,y)\\) caigan en la región \\(R\\) (como por ejemplo un rectángulo) se obtiene integrando su función de probabilidad conjunta\n\\[P((x,y)\\in R)=\\int\\int_{R} F(x,y) dx dy\\,.\\]\nEn particular, si \\(R\\) es un rectángulo 2d \\({(x,y):r\\le x \\le r+dr, s \\le y \\le s+ds}\\), entonces\n\\[P((x,y)\\in R)=P(r\\le x \\le r+dr, s \\le y \\le s+ds)=\\int^{r+dr}_r\\int^{s+ds}_{s} F(x,y) dx dy\\,.\\]\nAlgunas propiedades:\n\n\n\\(F(x,y)\\ge0\\) para todo x,y.\n\n\\(\\int^{\\infty}_{-\\infty}\\int^{\\infty}_{-\\infty} F(x,y) dx dy=1\\)\n\n\nDefinición: La función de densidad de probabilidad marginal de variables aleatorias \\(x\\) e \\(y\\) son:\n\\(Fx(x)=\\int^{\\infty}_{-\\infty} F(x,y)dy\\) y \\(Fy(y)=\\int^{\\infty}_{-\\infty} F(x,y)dx\\,.\\)\n{} Ejemplo del uso de la función de densidad de probabilidad conjunta\n\nImaginemos que una empresa de instrumentación oceanográfica fabrica boyas Lagrangianas de grosor \\(x\\) y diámetro \\(y\\), los cuales varian de una boya a la otra. Imaginemos que la función de densidad de probabilidad conjunta de la variable aleatoria “dimensión del instrumento oceanográfico” es:\n\\[F(x,y)=\\frac{1}{6}(r+s)\\,\\,\\,si\\,\\,\\,(x,y)\\in R=\\{1\\le x \\le 2 ; 4 \\le y \\le 5\\}\\]\n\\[F(x,y)=0\\,\\,\\,si\\,\\,\\,(x,y)\\,fuera\\,de\\,R\\]\nAhora queremos saber que probabilidad hay de que una boya tenga un grosor \\(1 \\le x \\le 1.5m\\) y un diámetro \\(4.5 \\le y \\le 5m\\), es decir \\[P(1 \\le x \\le 1.5, 4.5 \\le y \\le 5)=\\int^{1.5}_{1} \\int^{5}_{4.5} \\frac{1}{6}(r+s) ds dr = 0.253= 25\\%\\]\n{} Significancia estadística utilizando la distribución Normal\n\nComo vimos en el teorema central del límite, para una población infinita (\\(N\\rightarrow\\infty\\)) la desviación estándar de la distribución de las medias muestrales es:\n\\[\\sigma(\\bar{x})=\\frac{\\sigma}{\\sqrt{N}}=error\\,estándar\\,del\\,estimado\\,de\\,la\\,media\\,.\\]\nAquí, \\(\\sigma\\) es la desviación estándar de la población y \\(N\\) es el número de datos (independoentes) utilizado para calcular la media muestral. Entonces, si promediamos observaciones de una población de desviación estándar \\(\\sigma\\), la desviación estándar de esos promedios disminuye como el inverso de la raíz cuadrada del tamaño muestral \\(N\\).\n\nSi \\(N\\) es suficientemente grande podemos usar las estimaciones de \\(\\sigma\\) y \\(\\bar{x}\\) para calcular el denominado estadístico \\(z\\) que corresponde a una distribución normal estandarizada de media \\(\\mu=0\\) y \\(\\sigma=1\\)\n\\[z=\\frac{\\bar{x}-\\mu}{\\sigma(\\bar{x})}=\\frac{\\bar{x}-\\mu}{\\frac{\\sigma}{\\sqrt{N}}}\\]\n\nLa fórmula de arriba puede modificarse convenientemente para darnos un test de significancia estadística para la diferencia entre medias muestrales con tamaños muestrales y desviación estándar diferentes:\n\\[z=\\frac{\\bar{x}_1-\\bar{x}_2-\\Delta_{1,2}}\n  {\\frac{\\sigma_1^2}{\\sqrt{N_1}} + \\frac{\\sigma_2^2}{\\sqrt{N_2}}}\\,,\\]\ndonde \\(\\Delta_{1,2}\\) es la diferencia esperada entre las dos medias, lo que se suele asumir cero en la práctica.\n\nSi el tamaño muestral \\(N\\) es menor de \\(30\\) entonces no podemos usar el estadístico \\(z\\), pero podemos utilizar la distribución t-student; o cuando queremos comparar varianzas, podemos usar la distribución chi-cuadrada. La t-student converge a una distribución normal para largos tamaños muestrales y se define como\n\\[t=\\frac{\\bar{x}-\\mu}{\\frac{s}{\\sqrt{N-1}}}=\\frac{\\bar{x}-\\mu}{\\frac{\\hat{s}}{\\sqrt{N}}};\n\\hat{s}=\\sqrt{\\frac{N}{N-1}s}\\,.\\]\nSi consideramos una población normalmente distribuida de media \\(\\mu\\) la función de densidad de probabilidad de la t-student es\n\\[F_{x}(t)=\\frac{f_0(\\nu)}{\\left(1+\\frac{t^2}{\\nu} \\right)^{\\frac{\\nu+1}{2}}}\\,,\\]\ndonde \\(\\nu=N-1\\) es el número de grados de libertad y \\(f_0(\\nu)\\) es una constante que depende en \\(\\nu\\) y permite que el área bajo la curva \\(F_x(t)\\) sea igual a la unidad. Los grados de libertad se definen como el número de muestras independientes \\(N\\) menos el número de parámetros del estadístico que queremos estimar.\n\nA diferencia del estadístico \\(z\\), la t-student depende del número de grados de libertad; la cola de la distribución es larga para números de grados de libertad bajos (o \\(N\\) pequeña). Para números altos de grados de libertad (o \\(N\\) grande), la distribución t-student se acerca al estadístico \\(z\\) o distribución Normal.\nIntervalos de confianza\n\nPara calcular valores de los estadísticos \\(z\\) y t-student debemos de fijar el nivel de confianza definido como \\(1-\\alpha\\); porcentaje del nivel de confianza \\(100(1-\\alpha)\\%\\). Esto se puede escribir simbolicamente cómo\n\\[P(-z_{\\alpha/2}&lt;z&lt;z_{\\alpha/2})=1-\\alpha\\]\n\\[P(-t_{\\alpha/2}&lt;t&lt;t_{\\alpha/2})=1-\\alpha\\,.\\]\nUna vez definido el nivel de confianza y los grados de libertad \\(\\nu\\) (para la t-student) podemos leer el valor de dichos estadísticos en tablas. En esas tablas \\(z_{\\alpha/2}\\) es el valor de \\(z\\) para el cual solo el \\(100*{\\alpha/2}\\%\\) de los valores de \\(z\\) es esperado ser mas grande (cola de la derecha de la distribución). Igualmente, \\(z_{-\\alpha/2}=-z_{\\alpha/2}\\) es el valor de \\(z\\) para el cual solo el \\(100*{\\alpha/2}\\%\\) de los valores de \\(z\\) es esperado ser mas pequeño (cola de la izquierda de la distribución). O dicho de otra forma,\\(z_{\\alpha/2}\\) es el valor por encima del cual existe un área bajo la curva de \\(\\alpha/2\\). Los valores de \\(z\\) y \\(t\\) son las integrales bajo las correspondientes funciones de densidad de probabilidad.\n\nIntervalo de confianza para \\(\\mu\\) (\\(N&gt;30\\), \\(\\sigma\\) conocida)\n\n\nCuando \\(N&gt;30\\) y \\(\\sigma\\) es conocida, podemos usar el estadístico \\(z\\) para encontrar el intervalo de confianza para \\(\\mu\\). Hay un \\(100*(1-\\alpha)\\%\\) que cualquier estadístico \\(z\\) caiga en el intervalo\n\\[z_{-\\alpha/2}&lt;\\frac{\\bar{x}-\\mu}{\\sigma}\\sqrt{N}&lt;z_{\\alpha/2}\\] \\[\\frac{\\sigma}{\\sqrt{N}}z_{-\\alpha/2}&lt;{\\bar{x}-\\mu}&lt;\\frac{\\sigma}{\\sqrt{N}}z_{\\alpha/2}\\] \\[-1\\frac{\\sigma}{\\sqrt{N}}z_{-\\alpha/2}&gt;{\\mu-\\bar{x}}&gt;-1\\frac{\\sigma}{\\sqrt{N}}z_{\\alpha/2}\\] \\[\\bar{x}-1\\frac{\\sigma}{\\sqrt{N}}z_{-\\alpha/2}&gt;\\mu&gt;\\bar{x}-1\\frac{\\sigma}{\\sqrt{N}}z_{\\alpha/2}\\]\ny sabiendo que es simétrica \\(-z_{\\alpha/2}=z_{-\\alpha/2}\\):\n\\[\\bar{x}-z_{\\alpha/2}\\frac{\\sigma}{\\sqrt{N}} &lt; \\mu &lt;\n   \\bar{x}+z_{\\alpha/2}\\frac{\\sigma}{\\sqrt{N}}\\,.\\]\nSupongamos que queremos encontrar el intervalo de confianza de \\(\\mu\\) al \\(95\\%\\) de confianza, es decir, entonces \\(\\alpha=0.05\\). Entonces \\(z_{\\alpha/2}=1.96\\) (de tablas estadísticas).\n\nEjemplo: \\(N=40\\), \\(\\sigma=0.5^\\circ{C}\\), y \\(\\bar{x}=12.7^\\circ{C}\\):\n\\[\\bar{x}-z_{0.025}\\frac{\\sigma}{\\sqrt{N}}&lt;\\mu&lt;\\bar{x}+z_{0.025}\\frac{\\sigma}{\\sqrt{N}}\\,,\\]\n\\[\\left[12.7-(1.96)0.5/\\sqrt{40}\\right]\\,^\\circ{C}&lt;\\mu&lt;\\left[12.7+(1.96)0.5/\\sqrt{40}\\right]\\,^\\circ{C}\\] \\[12.54^\\circ{C} &lt; \\mu &lt; 12.85^\\circ{C}\\]\n\nIntervalo de confianza para \\(\\mu\\) (\\(N&lt;30\\), \\(\\sigma\\) desconocida)\nCuando \\(N&lt;30\\) y \\(\\sigma\\) es desconocida, podemos usar el estadístico \\(t\\)-student para encontrar el intervalo de confianza para \\(\\mu\\). Hay un \\(100*(1-\\alpha)\\%\\) que cualquier estadístico \\(t\\) caiga en el intervalo\n\n\\[t_{-\\alpha/2}&lt;\\frac{\\bar{x}-\\mu}{s}\\sqrt{N-1}&lt;t_{\\alpha/2}\\,,\\]\n\\[\\bar{x}-t_{\\alpha/2}\\frac{s}{\\sqrt{N-1}} &lt; \\mu &lt;\n   \\bar{x}+t_{\\alpha/2}\\frac{s}{\\sqrt{N-1}}\\,.\\]\nSi \\(\\alpha=0.05\\), hay un 95% de probabilidad que cualquier estadístico \\(t\\) caiga en el intervalo \\[t_{-0.025}&lt;\\frac{\\bar{x}-\\mu}{s}\\sqrt{N-1}&lt;t_{0.025}\\,,\\] de lo cual podemos deducir que la verdadera media \\(\\mu\\) es de esperar con un 95% de confianza que caiga en el intervalo: \\[\\bar{x}-t_{0.025}\\frac{s}{\\sqrt{N-1}}&lt;\\mu&lt;\\bar{x}+t_{0.025}\\frac{s}{\\sqrt{N-1}}\\,.\\]\nDe forma general, podemos definir el intervalo de confianza como: \\[\\mu=\\bar{x}\\pm t_c\\frac{\\hat{s}}{\\sqrt{N}}\\,,\\] donde \\(t_c\\) es el valor crítico del estadístico \\(t\\) (límites del intervalo), el cual depende del número de grados de libertad y del nivel de confiabilidad deseado. El intervalo de confianza con el estadístico \\(z\\), el cual solo es apropiado para tamaños muestrales grandes (\\(N&gt;30\\)) donde la desviación estándar es conocida: \\[\\mu=\\bar{x}\\pm z_c\\frac{\\sigma}{\\sqrt{N}}\\,.\\] Observamos que la teoría para tamaños muestrales pequeños reemplaza el estadístico \\(z\\) por el \\(t\\) y utiliza una desviación estándar muestral modificada \\[\\hat{s}=\\sqrt{\\frac{N}{N-1}s}\\,.\\] % % {iferencias entre medias}\\ %Supongamos dos muestras de tamaño \\(N_1\\) y \\(N_2\\) extraidas de una población %con distribución normal con desviaciones estándar siguales. Supongamos %que las medias muestrales son \\(\\bar{x_1}\\) y \\(\\bar{x_2}\\) y las %desviaciones estándar muestrales son \\(s_1\\) y \\(s_2\\). Para comprobar %la hipótesis nula (\\(H_0\\)) que ambas muestras provienen de la misma población, %es decir \\(\\mu_1=\\mu_2\\) y \\(\\sigma_1=\\sigma_2\\) podemos usar la siguiente expresión\n\\[t=\\frac{(\\bar{x_1}-\\bar{x_2})-(\\mu_1-\\mu_2)}{\\sigma\\sqrt{\\frac{1}{N_1} + \\frac{1}{N_2}}};\\]\ndonde \\(\\nu=N_1+N_2-2\\).\n\n\nIntervalo de confianza para la diferencia de medias \\(\\mu_1 - \\mu_2\\)}\n\n\nEl teorema central del límite (TCL) para la diferencia de medias muestrales de dos poblaciones viene dado por\n\\[\\bar{x}_1-\\bar{x}_2\\,\\sim\\,{ N}(\\mu_{\\bar{x}_1-\\bar{x}_2},\\sigma_{\\bar{x}_1-\\bar{x}_2})\\,,\\]\n\ndonde \\[{ Media:}\\,\\,\\mu_{\\bar{x}_1-\\bar{x}_2}=\\mu_{\\bar{x}_1}-\\mu{\\bar{x}_2}=\\mu_1-\\mu_2\\]\n\\[\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,{ Varianza:}\\,\\,\\sigma^2_{\\bar{x}_1-\\bar{x}_2}=\n   \\sigma^2{\\bar{x}_1}+\\sigma^2{\\bar{x}_2}=\n   \\sqrt{  \\frac{{\\sigma_1}^2}{N_1} + \\frac{{\\sigma_2}^2}{N_2} }\\]\n\n Desviaciones estándar poblacionales (\\(\\sigma_1\\) y \\(\\sigma_2\\)) conocidas; \\(\\mu_1\\) y \\(\\mu_2\\) desconocidas} (estadístico z):\n\\[z=\\frac{\\bar{x}_1-\\bar{x}_2-(\\mu_1-\\mu_2)}\n         {\\sqrt{\\frac{{\\sigma_1}^2}{N_1}+\\frac{{\\sigma_2}^2}{N_2}}}\\]\n\n3.2 Desviaciones estándar poblacionales (\\(\\sigma_1\\) y \\(\\sigma_2\\)) y medias poblacionales (\\(\\mu_1\\) y \\(\\mu_2\\)) desconocidas (estadístico t):\n\\[t=\\frac{\\bar{x}_1-\\bar{x}_2-(\\mu_1-\\mu_2)}\n              {\\sqrt{\\frac{{s_1}^2}{N_1}+\\frac{{s_2}^2}{N_2}}}\\]\n\n3.3 Desviaciones estándar poblacionales (\\(\\sigma_1\\) y \\(\\sigma_2\\)) desconocidas pero iguales (estadístico t):\n\nSupongamos dos muestras de tamaño \\(N_1\\) y \\(N_2\\) extraídas de dos poblaciones Normales con desviaciones estándar iguales (\\(\\sigma_1=\\sigma_2\\)). Supongamos que conocemos las medias y desviaciones estándar muestrales \\(\\bar{x}_1\\) y \\(\\bar{x}_2\\) y \\(s_1\\) y \\(s_2\\). Para comprobar la hipótesis nula \\(H_o\\) que las muestran proceden de la misma población (\\(\\mu_1=\\mu_2\\) y \\(\\sigma_1=\\sigma_2\\)) usamos el estadístico t-score (o pooled t):\n\\[t=\\frac{\\bar{x}_1-\\bar{x}_2-(\\mu_1-\\mu_2)}\n         {\\hat{s}_d\\sqrt{\\frac{1}{N_1}+\\frac{1}{N_2}}}\\] \\ \\[\\hat{s}_d=\\sqrt{\\frac{(N_1-1)s^2_1 + (N_2-1)s^2_2}{N_1+N_2-2}}\\,,\\]\n\ndonde \\(\\nu=N_1+N_2-2\\) es el número de grados de libertad.\n\nEjemplo: Un ingeniero que diseña instrumentos oceanográficos está ineteresado en aumentar el tiempo durante el cuál la pintura “antifouling” evita que los microorganismos se peguen y crezcan sobre el instrumento oceanográfico. Se prueban dos fórmulas de pintura: fórmula 1 estándar y fórmula 2 con un nuevo ingrediente que aumenta el tiempo de acción.\nDe la experiencia se sabe que la desviación estándar del tiempo de acción de la pintura es de 8 días y ésta variabilidad no se vé afectada por el nuevo ingrediente. Se pintan 35 instrumentos con la fórmula 1 y otros 35 con la fórmula 2. Los tiempos promedios de acción del “antifouling” son de 116 días para la fórmula 1 y 112 días para la fórmula 2. ¿A qué conclusión puede llegar el ingeniero diseñador del instrumento sobre la eficacia del nuevo ingrediente, con un nivel de significancia de 0.01?\\\n\\(x_1\\equiv\\) Tiempo de acción “antifouling” fórmula 1\n\n\\(x_2\\equiv\\) Tiempo de acción “antifouling” fórmula 2\n\\(x_1\\sim\\) Desconocida (\\(\\mu_1,\\sigma_1=\\) 8 días)\n\\(x_2\\sim\\) Desconocida (\\(\\mu_2,\\sigma_2=\\) 8 días)\n\\(\\bar{x}_1-\\bar{x}_2\\,\\sim\\,{ N}(\\mu_1-\\mu_2,\\sigma_1/\\sqrt{N_1} + \\sigma_2/\\sqrt{N_2})\\) (TCL)\n\\(\\bar{x}_1=116\\) días\\ \\(\\bar{x}_2=112\\) días\n\\(N_1=N_2=35\\)  \\(\\alpha=0.01\\)\n\\(H_0:\\,\\mu_1 - \\mu_2 =0\\)\n\\(H_1:\\,\\mu_1 - \\mu_2 \\neq0\\)\n\nEl intervalo de confianza de la diferencia de medias \\(\\mu_1-\\mu_2\\)\n\\[\\bar{x}_1-\\bar{x}_2-z_{\\alpha/2} \\left(\n   \\frac{\\sigma_1}{\\sqrt{N_1}} + \\frac{\\sigma_2}{\\sqrt{N_2}}\\right)&lt;\n   \\mu_1-\\mu_2 &lt;\n   \\bar{x}_1-\\bar{x}_2+z_{\\alpha/2} \\left(\n   \\frac{\\sigma_1}{\\sqrt{N_1}} + \\frac{\\sigma_2}{\\sqrt{N_2}}\\right)\\]\n\\[\\bar{x}_1-\\bar{x}_2-2.33 (1.9124)&lt;\n   \\mu_1-\\mu_2 &lt;\n   \\bar{x}_1-\\bar{x}_2+2.33 (1.9124)\\]\n\\[\\bar{x}_1-\\bar{x}_2-4.4559&lt;\n   \\mu_1-\\mu_2 &lt;\n   \\bar{x}_1-\\bar{x}_2+4.4559\\]\n\\[4-4.4559&lt;\n   \\mu_1-\\mu_2 &lt;\n   4+4.4559\\,,\\]\n\ny el intervalo de confianza es:\n\n\\[-0.4559&lt;\\mu_1-\\mu_2&lt;8.4559\\,\\,{ al}\\,\\,99\\%\\,(\\alpha=0.01)\\]\n\\[H_0:\\,\\mu_1 - \\mu_2 = 0\\,\\,{ al}\\,\\,99\\%\\,,\\]\ny aceptamos hipótesis nula \\(H_0\\).\n\n\nIntervalo de confianza para la varianza:\n\nDistribución chi-cuadrada\n\nEn ocasiones queremos definir un intervalo de confianza para la varianza muestral. Para ello podemos usar el estadístico chi-cuadrado. Definamos\n\\[\\chi^2=(N-1)\\frac{s^2}{\\sigma^2}\\,.\\]\n\nPropiedades:\nPara definir el intervalo de confianza sabemos que hay un \\(100*(1-\\alpha)\\%\\) que cualquier estadístico \\(\\chi^2\\) caiga en el intervalo\n\\[\\chi^2_{1-\\alpha/2}&lt;(N-1)\\frac{s^2}{\\sigma^2}&lt;\\chi^2_{\\alpha/2}\\,,\\] \\[\\frac{1}{\\chi^2_{1-\\alpha/2}}&gt;\\frac{\\sigma^2}{(N-1)s^2}&gt;\\frac{1}{\\chi^2_{\\alpha/2}}\\,,\\]\ny entonces:\n\\[\\frac{(N-1)s^2}{\\chi^2_{\\alpha/2}}&lt;\\sigma^2&lt;\\frac{(N-1)s^2}{\\chi^2_{1-\\alpha/2}}\\,.\\]\nUsamos \\(1-{\\alpha/2}\\) porque la \\(\\chi^2\\) es positiva. El valor \\(\\chi^2_{\\alpha/2}\\) es mayor que el valor \\(\\chi^2_{1-\\alpha/2}\\). Las tablas dan la probabilidad a la derecha del valor.\nPara una población normalmente distribuida con desviación estándar \\(\\sigma\\), la función de densidad de probabilidad de la chi-cuadrada es: \\[F_x(\\chi)=f_0\\chi^{\\nu-2}e^{-\\frac{1}{2}\\chi^2};\\,\\,\\nu=N-1\\,.\\]\nPuesto que la distribución chi es asimétrica y positiva, si \\(\\alpha=0.05\\) (95% confianza), el intervalo de confianza para la varianza \\(\\sigma^2\\) como \\[\\frac{(N-1)s^2}{\\chi^2_{0.025}}&lt;\\sigma^2&lt;\\frac{(N-1)s^2}{\\chi^2_{0.975}}\\,,\\]\ny si leemos en las tablas para \\(\\nu=9\\) grados de libertad:\n\\[\\frac{(N-1)s^2}{19.023}&lt;\\sigma^2&lt;\\frac{(N-1)s^2}{2.700}\\,,\\]\nEjemplo: Supongamos que tenemos \\(\\nu=9\\) grados de libertad de nuestra estimación espectral de la componente meridional de la velocidad de la corriente. Sabemos que la varianza muestral de un pico espectral es \\(s^2=10\\,{ cm}\\,{ s}^2\\) ?`Cuál es el intervalo de confianza al 95% para la varianza?\n\nDe las tablas estadísticas vemos que para \\(\\nu=N-1=9\\) grados de libertad, \\(\\chi^2_{1-\\alpha/2}=\\chi^2_{0.095}=19.02\\) y \\(\\chi^2_{\\alpha/2}=\\chi^2_{0.025}=2.70\\). Entonces, el intervalo es:\n\\[\\frac{(9)10}{19.023}&lt;\\sigma^2&lt;\\frac{(9)10}{2.700}\\] \\[4.7\\,{ cm}^2\\,{ s}^{-2}&lt;\\sigma^2&lt;33.3\\,{ cm}^2\\,{ s}^{-2}\\]\n{rados de libertad}\\ El número de grados de libertad es el número de muestras independientes N menos el número de parámetros del estadístico que queremos estimar. Por ejemplo en el estadístico \\(t\\) \\[t=\\frac{\\bar{x}-\\mu}{\\frac{s}{\\sqrt{N-1}}}=\\frac{\\bar{x}-\\mu}{\\frac{\\hat{s}}{\\sqrt{N}}};\n\\hat{s}=\\sqrt{\\frac{N}{N-1}s}\\,,\\] calculamos la media muestral y la desviación estándar \\(s\\) a partir de los datos, pero la verdadera media \\(\\mu\\) debe ser estimada, por lo que \\(\\nu=N-1\\). Similarmente en el estadístico chi-cuadrada \\[\\chi^2=(N-1)\\frac{s^2}{\\sigma^2}\\,,\\] conocemos la varianza muestral \\(s^2\\) y el tamaño muestral \\(N\\), pero debemos estimar la verdadera varianza, y entonces \\(\\nu=N-1\\).\n{stadístico \\(F\\)}\\ Otro estadístico útil para tests espectrales es el estadístico \\(F\\). Si \\(s^2_1\\) y \\(s^2_2\\) son las varianzas de muestras aleatorias independientes de tamaño \\(N_1\\) y \\(N_2\\), tomadas de dos poblaciones Normales con la misma varianza \\(\\sigma^2\\), entonces \\[F=\\frac{s^2_1}{s^2_2}\\,,\\] es el valor de una variable aleatria cuya distribución es \\(F\\) con los parámetros \\(\\nu_1=N_1-1\\) y \\(\\nu_2=N_2-1\\). Este estadístico es muy útil en tests de significancia para los picos de los espectros frecuenciales de potencia. Los dos parámetros son los grados de libertad para las varianzas del cociente; \\(\\nu_1\\) para \\(s^2_1\\) y \\(\\nu_2\\) para \\(s^2_2\\).\n{ests para hipótesis}\\ Para usar los test de significancia estadística debemos de seguir 5 pasos:\n(1) Definir el nivel de confianza\n(2) Definir la hipótesis nulla \\(H_0\\) y su alternativa \\(H_1\\)\n(3) Definir el estadístico que usaremos\n(4) Definir la región crítica\n(5) Evaluar el estadístico y concluir\n\nEs muy importante definir correctamente la hipótesis nula. Es decir, estar seguro que rechazar la hipótesis nula \\(H_0\\) implica únicamente la existencia de su alternativa \\(H_1\\). Normalmente la hipótesis nula y su alternativa son mutualmente excluyentes. Ejemplos:\n\n\\(H_0\\): Las medias de dos muestras son iguales \\(H_1\\): Las medias de dos muestras no son iguales\n\n\\(H_0\\): El coeficiente de correlación es cero \\(H_1\\): El coeficiente de correlación no es cero\n\n\\ En una muestra de 41 inviernos la temperatura media de Enero es \\(5.55^\\circ{C}\\) y la desviación es de \\(0.65^\\circ{C}\\) ?`Cual es el intervalo de confianza al 95% de que la verdadera temperatura media sea esa? \\ (1) Nivel de confianza del 95% \\ (2) \\(H_0\\) es que la media verdadera se encuentra en el intervalo \\(5.55\\pm \\Delta{T}\\) y su alternativa \\(H_1\\) es que se encuentra fuera de este intervalo.\n(3) Usamos el estadístico \\(t\\).\n(4) La región crítica es \\(|t|&lt;t_{0.025}\\), lo cual para \\(\\nu=N-1=40\\) es \\(|t|&lt;2.26\\) (leido de tablas estadísticas). Escrito en términos de intervalo de confianza para la media poblacional: \\[\\bar{x}-2.0211\\frac{s}{\\sqrt{N-1}}&lt;\\mu&lt;\\bar{x}+2.0211\\frac{s}{\\sqrt{N-1}}  \\]\n(5) Si ponemos en números el intervalo obtenemos \\(5.06&lt;\\mu&lt;6.03\\). Tenemos un 95% de confianza que la verdadera temperatura media se encuentra en ese intervalo.\n{eorema de Bayes:}\\ Sea \\(E_i\\,,i=1,2,3,...,n\\) un conjunto de \\(n\\) eventos que constituyen una partición del espacio muestral \\(S\\)\n\\[\\bigcup^n_{i=1}E_i\\in S,\\]\n{}cada uno de los cuales tiene probabilidad positiva de ocurrir \\(P(Ei)&gt;0\\) para \\(i=1,2,....,n\\) y son exclusivos entre si \\[E_i\\cap E_j=\\o \\,\\,\\,\\,i\\ne j\\,.\\] Entonces dada la ocurrencia previa de un evento cualquiera \\(B\\), la probabilidad de que suceda el evento \\(E_j\\) es\n\\[\\begin{equation}\nP(E_j|B)=\\frac{P(B|E_j)P(E_j)}{\\sum^n_{i=1}P(B|E_i)P(E_i)}\\,,\n\\end{equation}\\]\n{}donde\n\\[P(E_j|E_i)=\\frac{P(E_i\\cap E_j)}{P(E_i)}\\,,\\]\nes la probabilidad condicional, es deicr, la probabilidad que ocurra el evento \\(E_j\\) si previamente ha ocurrido el evento \\(E_i\\) y \\(P(E_i \\cap E_j)\\) es la probabilidad que ambos eventos ocurran, i.e. la intersección de dos eventos. La intersección es puede escribir\n\\[P(E_i\\cap E_j)=P(E_j|E_i)*P(E_i)=P(E_i|E_j)*P(E_j)\\,.\\]\nSi ambos eventos son independientes (no interseccionan) tal que \\(P(E_i|E_j)=P(E_i)\\) obtenemos\n\\[P(E_i\\cap E_j)=P(E_i)*P(E_j)\\,,\\]\nes decir, la definición de independencia estadística entre eventos.\n\n{jemplo teorema de Bayes:} Imaginemos que queremos saber si una muestra de agua contiene diatomeas o no. La probabilidad de que una muestra de agua tomada al azar en la bahía de Todos Santos tenga diatomeas es de \\(1/100\\) (\\(P(D)=0.01\\)). La probabilidad de que si hay diatomeas el test de negativo es cero (\\(P(+|D)=1\\)) y la probabilidad de que el test de un falso positivo es del \\(5\\%\\) (\\(P(+|noD)=0.05\\)). Si agarramos una muestra de agua y da positivo, ?`Cual es la probabilidad de que hayan diatomeas? Intuitivamente, sabemos que existe un \\(5\\%\\) de probabilidad de que el test me de un falso positivo y por lo tanto, un \\(95\\%\\) de que si da positivo tenga diatomeas en la muestra de agua. Veamos que nos dice el teorema de Bayes. Sabemos que \\(P(D)=0.01\\), \\(P(+|D)=1\\), y \\(P(+|noD)=0.05\\). Si usamos el teorema de Bayes:\n\\[ P(D|+)=\\frac{P(+|D)P(D)}{P(+|D)P(D) + P(+|noD)P(noD)}=\n          \\frac{1*0.01}{1*0.01+0.05*0.99}\\sim\\frac{1}{6}\\,.\\]\nEn verdad solamente existe \\(1\\) probabilidad de \\(6\\) (\\(\\sim16\\%\\)) que si el test es positivo existan diatomeas. De cada \\(100\\) muestras solamente \\(1\\) tiene diatomeas y \\(5\\%*100=0.05*100=5\\) muestras de las \\(100\\) dan positivo y no tienen diatomeas. Por ello, sabiendo que la muestra que tiene diatomeas dio positivo, de \\(6\\) positivos, solamente \\(1\\) es cierto."
  },
  {
    "objectID": "article2_new.html#repaso-de-álgebra-lineal",
    "href": "article2_new.html#repaso-de-álgebra-lineal",
    "title": "Julio Sheinbaum",
    "section": "Repaso de álgebra Lineal",
    "text": "Repaso de álgebra Lineal\nUna matriz es un elemento matemático compuesto por filas y columnas. Una matriz rectangular de dimensiones \\(m\\times n\\) se define como \\[{\\textbf A}=\\left(\\begin{array}{cccc}\n  a_{11} & a_{12} & .... & a_{1n}\\\\\n  a_{21} & a_{22} & .... & a_{2n}\\\\\n    .    &   .    &      &   . \\\\\n    .    &   .    &      &   . \\\\\n    .    &   .    &      &   . \\\\\n  a_{m1} & a_{m2} & .... & a_{mn}\\\\\n\\end{array}\n  \\right)\n\\,.\\]\nUn elemento de la matriz \\({\\textbf A}\\) queda definida como \\({\\textbf A}=[a_{ij}],\\,\\,\\,i=1,2,...,m; j=1,2,...,n\\,.\\) El primer índice \\(i\\) denota filas y el segundo \\(j\\) columnas.\n{efinición} Un vector es una matriz con solo una columna (\\(m\\times1\\)) \\[{\\textbf v}=\\left( \\begin{array}{c}\nv_1 \\\\ v_2 \\\\ . \\\\ . \\\\ . \\\\ v_m\n       \\end{array} \\right)\n       \\,.\\]\n{ectores ortogonales}\nSean dos vectores \\({\\textbf u}=[u_1,u_2,...,u_N]\\) y \\({\\textbf v}=[v_1,v_2,...,v_N]\\) de longitud \\(N\\), decimos que son ortogonales si \\[{\\textbf u}\\cdot{\\textbf v}=({\\textbf u},{\\textbf v})=\\sum\\limits^N_{i=1} u_i v_i={\\textbf u}^T{\\textbf v}=0\\,.\\]\n{orma, módulo, longitud de un vector}\nSea el vector \\({\\textbf u}=[u_1,u_2,...,u_N]\\), entonces la norma de dicho vector se define como \\[||{\\textbf u}||=|{\\textbf u}|=\\sqrt{u^2_1 + u^2_2 + ... + u^2_N}=({\\textbf u},{\\textbf u})^{1/2}=({\\textbf u}^T{\\textbf u})^{1/2}\\,.\\]\nEn general se puede obtener un {ector unitario} (de módulo \\(=1\\)) dividiendo el vector por su norma: \\[{\\textbf u}_I=\\frac{{\\textbf u}}{||{\\textbf u}||}\\,.\\]\n{} Vectores ortonormales\nDos vectores ortogonales \\({\\textbf u}\\) y \\({\\textbf v}\\) si tienen módulo la unidad, entonces se denominan vectores ortonormales.\n{uma de vectores}\nLa suma de 2 vectores \\({\\textbf u}=[u_1,u_2,...,u_N]\\) y \\({\\textbf v}=[v_1,v_2,...,v_N]\\) de longitud \\(N\\) se define como \\[{\\textbf u} + {\\textbf v}= [u_1 + v_1, u_2 + v_2,..., u_N + v_N]=\\sum\\limits^N_{i=1} u_i + v_i\\,.\\]\n{ombinación lineal de vectores}\nUn vector \\({\\textbf y}\\) se dice que es combinación lineal de un conjunto de vectores \\({\\textbf x}_1, {\\textbf x}_2, ...,{\\textbf x}_N\\) si se puede expresar como la suma de los \\(N\\) vectores multiplicados por \\(N\\) coeficientes escalares \\(a_1,a_2,...,a_N\\):\n\\[{\\textbf y}=a_1 {\\textbf x}_1 + a_2 {\\textbf x}_2 +...+a_N {\\textbf x}_N=\\sum\\limits^N_{i=1}a_i {\\textbf x}_i\\,.\\]\n{ndependencia lineal}\nUn conjunto de vectores \\({\\textbf y}_1, {\\textbf y}_2, ...,{\\textbf y}_N\\) se dice que es linealmente independiente si existe una combinación lineal finita de los vectores del conjunto tal que:\n\\[\\sum\\limits^N_{i=1} a_i {\\textbf y}_i=a_1{\\textbf y}_1 + a_2{\\textbf y}_2+...+a_N{\\textbf y}_N=0\\,,\\]\nque se satisface cuando no todos los coeficientes son cero. En caso contarrio, se dice que son linealmente dependientes.\n{} Ortonormalización Gram-Schmidt\nEs un método para convertir un conjunto de vectores \\({\\textbf v}\\) en vectores ortonormales. De forma general el proceso definido por Gram-Schmidt para ortonormalizar el vector ortogonal \\({\\textbf w}_k\\) a partir de un conjunto de vectores ortonormales \\({\\textbf u}_i=[{\\textbf u}_1, {\\textbf u}_2,...,{\\textbf u}_{k-1}]\\) se define como:\n\\[{\\textbf w}_k={\\textbf v}_k-\\sum\\limits^{k-1}_{i=1} {\\textbf u}_i\\cdot{\\textbf v}_k{\\textbf u}_i\\,.\\]\n\\[{\\textbf v}_k\\equiv\\,\\,{ vectores}\\,\\,{ originales}\\]\n\\[{\\textbf u}_i\\equiv\\,\\,{ vectores}\\,\\,{ ortonormales}\\]\n\\[{\\textbf w}_k\\equiv\\,\\,{ vectores}\\,\\,{ ortogonales}\\,\\,{ a}\\,\\,{\\textbf u}_j\\]\nEjemplo:\n\nConvertir el conjunto de vectores de la base \\(A\\) en una base ortonormal:\n\\[{\\textbf A}=\\left(\\begin{array}{cccc}\n  1 & 2 & 1\\\\\n  0 & 2 & 0\\\\\n  2 & 3 & 1\\\\\n  1 & 1 & 0\\\\\n\\end{array}\n  \\right)\n\\,.\\]\nPrimero normalizamos el primer vector columna \\({\\textbf v}_1\\): \\[{\\textbf u}_1=\\frac{{\\textbf v}_1}{||{\\textbf v}_1||}=\\left[\\frac{1}{\\sqrt{6}},0,\\frac{2}{\\sqrt{6}},\\frac{1}{\\sqrt{6}}\\right]\\,.\\]\n(1er vector ortonormal)\n\nAhora usamos la fórmula de arriba para encontrar el vector \\({\\textbf w}_2\\) ortogonal a \\({\\textbf u}_1\\)\n\\[{\\textbf w}_2={\\textbf v}_2- {\\textbf u}_1\\cdot{\\textbf v}_2{\\textbf u}_1=[2,2,3,1]-\n   \\left[\\frac{1}{\\sqrt{6}},0,\\frac{2}{\\sqrt{6}},\\frac{1}{\\sqrt{6}}\\right]\\cdot\n   [2,2,3,1]\\left[\\frac{1}{\\sqrt{6}},0,\\frac{2}{\\sqrt{6}},\\frac{1}{\\sqrt{6}}\\right]=\\]\n\\[=[2,2,3,1]-\\left(\\frac{9}{\\sqrt{6}}\\right)\\left[\\frac{1}{\\sqrt{6}},0,\\frac{2}{\\sqrt{6}},\\frac{1}{\\sqrt{6}}\\right]=\n   [2,2,3,1]-\\left[\\frac{3}{2},0,3,\\frac{3}{2}\\right]=\\left[ \\frac{1}{2},2,0,\\frac{-1}{2}\\right]\\,.\\]\n(vector ortogonal a \\(u_1\\))\nNormalizamos \\({\\textbf w}_2\\) para obtener el primer vector ortonormal a \\({\\textbf u}_1\\) \\[{\\textbf u}_2=\\frac{{\\textbf w}_2}{||{\\textbf w}_2||}=\\left[ \\frac{\\sqrt{2}}{6},\\frac{2\\sqrt{2}}{3},0,\\frac{-\\sqrt{2}}{6}\\right]\\]\n\n(2o vector ortonormal)\n\nAhora calculamos \\({\\textbf w}_3\\) en términos de \\({\\textbf u}_1\\) y \\({\\textbf u}_2\\) \\[{\\textbf w}_3=  {\\textbf v}_3- {\\textbf u}_1\\cdot{\\textbf v}_3{\\textbf u}_1 - {\\textbf u}_2\\cdot{\\textbf v}_3{\\textbf u}_2=\n  \\left[ \\frac{4}{9},\\frac{-2}{9},0,\\frac{-4}{9}\\right]\\,,\\] (vector ortogonal a \\(u_1\\) y \\(u_2\\)) \\ \\ y si normalizamos\n\n\\[{\\textbf u}_3=\\left[ \\frac{2}{3},\\frac{-1}{3},0,\\frac{-2}{3}\\right]\\,.\\]   (3er vector ortonormal)\n\nLa matriz o conjunto de vectores ortonormal es\n\\[{\\textbf A}=\\left(\\begin{array}{cccc}\n  \\frac{\\sqrt{6}}{6} & \\frac{\\sqrt{2}}{6} & \\frac{2}{3}\\\\\n  0 & \\frac{2\\sqrt{2}}{3} & \\frac{-1}{3}\\\\\n  \\frac{\\sqrt{6}}{3} & 0 & 0\\\\\n  \\frac{\\sqrt{6}}{6} & \\frac{-\\sqrt{2}}{6} & \\frac{-2}{3}\\\\\n\\end{array}\n  \\right)\n\\,\\] (base ortonormal)\n{} **Aplicación lineal*\nSean dos espacios vectoriales \\(V\\) y \\(W\\), decimos que una aplicación \\(f:V \\rightarrow W\\) es lineal si la `imagen’ de la combincación lineal es la combinación lineal de las imágenes. Es decir,\n\\[f(\\alpha {\\textbf u} + \\beta {\\textbf v} )=\\alpha f({\\textbf u}) + \\beta f({\\textbf v})\\,.\\]\nLa imagen de una aplicación es el resultado de aplicar al vector una aplicación.\nEjemplos:\n\nLa aplicación \\(f\\): \\({\\cal R}^3 \\rightarrow {\\cal R}^2\\) definida por \\(f(x,y,z)=(x+y,y+2z)\\) es lineal.\n\nDemostración: Definimos \\({\\textbf u}=[u_1,u_2,u_3]\\) y \\({\\textbf v}=[v_1,v_2,v_3]\\). Entonces \\(f\\) es lineal si \\[f(\\alpha (u_1,u_2,u_3) + \\beta(v_1,v_2,v_3))=\\alpha f(u_1,u_2,u_3) + \\beta f(v_1,v_2,v_3)=\\] \\[f(\\alpha u_1 + \\beta v_1,\\alpha u_2 + \\beta v_2,\\alpha u_3 + \\beta v_3 )=\\alpha [u_1 + u_2,u_2 + 2u_3] +\n\\beta [v_1 + v_2,v_2 + 2v_3]\\] \\[[\\alpha u_1 + \\beta v_1 + \\alpha u_2 + \\beta v_2,\\alpha u_2 + \\beta v_2 + 2 (\\alpha u_3 + \\beta v_3)]=\n[\\alpha u_1 + \\alpha u_2 + \\beta v_1 + \\beta v_2, \\alpha u_2 + 2\\alpha u_3 + \\beta v_2 + 2\\beta v_3]\\]\n\nLa aplicación \\(f\\): \\({\\cal R}^3 \\rightarrow {\\cal R}^2\\) definida por \\(f(x,y,z)=(x+y+1,y+2z)\\) no es lineal.\n\n\nLa aplicación \\({\\textbf R}\\): \\({\\cal R}^2 \\rightarrow {\\cal R}^2\\) definida por \\[{\\textbf x}'={\\textbf R}{\\textbf x}\\,,\\] \\[{\\textbf R}=\\left( \\begin{array}{cc}\ncos(\\theta) & -sin(\\theta)\\\\\nsin(\\theta) & cos(\\theta)\n   \\end{array} \\right)\\,,\\] rota el vector un ángulo \\(\\theta\\) en sentido contrario a las agujas del reloj. Si queremos cambiar el sentido de la rotación solo debemos tomar el signo de \\(\\theta\\) negativo. Aqui \\({\\textbf x}'\\) es el vector rotado o la imagen de la aplicación rotación.\n\n{atriz Identidad}\nSe define la matriz identidad \\({\\textbf I}\\) como una matriz diagonal compuesta por unos\n\\[{\\textbf I}=\\left(\\begin{array}{ccc}\n  1 & 0 & 0\\\\\n  0 & 1 & 0\\\\\n  0 & 0 & 1\n\\end{array}\\right)\\,.\\]\nEl producto matricial de cualquier matriz \\({\\textbf A}\\) por la matriz identidad \\({\\textbf I}\\) es igual a la matriz original \\[{\\textbf A}{\\textbf I}={\\textbf A}\\,.\\]\n{atriz transpuesta}\nSea \\({\\textbf A}\\) una matriz \\(m\\times n\\) con elementos \\([a_{ij}]\\,\\,{ para}\\,\\,i=1,2,...,m; j=1,2,...,n\\). Se define el elemento de su transpuesta como \\[{\\textbf A}^T=[a_{ji}]\\,.\\]\nLa inversa de la matriz transpuesta es \\[({\\textbf A}^T)^{-1}={\\textbf A}\\,,\\] si \\({\\textbf A}\\) es ortogonal. \\ {emostración}: \\[({\\textbf A}^T)^{-1}={\\textbf A}\\] \\[{\\textbf A}^T({\\textbf A}^T)^{-1}={\\textbf A}^T{\\textbf A}={\\textbf I}\\] \\[{\\textbf A}{\\textbf A}^T({\\textbf A}^T)^{-1}={\\textbf A}{\\textbf I}\\]\n{atriz Ortogonal}\nUna matriz \\({\\textbf A}\\) es ortogonal si \\({\\textbf A}{\\textbf A}^T={\\textbf A}^T{\\textbf A}={\\textbf I}\\,.\\)\n{atriz Diagonal}\nUna matriz es diagonal si únicamente contiene algunos elementos diferentes de cero en la diagonal principal (ceros en la matriz triangular superior e inferior).\n\\[{\\textbf D}=\\left(\\begin{array}{ccc}\n  a_{11} & 0 & 0\\\\\n  0 & a_{22} & 0\\\\\n  0 & 0 & a_{33}\n\\end{array}\\right)\n\\]\n{atriz Simétrica}\nUna matriz simétrica se define como aquella matriz \\({\\textbf A}\\) tal que sea igual a su traspuesta \\[{\\textbf A}={\\textbf A}^T\\,\\,(a_{ij}=a_{ji})\\]\n\\[{\\textbf A}=\\left(\\begin{array}{cccc}\n  a_{11} & a_{12} & a_{31}\\\\\n  a_{12} & a_{22} & a_{32}\\\\\n  a_{31} & a_{32} & a_{33}\\\\\n\\end{array}\\right)\n\\]\n{atriz Antisimétrica}\nUna matriz antisimétrica se define como aquella matriz \\({\\textbf A}\\) tal que sea igual a su traspuesta \\[{\\textbf A}=-{\\textbf A}^T\\,\\,(a_{ij}=-a_{ji})\\]\n\\[{\\textbf A}=\\left(\\begin{array}{cccc}\n  0 & a_{12} & a_{13}\\\\\n  -a_{12} & 0 & a_{23}\\\\\n  -a_{13} & -a_{23} & 0\\\\\n\\end{array}\\right)\n\\] \\ NOTA: Cualquier matriz \\({\\textbf A}\\) se puede descomponer en una parte simétrica y otra antisimétrica:\n\\[A=\\frac{1}{2}({\\textbf A}+{\\textbf A}^T) + \\frac{1}{2}({\\textbf A}-{\\textbf A}^T)\\,.\\]\n{atriz Singular}\nUna matriz singular es aquella matriz cuadrada cuyo determinante es igual a cero. Las matrices singulares no tienen matriz inversa. \\ \\ Ejemplo:\n\\[{\\textbf S}=\\left(\\begin{array}{cc}\n  3 & 2\\\\\n  6 & 4\n\\end{array}\\right)\n\\]\nSi nos fijamos las 2 filas de la matriz singular \\({\\textbf S}\\) son linealmente dependientes, es decir, podemos recuperar la segunda fila multiplicando la primera fila por 2. Si la matriz tiene columnas o filas linealmente dependientes, el determinante es cero.\nSi \\({\\textbf A}\\) es ortogonal\n\\[{\\textbf A}^{-1}={\\textbf A}^T\\] \\[{\\textbf A}^{-1}({\\textbf A}^T)^{-1}={\\textbf I}\\] \\[{\\textbf A}^{-1}\\underbrace{({\\textbf A}^T)^{-1}({\\textbf A}^T)}_{{\\textbf I}}={\\textbf A}^T\\]\nUna matriz cualquiera se puede convertir en matriz cuadrada si es multiplicada por su transpuesta.\n{eterminante de una matriz}\nEl determinante de una matriz \\({\\textbf A}\\) \\(3\\times 3\\) se puede calcular como \\[|{\\textbf A}|=\\left|\\begin{array}{cccc}\n  a_{11} & a_{12} & a_{13}\\\\\n  a_{21} & a_{22} & a_{23}\\\\\n  a_{31} & a_{32} & a_{33}\\\\\n\\end{array}\n  \\right|=a_{11}a_{22}a_{33} + a_{12}a_{23}a_{31} + a_{13}a_{21}a_{32}-(a_{31}a_{22}a_{13} + a_{32}a_{23}a_{11} +\n          a_{33}a_{21}a_{12})\\,.\\]\nPara matrices de orden superior se puede utilizar la formula de adjuntos. El determinante de una matriz \\(n\\times n\\) es el producto escalar entre cualquier fila o columna con sus adjuntos \\[|{\\textbf A}|=a_{i1}C_{i1} + a_{i2}C_{i2}+...+a_{in}C_{in}\\,,\\] donde los adjuntos \\(C_{ij}\\) son subdeterminantes (de orden \\(n-1\\), sin contar la columna j y fila i) con el signo adecuado \\[C_{ij}=(-1)^{i+j}|{\\textbf M}_{ij}|\\,.\\]\n{ango de una matriz} \\ \\ {efinición 1}: El rango de una matriz se define como el número de filas o columnas de la matriz que son linealmente independientes. \\ \\ {efinición 2}: El orden de la mayor submatriz cuadrada no nula\nAsí para calcular el rango debemos de hacer cero todos los elementos posibles de la matriz hasta obtener una submatriz no nula que nos indicará el rango de la matriz.\n\\[\\left[\\begin{array}{cccc}\n  1 & 2 & 1\\\\\n  -2 & -3 & 1\\\\\n  3 & 5 & 0\\\\\n\\end{array}\\right]\\underbrace{\\rightarrow}_{2r_1+r_2}\n\\left[\\begin{array}{cccc}\n1 & 2 & 1\\\\\n  0 & 1 & 3\\\\\n  3 & 5 & 0\\\\\n\\end{array}\\right]\\underbrace{\\rightarrow}_{-3r_1+r_3}\n\\left[\\begin{array}{cccc}\n1 & 2 & 1\\\\\n  0 & 1 & 3\\\\\n  0 & -1 & -3\\\\\n\\end{array}\\right]\\underbrace{\\rightarrow}_{r_2+r_3}\n\\left[\\begin{array}{cccc}\n1 & 2 & 1\\\\\n  0 & 1 & 3\\\\\n  0 & 0 & 0\\\\\n\\end{array}\\right]\\underbrace{\\rightarrow}_{-2r_2+r_1}\n\\left[\\begin{array}{cccc}\n1 & 0 & -5\\\\\n  0 & 1 & 3\\\\\n  0 & -1 & -3\\\\\n\\end{array}\\right]\\,,\\] y entonces el rango de la matriz es 2. El rango lo podíamos haber encontrado simplimente viendo que sustrayendo la segunda fila de la matriz a la primera obtenemos la tercera fila; solamente hay 2 vectores linealmente independientes.\n{atriz inversa}\nLa matriz cuadrada \\({\\textbf A}\\) es invertible si existe una matriz \\({\\textbf A}^{-1}\\) tal que \\[{\\textbf A}^{-1}{\\textbf A}={\\textbf I}\\,\\,\\,{ and}\\,\\,\\,{\\textbf A}{\\textbf A}^{-1}={\\textbf I}\\,.\\]\n{roposición} Si \\({\\textbf A}\\) es invertible entonces la única solución de \\({\\textbf A}{\\textbf x}={\\textbf b}\\) es \\[{\\textbf x}={\\textbf A}^{-1}{\\textbf b}\\,.\\]\n{roposición} Si \\({\\textbf A}\\) es una matriz cuadrada de dimensión \\(2\\times 2\\) y \\(|{\\textbf A}|=a_{11}a_{22} - a_{12}a_{21}\\) no es cero, entonces \\[{\\textbf A}^{-1}=\\left(\\begin{array}{cc}\n  a_{11} & a_{12} \\\\\n  a_{21} & a_{22} \\\\\n\\end{array}\n  \\right)^{-1} =\n  \\frac{1}{|{\\textbf A}|}\n  \\left(\\begin{array}{cc}\n  a_{22} & -a_{12} \\\\\n  -a_{21} & a_{11} \\\\\n\\end{array}\n  \\right)\n\\,.\\]\n{álculo de matriz inversa con método Gauss-Jordan}\nEl método de Gauss-Jordan usa la siguiente identidad matemática \\([{\\textbf A}|{\\textbf I}]\\rightarrow [{\\textbf I}|{\\textbf A}^{-1}]\\). \\ {jemplo:} Calcula la matriz inversa de \\[\\left(\\begin{array}{cc}\n  2 & 3 \\\\\n  4 & 7 \\\\\n\\end{array}\n  \\right)\\]\n\\[[{\\textbf A}  {\\textbf I}]=\n  \\left(\n        \\begin{array}{ccccc}\n  2 & 3 & & 1 & 0\\\\\n  4 & 7 & & 0 & 1\\\\\n        \\end{array}\n  \\right)\n  \\underbrace{\\rightarrow}_{-2r_1+r_2}\n  \\left(\n        \\begin{array}{ccccc}\n  2 & 3 & & 1 & 0\\\\\n  0 & 1 & & -2 & 1\\\\\n        \\end{array}\n  \\right)\n  \\underbrace{\\rightarrow}_{-3r_2+r_1}\n  \\left(\n        \\begin{array}{ccccc}\n  2 & 0 & & 7 & -3\\\\\n  0 & 1 & & -2 & 1\\\\\n        \\end{array}\n  \\right)\\rightarrow\\] \\[\\underbrace{\\rightarrow}_{r_1/2}\n  \\left(\n        \\begin{array}{ccccc}\n  1 & 0 & & 7/2 & -3/2\\\\\n  0 & 1 & & -2 & 1\\\\\n        \\end{array}\n  \\right)\n  \\,.\n  \\]\nEntonces\n\\[{\\textbf A}^{-1}=\\left(\n        \\begin{array}{ccccc}\n  7/2 & -3/2\\\\\n  -2 & 1\\\\\n        \\end{array}\n  \\right)\\,.\\]\n \\[{\\textbf A}=\\left[\\begin{array}{cccc}\n  1 & -1 & 2\\\\\n  2 & 0 & 3\\\\\n  0 & 1 & -1\\\\\n\\end{array}\\right]\\rightarrow\n{\\textbf A}^{-1}=\\left[\\begin{array}{cccc}\n  3 & -1 & 3\\\\\n  -2 & 1 & -1\\\\\n  -2 & 1 & -2\\\\\n\\end{array}\\right]\\,.\\]\n{actorización LU}\nEs la descomposición de una matriz en una matriz triangular inferior \\({\\textbf L}\\) y una matriz triangular superior \\({\\textbf U}\\), es decir \\[{\\textbf A}={\\textbf L}{\\textbf U}\\,,\\] donde \\[{\\textbf L}=\\left(\\begin{array}{cccc}\n  a_{12} & 0\\\\\n  a_{21} & a_{22}\\\\\n\\end{array}\\right)\\,\\] es una matriz triangular inferior (L; lower) y \\[\n{\\textbf U}=\\left(\\begin{array}{cccc}\n  a_{12} & a_{21}\\\\\n  0 & a_{12}\\\\\n\\end{array}\\right)\\,,\\] es una matriz triangular superior (U; superior).\\ Esta factorización se suele usar para resolver un sistema de ecuaciones lineal \\({\\textbf A}{\\textbf x}={\\textbf b}\\) y no es única. Es decir, pueden existir más de una factorización. Si sustituimos la definición de arriba \\[{\\textbf A}{\\textbf x}=({\\textbf L}{\\textbf U}){\\textbf x}={\\textbf b}\\,,\\] lo que implica que \\[{\\textbf L}({\\textbf U}{\\textbf x})={\\textbf b}\\,.\\] \\ Si definimos \\({\\textbf U}{\\textbf x}={\\textbf z}\\), entonces tenemos que \\[{\\textbf L}{\\textbf z}={\\textbf b}\\,.\\] Como \\({\\textbf L}\\) es una matriz triangular inferior, podemos resolver para \\({\\textbf z}\\) utilizando sustitución hacia delante. Luego, como \\({\\textbf U}\\) es una matriz triangular superior, resolvemos \\({\\textbf U}{\\textbf x}={\\textbf z}\\) por sustitución en reversa.\n{jemplo:}\nEncuentre la descomposición LU de la matriz \\[\\left(\\begin{array}{ccccc}\n  2 & 5\\\\\n-3 & -4\\\\\n        \\end{array}\\right)\\,.\n\\]\nSi multiplicamos la primera fila por \\(L_{21}=3/2\\) y le sumamos la segunda fila hacemos cero el elemento \\(a_{21}=-3\\) \\[{\\textbf U}=\\left(\\begin{array}{ccccc}\n  2 & 5\\\\\n0 & 7/2\\\\\n        \\end{array}\\right)\\,.\n\\] Esta matriz ya es una matriz triangular superior, es decir, la matriz \\({\\textbf U}\\). Para encontrar la matriz triangular inferior solo debemos de conocer el valor del elemento \\(L_{21}\\). Ese elemento es el multiplicador con signo opuesto usado en la eliminación de Gauss-Jordan. Es decir, \\(L_{21}=-3/2\\). La matriz \\({\\textbf L}\\) es \\[{\\textbf L}=\\left(\\begin{array}{ccccc}\n  1 & 0\\\\\n-3/2 & 1\\\\\n        \\end{array}\\right)\\,.\\]\nVamos a comprobar \\[\\left(\\begin{array}{ccccc}\n  1 & 0\\\\\n-3/2 & 1\\\\\n        \\end{array}\\right)\n    \\left(\\begin{array}{ccccc}\n  2 & 5\\\\\n0 & 7/2\\\\\n        \\end{array}\\right)\n    =\n    \\left(\\begin{array}{ccccc}\n  2 & 5\\\\\n-3 & -4\\\\\n        \\end{array}\\right)\\,.\\]\n{jemplo:}\nResuelva el siguiente sistema de ecuaciones con la descomposición LU \\[2x_1+3x_2+4x_3=6\\] \\[4x_1+5x_2+10x_3=16\\] \\[4x_1+8x_2+2x_3=2\\]\nLa matriz de coeficientes es \\[{\\textbf A}=\\left(\\begin{array}{ccccc}\n  2 & 3 & 4\\\\\n  4 & 5 & 10\\\\\n  4 & 8 & 2 \\\\\n        \\end{array}\\right)\\,.\n\\]\nSi factorizamos \\[{\\textbf L}=\\left(\\begin{array}{ccccc}\n  1 & 0 & 0\\\\\n  2 & 1 & 0\\\\\n  2 & -2 & 1 \\\\\n        \\end{array}\\right)\\,,\n\\]\ny\n\\[{\\textbf U}=\\left(\\begin{array}{ccccc}\n  2 & 3 & 4\\\\\n  0 & -1 & 2\\\\\n  0 & 0 & 0 \\\\\n        \\end{array}\\right)\\,.\\]\nSi utilizamos la identidad \\({\\textbf L}{\\textbf z}={\\textbf b}\\) obtenemos \\[\\left(\\begin{array}{ccccc}\n  1 & 0 & 0\\\\\n  2 & 1 & 0\\\\\n  2 & -2 & 1 \\\\\n        \\end{array}\\right)\n    \\left(\\begin{array}{c}\n  z_1 \\\\\n  z_2 \\\\\n  z_3 \\\\\n        \\end{array}\\right)=\n    \\left(\\begin{array}{c}\n  6 \\\\\n  16 \\\\\n  2 \\\\\n        \\end{array}\\right)\\,.\\]\nSi resolvemos para \\({\\textbf z}\\) \\[z_1=6\\] \\[z_2=16-2z_1=4\\] \\[z_3=2+2z_2-2z_1=-2\\]\nAsí que \\[{\\textbf z}=\\left(\\begin{array}{c}\n  6 \\\\\n  4 \\\\\n  -2 \\\\\n        \\end{array}\\right)\\,.\\]\nSi utilizamos ahora la definición \\({\\textbf U}{\\textbf x}={\\textbf z}\\,,\\) \\[\\left(\\begin{array}{ccccc}\n  2 & 3 & 4\\\\\n  0 & -1 & 2\\\\\n  0 & 0 & 0 \\\\\n        \\end{array}\\right)\n    \\left(\\begin{array}{c}\n  x_1 \\\\\n  x_2 \\\\\n  x_3 \\\\\n        \\end{array}\\right)=\n    \\left(\\begin{array}{c}\n  6 \\\\\n  4 \\\\\n  -2 \\\\\n        \\end{array}\\right)\\,,\\] y obtenemos \\[x_3=1\\] \\[x_2=\\frac{4-2x_3}{-1}=-2\\] \\[x_1=\\frac{6-4x_3-3x_2}{2}=4\\]\nPor lo tanto la solución del sistema de ecuaciones es \\[{\\textbf x}=\\left(\\begin{array}{c}\n  4 \\\\\n  -2\\\\\n  1 \\\\\n        \\end{array}\\right)\\,.\\]\n{alores propios y vectores propios}\nSea \\({\\textbf A}\\) una matriz cuadrada, un número real \\(\\lambda\\) se dice que es un valor propio de \\({\\textbf A}\\) si existe un vector, diferente del vector cero, \\({\\textbf x}\\) tal que \\[{\\textbf A}{\\textbf x}=\\lambda{\\textbf x}\\,.\\]\nEs decir, \\({\\textbf x}\\) es un vector que al transformarlo mediante la multiplicación por \\({\\textbf A}\\) el vector resultante mantiene la misma dirección; solamente se modifica su longitud (magnitud) y/o sentido. El valor propio \\(\\lambda\\) nos informa si el vector propio \\({\\textbf x}\\) se acorta o alarga o cambia de signo cuando es multiplicado por \\({\\textbf A}\\).\n{efinición} El número \\(\\lambda\\) es un valor propio si y solo si \\[|{\\textbf A}-\\lambda{\\textbf I}|=0\\,.\\]\n{jemplo:}\nCalcula los valores propios y vectores propios de la matriz \\[{\\textbf A}=\\left(\\begin{array}{ccccc}\n  1 & 2\\\\\n  2 & 4\\\\\n        \\end{array}\\right)\\,.\\]\nSabemos \\[|{\\textbf A}-\\lambda{\\textbf I}|=\\left|\\begin{array}{ccccc}\n  1-\\lambda & 2\\\\\n  2 & 4-\\lambda\\\\\n        \\end{array}\\right|=\\lambda^2-5\\lambda=0\\,.\\]\nEl polinomio de arriba se llama polinomio característico y es igual a cero cuando \\(\\lambda\\) es un valor propio. Resolviendo obtenemos dos soluciones \\(\\lambda=0\\) y \\(\\lambda=5\\). Ahora para encontrar los vectores propios debemos resolver el sistema \\(({\\textbf A}-\\lambda{\\textbf I}){\\textbf x}=0\\) separadamente para las dos \\(\\lambda\\): \\[({\\textbf A}-0{\\textbf I}){\\textbf x}=\n\\left(\\begin{array}{ccccc}\n  1 & 2\\\\\n  2 & 4\\\\\n      \\end{array}\\right)\n      \\left(\\begin{array}{c}\n  x_1 \\\\\n  x_2 \\\\\n      \\end{array}\\right)=\n      \\left(\\begin{array}{c}\n  0 \\\\\n  0 \\\\\n      \\end{array}\\right)\\,\\,\\,\\rightarrow\\,\\,\\,{\\textbf x}=\\left(\\begin{array}{c}\n  2 \\\\\n  -1 \\\\\n      \\end{array}\\right)\\,\\,\\,{ para}\\,\\,\\,\\lambda_1=0\\,,\\]\ny\n\\[({\\textbf A}-5{\\textbf I}){\\textbf x}=\n\\left(\\begin{array}{ccccc}\n  -4 & 2\\\\\n  2 & -1\\\\\n      \\end{array}\\right)\n      \\left(\\begin{array}{c}\n  x_1 \\\\\n  x_2 \\\\\n      \\end{array}\\right)=\n      \\left(\\begin{array}{c}\n  0 \\\\\n  0 \\\\\n      \\end{array}\\right)\\,\\,\\,\\rightarrow\\,\\,\\,{\\textbf x}=\\left(\\begin{array}{c}\n  1 \\\\\n  2 \\\\\n      \\end{array}\\right)\\,\\,\\,{ para}\\,\\,\\,\\lambda_2=5\\,.\\]\n Calcule los valores y vectores propios de la matriz de rotación\n\\[{\\textbf R}=\n\\left(\\begin{array}{ccccc}\n  cos\\theta & -sen\\theta\\\\\n  sen\\theta & cos\\theta\\\\\n      \\end{array}\\right)\\,.\\]\n{ultiplicación de matrices}\nEl producto matricial de dos matrices \\({\\textbf A}\\) (\\(m\\times n\\)) y \\({\\textbf B}\\) (\\(n\\times p\\)) se define como \\[{\\textbf C}={\\textbf A}{\\textbf B}\\,,\\] donde \\({\\textbf C}\\) es una matriz \\(m\\times p\\), con el elementi \\((i,j)\\) definido por \\[c_{ij}=\\sum^n_{k=1} a_{ik} b_{kj}\\,,\\] para todo \\(i=1,2,...,m; j=1,2,...,p\\).\n{ultiplicación de matrices}\nSea \\({\\textbf A}\\) una matriz \\(m\\times n\\), y \\({\\textbf v}\\) un vector \\(n\\times 1\\), entonces el elemento del producto \\[{\\textbf z}={\\textbf A}{\\textbf v}\\] viene dado por \\[z_{i}=\\sum^n_{k=1} a_{ik} v_{k}\\,,\\] para todo \\(i=1,2,....,m\\). Similarmente, si \\({\\textbf u}\\) es un vector \\(m\\times 1\\), entonces el elemento del producto \\[{\\textbf z}^T={\\textbf u}^T{\\textbf A}\\] viene dado por \\[z_{i}=\\sum^n_{k=1} a_{ki} u_{k}\\,,\\] para todo \\(i=1,2,....,n\\). Finalmente, el escalar que resulta del producto \\(\\alpha={\\textbf u}^T{\\textbf A}{\\textbf v}\\,,\\) viene dado por \\[\\alpha=\\sum^m_{j=1}\\sum^n_{k=1}a_{jk} u_j v_k\\,.\\]\n{roposición:}\nSea \\({\\textbf C}={\\textbf A}{\\textbf B}\\) una matriz \\(m\\times p\\), entonces \\[{\\textbf C}^T={\\textbf B}^T {\\textbf A}^T\\,.\\]\n{roposición} Sea \\({\\textbf A}\\) y \\({\\textbf B}\\) matrices cuadradas \\(n\\times n\\) invertibles. Sea el producto matricial \\({\\textbf C}={\\textbf A}{\\textbf B}\\), entonces \\[{\\textbf C}^{-1}={\\textbf B}^{-1} {\\textbf A}^{-1}\\,.\\]\n{erivada de matrices}\n{roposición:}\nSea \\[{\\textbf y}={\\textbf A}{\\textbf x}\\,,\\] donde \\({\\textbf y}\\) es \\(m\\times 1\\), \\({\\textbf x}\\) es \\(n\\times 1\\), \\({\\textbf A}\\) es \\(m\\times n\\), y \\({\\textbf A}\\) no depende de \\({\\textbf x}\\), entonces la derivada de \\({\\textbf y}\\) es \\[\\frac{\\partial{\\textbf y}}{\\partial{\\textbf x}}={\\textbf A}\\,.\\]\n{roposición:}\nSea \\[{\\textbf y}={\\textbf A}{\\textbf x}\\,,\\] donde \\({\\textbf y}\\) es \\(m\\times 1\\), \\({\\textbf x}\\) es \\(n\\times 1\\), \\({\\textbf A}\\) es \\(m\\times n\\), y \\({\\textbf A}\\) no depende de \\({\\textbf x}\\). Supongamos que \\({\\textbf x}\\) es una función del vector \\({\\textbf z}\\), mientras que \\({\\textbf A}\\) es independiente de \\({\\textbf z}\\). Entonces \\[\\frac{\\partial{\\textbf y}}{\\partial{\\textbf z}}={\\textbf A}\\frac{\\partial{\\textbf x}}{\\partial{\\textbf z}}\\,.\\]\n{roposición:}\nSea el escalar \\(\\alpha\\) definido como \\[\\alpha={\\textbf x}^T{\\textbf A}{\\textbf x}\\,,\\] donde \\({\\textbf x}\\) es \\(n\\times 1\\), \\({\\textbf A}\\) es \\(n\\times n\\), y \\({\\textbf A}\\) es independiente de \\({\\textbf x}\\), entonces \\[\\frac{\\partial{\\alpha}}{\\partial{{\\textbf x}}}={\\textbf x}^T({\\textbf A}+{\\textbf A}^T)={\\textbf x}^T{\\textbf A}^T + {\\textbf x}^T{\\textbf A}={\\textbf x}({\\textbf A}^T+{\\textbf A})\\]\n{roposición:}\nSea el escalar \\(\\alpha\\) definido como \\[\\alpha={\\textbf y}^T{\\textbf A}{\\textbf x}\\,,\\] donde \\({\\textbf y}\\) es \\(m\\times 1\\), \\({\\textbf x}\\) es \\(n\\times 1\\), y \\({\\textbf A}\\) es \\(m\\times n\\), y \\({\\textbf A}\\) es independiente de \\({\\textbf x}\\) y \\({\\textbf y}\\), entonces \\[\\frac{\\partial{\\alpha}}{\\partial{{\\textbf x}}}={\\textbf y}^T{\\textbf A}\\] y \\[\\frac{\\partial{\\alpha}}{\\partial{{\\textbf y}}}={\\textbf x}^T{\\textbf A}\\,.\\]\n{efinición} Sea \\({\\textbf A}\\) una matriz \\(m\\times n\\) cuyos elementos son funciones de un escalar \\(\\alpha\\). Entonces la derivada de \\({\\textbf A}\\) con respecto \\(\\alpha\\) es una matriz \\(m\\times n\\) compuesta por las derivadas de elemento por elemento: \\[\\frac{\\partial{\\textbf A}}{\\partial{\\alpha}}=\\left(\\begin{array}{cccc}\n  \\frac{\\partial{a_{11}}}{\\partial{\\alpha}} & \\frac{\\partial{a_{12}}}{\\partial{\\alpha}} & .... &\n   \\frac{\\partial{a_{1n}}}{\\partial{\\alpha}}\\\\\n  \\frac{\\partial{a_{21}}}{\\partial{\\alpha}} & \\frac{\\partial{a_{22}}}{\\partial{\\alpha}} & .... &\n   \\frac{\\partial{a_{2n}}}{\\partial{\\alpha}}\\\\\n    .    &   .    &      &   . \\\\\n    .    &   .    &      &   . \\\\\n    .    &   .    &      &   . \\\\\n  \\frac{\\partial{a_{m1}}}{\\partial{\\alpha}} & \\frac{\\partial{a_{m2}}}{\\partial{\\alpha}} & .... &\n   \\frac{\\partial{a_{mn}}}{\\partial{\\alpha}}\\\\\n\\end{array}\n  \\right)\n\\,.\\]"
  },
  {
    "objectID": "article2_new.html#cuadrados-mínimos-y-regresión",
    "href": "article2_new.html#cuadrados-mínimos-y-regresión",
    "title": "Julio Sheinbaum",
    "section": "Cuadrados mínimos y regresión",
    "text": "Cuadrados mínimos y regresión\nEn esta sección se van a introducir algunos modelos lineales estadísticos o modelos de regresión. Aqui se incluyen ajustes lineales por cuadrados mínimos, coeficientes de correlación, regresión múltiple, etc.\n\nMétodos de cuadrados mínimos\nEstos métodos son utilizados para ajustar un modelo dependiente de un conjunto compuesto por \\(k\\) variables independientes \\(x_k;i=1,2,...,k\\).\n{A) Mínimos cuadrados lineales} \\ Empezamos aplicando el método en términos de estimación lineal. Nos referimos a lineal en cuanto a los coeficientes \\(b_0, b_1,...,b_k\\), es decir, \\(y=b_0 + b_1x_1 + \\epsilon\\) es lineal, pero \\(y=b_0+sin(b_1x_1)\\) no lo es. \\\n\n{juste de una recta a un conjunto de datos} \\ Queremos usar los mejores coeficientes \\(b_0\\) y \\(b_1\\) en el sentido que se reduzca la desviación estándar de la recta ajustada versus los datos. Sea \\(i=1,2,...,N\\) observaciones, entonces \\[y_i=\\hat{y_i} + \\epsilon\\,,\\] donde \\[\\hat{y_i}=b_0+b_1 x_i\\] es el estimador estadístico y \\(\\epsilon\\) es el residuo (medida de la diferencia de la recta ajustada versus conjunto de puntos). Para encontrar \\(b_0, \\, b_1\\) debemos de minimizar la suma de los errores cuadrados (SEC), donde SEC es la varianza total que no es explicada por nuestro modelo de regresión lineal \\[SEC=\\sum^N_{i=1} \\epsilon^2_i=\\sum^N_{i=1} (y_i - \\hat{y_i})^2=\\sum^N_{i=1} [y_i - (b_0+b_1 x_i)]^2\\,.\\] \\\n\nLa suma de los cuadrados totales se define como \\[SCT=\\sum^N_{i=1} (y_i - \\bar{y})^2\\,,\\] y la suma de los cuadrados residuales \\[SCR=\\sum^N_{i=1} (\\hat{y_i} - \\bar{y})^2\\,.\\] \\ La SCT es proporcional a la varianza de los datos y SCR es proporcional a la cantidad de varianza explicada por nuestra regresión. La varianza total (o SCT) se puede descomponer en función de SCR y SEC como: \\[SCT=\\sum^N_{i=1}(y_i-\\bar{y})^2 = \\sum^N_{i=1}(\\hat{y_i} + \\epsilon_i - \\bar{y})^2\n=\\sum^N_{i=1}((\\hat{y_i}- \\bar{y}) + \\epsilon_i)^2=\\sum^N_{i=1}(\\hat{y_i}- \\bar{y})^2 +\\] \\[+\\sum^N_{i=1}\\epsilon_i^2 + 2\\sum^N_{i=1}(\\hat{y_i}- \\bar{y})\\epsilon_i\\,,\\] y por lo tanto \\[SCT=SCR + SEC\\,\\] \\ ya que por las ecuaciones normales \\(2\\sum^N_{i=1}(\\hat{y_i}- \\bar{y})\\epsilon_i=0\\).\n\nEl problema de mínimos cuadrados consiste en minimizar la SEC con respecto los coeficientes, cuyas condiciones son \\[\\frac{\\partial{SEC}}{\\partial{b_0}}=0; \\frac{\\partial{SEC}}{\\partial{b_1}}=0\\,.\\]\n\nSubstituyendo obtenemos para \\(b_0\\) \\[\\frac{\\partial{SEC}}{\\partial{b_0}} =\\frac{\\partial}{\\partial{b_0}}\n  \\left\\{\n    \\sum^N_{i=1} [y_i - (b_0+b_1 x_i)]^2\n  \\right\\}=\n  -2\\sum^N_{i=1} [y_i - (b_0+b_1 x_i)]=\\] \\[=-2\\left(\\sum^N_{i=1}y_i -N b_0 - b_1\\sum^N_{i=1} x_i  \\right)=0\\,,\\] y para \\(b_1\\) \\[\\frac{\\partial{SEC}}{\\partial{b_1}} =\\frac{\\partial{}}{\\partial{b_1}}\n  \\left\\{\n    \\sum^N_{i=1} [y_i - (b_0+b_1 x_i)]^2\n  \\right\\}=\n  -2\\sum^N_{i=1} x_i [y_i - (b_0+b_1 x_i)]=\\] \\[=-2\\left(\\sum^N_{i=1}x_i y_i - b_0\\sum^N_{i=1}x_i - b_1\\sum^N_{i=1} x^2_i \\right)=0\\,.\\]\n\nSi resolvemos para \\(b_0\\)\n\\[b_0=\\bar{y}-b_1\\bar{x}\\,,\\]\ny sustituimos en la segunda ecuación, obtenemos:\n\\[b_1=\\frac{N \\sum\\limits^N_{i=1} x_i y_i - \\sum\\limits^N_{i=1}x_i \\sum\\limits^N_{i=1}y_i}\n           {N\\sum\\limits^N_{i=1}x^2_i-\\left(\\sum\\limits^N_{i=1}x_i \\right)^2}=\n      \\frac{\\sum\\limits^N_{i=1} x_i y_i - \\frac{1}{N}\\sum\\limits^N_{i=1}x_i \\sum\\limits^N_{i=1}y_i}\n           {\\sum\\limits^N_{i=1}x^2_i-\\frac{1}{N}\\left(\\sum\\limits^N_{i=1}x_i \\right)^2}=\\] \\[=\\frac{(N-1)C_{xy}}{(N-1)s^2_x}=\\frac{\\sum\\limits^N_{i=1} (x_i-\\bar{x})(y_i-\\bar{y})}\n                                      {\\sum\\limits^N_{i=1} (x_i-\\bar{x})^2}=\\frac{&lt;x'y'&gt;}{&lt;x'^2&gt;}\\,,\\] donde \\(C_{xy}=&lt;x'y'&gt;\\) es la covarianza entre \\(x\\) e \\(y\\), y \\(s^2_x=&lt;x'^2&gt;\\) es la varianza de \\(x\\). Así, una vez hemos calculado las medias de las variables \\(x\\) e \\(y\\), podemos encontrar el coeficiente \\(b_1\\) y, a partir de este, calcular el segundo coeficiente \\(b_0\\).\n\nSi sustituimos \\(b_0=\\bar{y}-b_1\\bar{x}\\) en la ecuación de la regresión lineal \\(\\hat{y_i}=b_0+b_1 x_i\\)\nobtenemos\n\\[\\hat{y_i}=\\bar{y} +b_1(x_i-\\bar{x})\\,,\\] o\n\\[\\hat{y_i}=\\bar{y} +b_1{x'_i}\\,,\\] o finalmente\n\\[\\hat{y'_i}=b_1{x'_i}\\,,\\]\nque nos informa que cuando \\(x'_i=0\\,(x_i=\\bar{x})\\) entonces \\(\\hat{y'_i}=0 \\,(\\hat{y}=\\bar{y})\\), es decir, la recta pasa por el punto \\((\\bar{x},\\bar{y})\\), de tal forma, que puesto que \\(\\partial{SEC}/{\\partial{b_0}}=0\\) minimiza la suma del error, \\(\\sum \\epsilon_i=0\\), los puntos estan dispersos respecto a la recta ajustada de tal forma que los residuos positivos (\\(\\epsilon&gt;0\\)) siempre se cancelan con los residuos negativos (\\(\\epsilon&lt;0\\)). El parámetro \\(b_0\\) se interpreta como la intersección (corte con el eje \\(y\\)) y \\(b_1\\) es la pendiente de la recta ajustada.\n\nEl cociente\n\\[100(SCR/SCT)\\,,\\]\nes el porcentaje de varianza explicada por nuestra regresión lineal (varianza explicada/varianza total) y nos informa de la bondad del ajuste denominado coeficiente de correlación, \\(r^2\\). Si la regresión se ajusta perfectamente a todos los datos, todos los residuos son cero y por lo tanto \\(SEC=0\\) y \\(SCR/SCT=r^2=1\\). A medida que el ajuste empeora el coeficiente \\(r^2\\) disminuye hasta un mínimo posible de \\(r^2=0\\).\n{rror estándar de la estimación}\nUna medida de la magnitud absoluta de la bondad del ajuste es el error estándar del estimado, \\(s_{\\epsilon}\\), definido como\n\\[s_{\\epsilon}=[SEC/(N-2)]^{1/2}=\\left[ \\frac{1}{N-2}\\sum\\limits^N_{i=1} (y-\\hat{y})^2\\right]^{1/2}\\,.\\]\nEl número de grados de libertad, \\(N-2\\), se debe a que necesitamos estimar dos parámetros para encontrar realizar la regresión lineal. Si \\({\\epsilon}\\) es una variable aleatoria que sigue una distribución Normal de media cero y desviación estándar \\(s_{\\epsilon}\\), entonces, el \\(68.3\\%\\) de las observaciones caen dentro del intervalo \\(\\pm 1s_{\\epsilon}\\) unidades de la recta ajustada, \\(95.4\\%\\) caerá dentro del intervalo \\(\\pm 2 s_{\\epsilon}\\) unidades de la recta, y \\(99.7\\%\\) caerán en el intervalo \\(\\pm 3 s_{\\epsilon}\\) unidades de la recta. \\(s_{\\epsilon}\\) es la desviación estándar de \\(y\\) alrededor de su media, i.e., la recta ajustada \\(b_0 + b_1 x\\).\nGeneralización de mínimos cuadrados en notación matricial\nSupongamos el modelo dependiente de \\(k\\) variables independientes \\(X_k\\) \\[Y=b_0 + b_1 X_1 + b_2 X_2 + .... + b_k X_k + \\epsilon\\,,\\] y supongamos que hacemos N observaciones independientes \\(y_1, y_2,....,y_N\\) de \\(Y\\). Por lo tanto podemos escribir el modelo como \\[y_i=b_0 + b_1 x_{i1} + b_2 x_{i2} + ... + b_k x_{ik} + \\epsilon_i\\,,\\] donde \\(x_{ij}\\) es la observación \\(i\\) del de la variable independiente \\(j\\). Es decir, \\[N;\\,\\,{ observaciones}\\] \\[k;\\,\\,{ variables\\,\\,independientes}\\] \\[k+1;\\,\\,{ coeficientes}\\]\nSi escribimos en notación matricial \\[{\\textbf Y}=\\left(\\begin{array}{c}\n  y_1\\\\\n  y_2\\\\\n  ...\\\\\n  ...\\\\\n  ...\\\\\n  y_N\n      \\end{array}\\right)\\,\\,\\,{\\textbf X}=\\left(\\begin{array}{cccc}\n  1 & x_{11} & ... & x_{1k}\\\\\n  1 & x_{21} & ... & x_{2k}\\\\\n  ... & ... & ... & ...\\\\\n  ... & ... & ... & ...\\\\\n  1 & x_{N1} & ... & x_{Nk}\\\\\n      \\end{array}\\right)\\] \\[{\\textbf B}=\\left(\\begin{array}{c}\n  b_0\\\\\n  b_1\\\\\n  ...\\\\\n  ...\\\\\n  ...\\\\\n  b_k\n      \\end{array}\\right)\\,\\,\\,{\\textbf E}=\\left(\\begin{array}{c}\n  \\epsilon_1\\\\\n  \\epsilon_2\\\\\n  ...\\\\\n  ...\\\\\n  ...\\\\\n  \\epsilon_N\n      \\end{array}\\right)\\,.\\] \\ \\ De esta forma, podemos escribir nuestro modelo en notación matricial como \\[{\\textbf Y}={\\textbf X}{\\textbf B} + {\\textbf E}\\,.\\] \\ \\ Si restringimos el modelo a una variable independiente (\\(k=1\\)), i.e. dos coeficientes \\((b_0, b_1)\\), entonces \\[{\\textbf B}=\\left(\\begin{array}{c}\n  b_0\\\\\n  b_1\\\\\n      \\end{array}\\right)\\,,\\] es la matriz de coeficientes, e \\[{\\textbf X}=\\left(\\begin{array}{cc}\n  1& x_{11}\\\\\n  1& x_{21}\\\\\n  ... & ...\\\\\n  ... & ...\\\\\n   ... & ...\\\\\n1 & x_{N1}\n\\end{array}\\right)\\,,\\] es la matriz de variables independientes. Si utilizamos la definición de los residuos como \\[{\\textbf E}={\\textbf Y}-{\\textbf X}{\\textbf B}\\,,\\] podemos encontrar la suma de los residuos al cuadrado como \\[SEC=\\sum^N_{i=1} \\epsilon^2_i=\\sum^N_{i=1} \\epsilon_i \\epsilon_i=\n{\\textbf E}^T{\\textbf E}=({\\textbf Y}-{\\textbf X}{\\textbf B})^T({\\textbf Y}-{\\textbf X}{\\textbf B})=\\] \\[{\\textbf Y}^T{\\textbf Y} - {\\textbf Y}^T{\\textbf X}{\\textbf B}-{\\textbf B}^T{\\textbf X}^T{\\textbf Y}+{\\textbf B}^T\n{\\textbf X}^T{\\textbf X}{\\textbf B}\\,.\\]\nSi queremos minimizar esa suma de errores entonces \\[\\frac{\\partial{SEC}}{\\partial{\\textbf B}}=0\\,.\\] Para calcular las derivadas de \\({SEC}\\) respecto de los coeficientes \\({\\textbf b}\\) vamos a asumir las siguientes consideraraciones \\ \\ (i) \\({\\textbf Y}^T{\\textbf X}{\\textbf B}={\\textbf B}^T{\\textbf X}^T{\\textbf Y}\\) ya que son matrices elemento (\\(1\\times1\\)) y siempre son simétricas. Por lo tanto \\(-{\\textbf Y}^T{\\textbf X}{\\textbf B}-{\\textbf B}^T{\\textbf X}^T{\\textbf Y}=-2{\\textbf B}^T{\\textbf X}^T{\\textbf Y}\\), y su derivada \\[-2{\\textbf X}^T{\\textbf Y}\\,.\\] \\ \\ (ii) \\(\\partial{{\\textbf B}^T{\\textbf X}^T{\\textbf X}{\\textbf B}}/\\partial{\\textbf B}=2{\\textbf X}^T{\\textbf X}{\\textbf B}\\). Para demostrar esto tomemos el caso para el ajuste lineal (2 parámetros). Definamos los elementos de \\({\\textbf X}^T{\\textbf X}\\) como \\(c_{ij}\\,\\,i,j=1,2\\) y \\(c_{12}=c_{21}\\) por ser simétrica. Entonces \\[{\\textbf B}^T{\\textbf X}^T{\\textbf X}{\\textbf B}=c_{11}b_0^2+c_{22}b_1^2 + 2c_{12} b_0 b_1\\,,\\] y su derivada respecto \\(b_0\\) es \\[2c_{11}b_0 + 2c_{12}b_1\\] y respecto a \\(b_1\\) es \\[2c_{12}b_0 + 2c_{22}b_1\\,.\\] Si acomodamos las derivadas en un vector columna \\(2\\times1\\) obtenemos \\[\\left(\\begin{array}{c}\n  2c_{11}b_0 + 2c_{12}b_1 \\\\\n  2c_{12}b_0 + 2c_{22}b_1 \\\\\n        \\end{array}\\right)=2{\\textbf X}^T{\\textbf X}{\\textbf B}\\,.\\] y por lo tanto \\[\\frac{\\partial{S}}{\\partial{\\textbf B}}=-2{\\textbf X}^T{\\textbf Y}+2{\\textbf X}^T{\\textbf X}{\\textbf\nB}\\,,\\] y las ecuaciones normales quedan \\[-2{\\textbf X}^T{\\textbf Y}+2{\\textbf X}^T{\\textbf X}{\\textbf\nB}=0\\,.\\]\nFinalmente, el problema de mínimos cuadrados es \\[({\\textbf X}^T{\\textbf X}){\\textbf B}={\\textbf X}^T{\\textbf Y}\\,.\\] Y resolviendo para \\({\\textbf B}\\) la forma general del método de regresión por mínimos cuadrados es \\[{\\textbf B}=({\\textbf X}^T{\\textbf X})^{-1}{\\textbf X}^T{\\textbf Y}\\,,\\] o equivalentemente \\[{\\textbf B}^T={\\textbf Y}^T{\\textbf X}({\\textbf X}^T{\\textbf X})^{-1}\\,.\\]\n \\ Algunas de las consideraciones de un modelo de regresión múltiple son:\n \\ La esperanza del vector de coeficientes \\(k\\times 1\\), \\(b\\), se obtiene a partir de las consideraciones 1,4, y 5. La consideración 5 implica que el estimador de los coeficientes \\(b=({\\textbf X}^T{\\textbf X})^{-1}{\\textbf X}^T { y}\\) se puede escribir cómo\n\\[b=({\\textbf X}^T{\\textbf X})^{-1}{\\textbf X}^T({\\textbf X} \\beta + \\epsilon)={\\textbf I}\\beta +\n    ({\\textbf X}^T{\\textbf X})^{-1}{\\textbf X}^T \\epsilon\\,.\\] \\\nDe las consideraciones 1, y 4 obtenemos \\[E[b]=E[\\beta + ({\\textbf X}^T{\\textbf X})^{-1}{\\textbf X}^T\\epsilon]=\n  \\beta + ({\\textbf X}^T{\\textbf X})^{-1}{\\textbf X}^T \\underbrace{E[\\epsilon]}_{0}=\\beta\\,,\\] lo que implica que \\(b\\) es insesgado.\n \\ Usando el resultado de arriba, bajo las consideraciones 1-6, la matriz de varianza de los estimadores de los coeficientes \\(b\\) (o momento centrado de orden 2) viene dada por \\[{ var(b)}=E[(b-\\beta)(b-\\beta)^T]=E[({\\textbf X}^T{\\textbf X})^{-1}{\\textbf X}^T\\epsilon \\epsilon^T{\\textbf X}\n    ({\\textbf X}^T{\\textbf X})^{-1}]=\\] \\[=({\\textbf X}^T{\\textbf X})^{-1}{\\textbf X}^T E[\\epsilon \\epsilon^T]{\\textbf X}({\\textbf X}^T{\\textbf X})^{-1}=\n    ({\\textbf X}^T{\\textbf X})^{-1}{\\textbf X}^T(\\sigma^2{\\textbf I}){\\textbf X}({\\textbf X}^T{\\textbf X})^{-1}=\n    \\sigma^2({\\textbf X}^T{\\textbf X})^{-1}\\,.\\] \\ \\ Los elementos de la diagonal principal de esta matriz son las varianzas asociadas a los estimadores de los coeficientes \\(b\\), y los elementos fuera de la diagonal principal representan la covarianza entre esos estimadores. \\ Si asumimos que las variables del problema han sido estandarizadas, es decir la media ha sido extraida de las variables \\(x_{ij}\\) e \\(y_i\\) y hemos dividido por las desviaciones estándar \\[{\\textbf X}^T{\\textbf X}=\\left(\\begin{array}{cccc}\n   \\sum\\limits_{i=1}^N{x_{i1}x_{i1}} & \\sum\\limits_{i=1}^N{x_{i1}x_{i2}} & ... & \\sum\\limits_{i=1}^N{x_{i1}x_{ik}}\\\\\n   \\sum\\limits_{i=1}^N{x_{i2}x_{i1}} & \\sum\\limits_{i=1}^N{x_{i2}x_{i2}} & ... & \\sum\\limits_{i=1}^N{x_{i2}x_{ik}}\\\\\n                . & . & . \\\\\n        . & . & . \\\\\n        . & . & . \\\\\n   \\sum\\limits_{i=1}^N{x_{ik}x_{i1}} & \\sum\\limits_{i=1}^N{x_{ik}x_{i2}} & ... & \\sum\\limits_{i=1}^N{x_{ik}x_{ik}}\\\\\n        \\end{array}\\right)\\,,\\] es una matriz \\(k\\times k\\) y en notación índice se puede escribir como \\[[{\\textbf X}^T{\\textbf X}]_{nm}=(N-1)\\rho_{x_{in} x_{im}}\\,\\,;(i=1,2,...,N)\\,,\\] que se puede interpretar como la matriz de correlaciones entre las variables independientes. La matriz \\[{\\textbf X}^T{\\textbf Y}=\\left(\\begin{array}{c}\n  \\sum\\limits_{i=1}^N y_i x_{i1}\\\\\n  \\sum\\limits_{i=1}^N y_i x_{i2}\\\\\n                   . \\\\\n           . \\\\\n           . \\\\\n  \\sum\\limits_{i=1}^N y_i x_{ik}\\\\\n        \\end{array}\\right)\\,,\\] es una matriz \\(k\\times 1\\) y en notación índice es \\[[{\\textbf Y}^T{\\textbf X}]_n=(N-1)\\rho_{y_i x_{in}}\\,\\,;(i=1,2,...,N)\\,.\\] Esta matriz se puede interpretar como la matriz de correlación entre las variables independientes y dependientes. \\ El modelo multivariado en notación índice es \\[(N-1)\\rho_{x_{in} x_{im}}b_m=(N-1)\\rho_{y_i x_{in}}\\] o \\[\\rho_{x_{in} x_{im}}b_m=\\rho_{y_i x_{in}}\\,;m=n=1,2,....,k; i=1,2,...,N\\] y para una única observación \\(i\\) dada obtenemos \\[\\rho_{x_{n} x_{m}}b_m=\\rho_{y x_{n}}\\,\\] Ahora supongamos, por simplicidad, que solo tenemos dos variables independientes. Entonces: \\[\\rho_{x_{1} x_{1}}b_1+\\rho_{x_{1} x_{2}}b_2=\\rho_{y x_{1}}\\] \\[\\rho_{x_{2} x_{1}}b_1+\\rho_{x_{2} x_{2}}b_2=\\rho_{y x_{2}}\\,,\\] y puesto que \\(\\rho_{x_{1} x_{1}}=\\rho_{x_{2} x_{1}}=1\\), y \\(\\rho_{x_{1} x_{2}}=\\rho_{x_{2} x_{1}}\\), podemos escribir el sistema como: \\[\\left(\\begin{array}{cc}\n  1 & \\rho_{x_{1} x_{2}} \\\\\n  \\rho_{x_{1} x_{2}} & 1 \\\\\n        \\end{array}\\right)\n  \\left(\\begin{array}{c}\n  b_1 \\\\\n  b_2 \\\\\n        \\end{array}\\right)=\n     \\left(\\begin{array}{c}\n  \\rho_{y x_{1}} \\\\\n  \\rho_{y x_{2}} \\\\\n        \\end{array}\\right)\\,.\\] De forma que los coeficientes quedan como \\[\\left(\\begin{array}{c}\n  b_1 \\\\\n  b_2 \\\\\n        \\end{array}\\right)=\\left(\\begin{array}{cc}\n  1 & \\rho_{x_{1} x_{2}} \\\\\n  \\rho_{x_{1} x_{2}} & 1 \\\\\n        \\end{array}\\right)^{-1}\n     \\left(\\begin{array}{c}\n  \\rho_{y x_{1}} \\\\\n  \\rho_{y x_{2}} \\\\\n        \\end{array}\\right)=\\frac{1}{1-\\rho^2_{x_{1} x_{2}}}\n    \\left(\\begin{array}{cc}\n  1 & -\\rho_{x_{1} x_{2}} \\\\\n  -\\rho_{x_{1} x_{2}} & 1 \\\\\n        \\end{array}\\right)\\left(\\begin{array}{c}\n  \\rho_{y x_{1}} \\\\\n  \\rho_{y x_{2}} \\\\\n        \\end{array}\\right)\\,.\\] \\[b_1=\\frac{1}{1-\\rho^2_{x_{1} x_{2}}}({\\rho_{y x_{1}} - \\rho_{x_{1} x_{2}} \\rho_{y x_{2}}})\\] \\[b_2=\\frac{1}{1-\\rho^2_{x_{1} x_{2}}}({\\rho_{y x_{2}} - \\rho_{x_{1} x_{2}} \\rho_{y x_{1}}})\\,.\\]\nFinalmente puntualizar que el problema de mínimos cuadrados se puede resolver utilizando la descomposición \\(LU\\). Para ello solo es necesario un cambio de variable en la ecuación \\[{\\textbf X}^T{\\textbf X}{\\textbf B}={\\textbf X}^T{\\textbf Y}\\] para obtener un sistema de ecuaciones tipo \\({\\textbf A}{\\textbf x}={\\textbf b}\\), donde ahora \\({\\textbf A}={\\textbf X}^T{\\textbf X}\\), \\({\\textbf x}={\\textbf B}\\), y \\({\\textbf b}={\\textbf X}^T{\\textbf Y}\\).\n{2) Mínimos cuadrados con restricciones}\n{NOTA: Multiplicadores de Lagrange} \\ Dada la función \\(f(x)=f(x_1,x_2,...,x_N)\\) que depende de \\(N\\) variables y \\(p\\) restricciones \\(g_1(x)=d_1, g_2(x)=d_2,....,g_p(x)=d_p\\) entonces el teorema de Lagrange nos dice que para minimizar la función \\(f(x)\\) bajo esas \\(p\\) restricciones debemos resolver el sistema de ecuaciones \\[\\frac{\\partial}{\\partial{x_i}}\\left[ f(x) +\\sum\\limits_{j=1}^p \\lambda_j g_j(x)\\right]=0\\,\\,\\,; i=1,2,...,N\\] \\[g_j(x)=d_j\\,\\,\\,; j=1,2,...,p\\]\nVamos a usar los multiplicadores de Lagrange para resolver el problema de mínimos cuadrados \\[{\\textbf Y}={\\textbf X}{\\textbf B}\\,,\\] pero incluyendo \\(p\\) restricciones de la forma \\[{\\textbf G}{\\textbf B}={\\textbf d}\\,.\\]\nQueremos minimizar la función: \\[{\\textbf \\cal L}=({\\textbf Y}-{\\textbf X}{\\textbf B})^T({\\textbf Y}-{\\textbf X}{\\textbf B}) +{\\textbf \\lambda}^T({\\textbf G}{\\textbf B}-{\\textbf d})\\,.\\] Aqui hemos introducido \\(p\\) incógnitas pero también tenemos \\(p\\) nuevas ecuaciones \\({\\textbf G}{\\textbf B}={\\textbf d}\\). Derivando \\({\\textbf \\cal L}\\) e igualando a cero \\[\\frac{\\partial{\\textbf \\cal L}}{\\partial{\\textbf B}}=-2{\\textbf X}^T{\\textbf Y}+2{\\textbf X}^T{\\textbf X}{\\textbf B} + {\\textbf G}^T{\\textbf \\lambda}=0\\,,\\] lo cual tiene la solución \\[{\\textbf B}=( {\\textbf X}^T {\\textbf X} )^{-1} ({\\textbf X}^T {\\textbf Y} -\\frac{1}{2}{\\textbf G}^T{\\textbf \\lambda})\\,,\\] que para \\({\\textbf \\lambda}=0\\) se reduce a la expresión de los mínimos cuadrados sin restricciones \\[{\\textbf B}=( {\\textbf X}^T {\\textbf X} )^{-1} ({\\textbf X}^T {\\textbf Y})\\,.\\]\nSi sutituimos en la ecuación de las restricciones \\[{\\textbf G}( {\\textbf X}^T {\\textbf X} )^{-1} ({\\textbf X}^T {\\textbf Y} -\\frac{1}{2}{\\textbf G}^T{\\textbf \\lambda})={\\textbf d}\\,,\\] y resolvemos para \\({\\textbf \\lambda}\\), obtenemos \\[\\frac{1}{2}{\\textbf \\lambda}=\\left[ {\\textbf G}({\\textbf X}^T {\\textbf X})^{-1}{\\textbf G}^T\\right]^{-1} \\left[ {\\textbf G}({\\textbf X}^T {\\textbf X})^{-1}{\\textbf X}^T{\\textbf Y}-{\\textbf d}\\right]\\,.\\]\nFinalmente, si sustituimos en la solución para la matriz de coeficientes \\({\\textbf B}\\) obtenemos \\[{\\textbf B}=({\\textbf X}^T {\\textbf X})^{-1}\\left( {\\textbf X}^T {\\textbf Y} - {\\textbf G}^T\\left[{\\textbf G}({\\textbf X}^T{\\textbf X})^{-1}{\\textbf G}^T \\right]^{-1}\n           \\left[{\\textbf G}({\\textbf X}^T{\\textbf X})^{-1}{\\textbf X}^T {\\textbf Y}-{\\textbf d}\\right] \\right)\\,.\\]\nUn ejemplo sería ajustar una recta a un conjunto de observaciones pero imponiendo que la recta pase por algún punto determinado del espacio \\(xy\\).\n{3) Mínimos cuadrados pesados} \\ En ocasiones debido a las incertidumbres asociadas a la medidas de las variables independientes es conveniente asignarles un peso diferencial en el problema de mínimos cuadrados. Supongamos que unas variables independientes son conocidas con mayor precisión que otras. Entonces el modelo de mínimos cuadrados pesados es: \\[SEC=({\\textbf Y}-{\\textbf X}{\\textbf B})^T{\\textbf W}_{\\epsilon}({\\textbf Y}-{\\textbf X}{\\textbf B})\\,,\\] donde \\({\\textbf W}_{\\epsilon}\\) es una matriz diagonal con los elementos \\(\\sigma_j^{-2}\\), el inverso de la varianza de cada variable independiente. \\ Sin embargo, en general, los errores en los datos estan correlacionados, y \\({\\textbf W}_{\\epsilon}\\) no es diagonal. Una elección razonable para \\({\\textbf W}_{\\epsilon}\\) es el inverso de la matriz de covarianzas. Si asumimos que \\({\\textbf Y}\\) se compone de un valor medio mas un error o fluctuación respecto la media \\[{\\textbf Y}=\\bar{\\textbf Y}+{\\textbf Y}'\\,,\\] entonces la matriz de covarianzas es \\(&lt;{\\textbf Y}'{\\textbf Y}'^T&gt;\\) y \\[{\\textbf W}_{\\epsilon}=&lt;{\\textbf Y}'{\\textbf Y}'^T&gt;^{-1}\\,.\\]\n{jemplo (1):} Emery and Thompson (sección 3.12.4). \\ En la tabla adjunta se muestran 5 observaciones de la variable independiente (\\(x_i\\)) y dependiente (\\(y_i\\)). Se pide ajustar una recta al conjunto de datos y calcular las medidas de error (varianza \\(s^2\\) y coeficiente de correlación al cuadrado \\(r^2\\)) asociadas al ajuste lineal.  % %insert table\nLos coeficientes del modelo lineal se pueden calcular con las expresiones: \\[\\hat{b}_1=\\frac{\\left[N \\sum\\limits^N_{i=1} x_i y_i - \\sum\\limits^N_{i=1}x_i \\sum\\limits^N_{i=1}y_i\\right]}\n           {N\\sum\\limits^N_{i=1}x^2_i-\\left(\\sum\\limits^N_{i=1}x_i \\right)^2}=\\] \\[\\frac{[(5)(7)-(0)(5)]}{[(5)(10)-10^2]}=0.7\\]\n\\[b_0=\\bar{y}-b_1\\bar{x}=5/5-(0.7)(0)=1\\]\nEn notación matricial obtenemos el mismo resultado:\n\\[{\\textbf Y}=\\left(\\begin{array}{c}\n  0 \\\\ 0 \\\\ 1 \\\\ 1 \\\\ 3\\\\\n        \\end{array}\\right)\\]\n\\[{\\textbf X}=\\left(\\begin{array}{cc}\n  1 & -2 \\\\\n  1 & -1 \\\\\n  1 & 0 \\\\\n  1 & 1 \\\\\n  1 & 2\\\\\n        \\end{array}\\right)\\]\n\\[{\\textbf X}^T{\\textbf X}= \\left(\\begin{array}{ccccc}\n   5 &  0 \\\\\n  0 & 10 \\\\\n        \\end{array}\\right)\\]\n\\[{\\textbf X}^T{\\textbf Y}= \\left(\\begin{array}{ccccc}\n   5  \\\\\n  7  \\\\\n        \\end{array}\\right)\\]\n\\[({\\textbf X}^T{\\textbf X})^{-1}= \\left(\\begin{array}{ccccc}\n   1/5 &  0 \\\\\n   0 & 1/10 \\\\\n        \\end{array}\\right)\\]\n\\[ ({\\textbf X}^T{\\textbf X})^{-1}{\\textbf X}^T{\\textbf Y}=\n    \\left(\\begin{array}{ccccc}\n   1/5 &  0 \\\\\n   0 & 1/10 \\\\\n        \\end{array}\\right)\\left(\\begin{array}{ccccc}\n   5  \\\\\n  7  \\\\\n        \\end{array}\\right)=\n    \\left(\\begin{array}{ccccc}\n   1  \\\\\n  0.7  \\\\\n        \\end{array}\\right)\\,.\\]\n{} Como ya vimos, para calcular la varianza de nuestro ajuste usamos\n\\[s^2=\\frac{1}{N-2}\\sum\\limits^N_{i=1}(y_i-\\hat{y_i})^2=\\frac{1}{N-2}SEC\\,,\\]\ndonde \\(SEC\\) es la suma de los errores cuadrados y \\(N-2\\) resulta debido a que en la regresión lineal se requiere la estimación de dos parámetros. En notación matricial \\[SEC={\\textbf Y}^T{\\textbf Y} - {\\textbf Y}^T{\\textbf X}{\\textbf B}-{\\textbf B}^T{\\textbf X}^T{\\textbf Y}+{\\textbf B}^T\n{\\textbf X}^T{\\textbf X}{\\textbf B}=\n{\\textbf Y}^T{\\textbf Y}-2{\\textbf B}^T{\\textbf X}^T{\\textbf Y}+{\\textbf B}^T{\\textbf X}^T{\\textbf X}{\\textbf B}=\\]\n\\[={\\textbf Y}^T{\\textbf Y}-2{\\textbf B}^T{\\textbf X}^T{\\textbf Y}+{\\textbf B}^T{\\textbf X}^T{\\textbf Y}=\n{\\textbf Y}^T{\\textbf Y}-{\\textbf B}^T{\\textbf X}^T{\\textbf Y}\\,,\\]\ndonde hemos usado la identidad \\({\\textbf X}^T{\\textbf X}{\\textbf B}={\\textbf X}^T{\\textbf Y}\\).\nSi sustituimos las matrices de nuestro ejemplo, obtenemos\n\\[SEC=\\left(\\begin{array}{ccccc}\n  0 & 0 & 1 & 1 & 3\\\\\n        \\end{array}\\right)\n      \\left(\\begin{array}{c}\n  0 \\\\ 0 \\\\ 1 \\\\ 1 \\\\ 3\\\\\n        \\end{array}\\right)-\n    \\left(\\begin{array}{ccccc}\n  1 & 0.7\\\\\n        \\end{array}\\right)\n    \\left(\\begin{array}{ccccc}\n   1 &  1 & 1 & 1 & 1\\\\\n  -2 & -1 & 0 & 1 & 2 \\\\\n        \\end{array}\\right)\n    \\left(\\begin{array}{c}\n  0 \\\\ 0 \\\\ 1 \\\\ 1 \\\\ 3\n        \\end{array}\\right)=\\] \\[=11-\\left(\\begin{array}{ccccc}\n  1 & 0.7\\\\\n        \\end{array}\\right)\n    \\left(\\begin{array}{c}\n        5 \\\\ 7\\\\\n        \\end{array}\\right)=11-9.9=1.1\\]\n{} \\(SEC\\) puede ser calculado directamente con la expresión: \\[SEC=\\sum \\limits^N_{i=1}(\\hat{y_i}-y_i)^2=\n   (-0.4)^2 + (-0.3)^2+(0)^2 + (0.7)^2 + (0.6)^2=1.1\\]\nY entonces la desviación estándar de nuestro ajuste lineal es \\[s=\\sqrt{\\frac{1}{N-2}SEC}=\\sqrt{\\frac{1}{5-2}1.1}=\\sqrt{1.1/3}\\simeq0.366\\,.\\]\n{} El coeficiente de correlación se puede escribir como\n\\[r^2=\\frac{SCR}{SCT}=\\frac{\\sum\\limits^N_{i=1}(\\hat{y_i}-\\bar{y})^2}{\\sum\\limits^N_{i=1}\n({y_i}-\\bar{y})^2}=\\frac{4.9}{6}\\simeq0.8167\\]\n\nAjuste de curvas con mínimos cuadrados  \n\n{} En general, podemos escribir nuestro modelo lineal como \\[Y=b_0 + b_1x + b_2 x^2 + ... + b_k x^k + \\epsilon\\,.\\] \\ \\ El procedimiento es el mismo que para el caso de la línea recta, pero ahora la matriz \\({\\textbf X}\\) tiene una columna mas. Es decir, para \\(k=2\\) y para \\(N\\) observaciones independientes las ecuaciones independientes son \\[y_1=b_0 + b_1x_1 + b_2 x_1^2 + \\epsilon_1\\] \\[y_2=b_0 + b_1x_2 + b_2 x_2^2 + \\epsilon_2\\] \\[...\\] \\[...\\] \\[...\\] \\[y_N=b_0 + b_1x_N + b_2 x_N^2 + \\epsilon_N\\,\\] y puden resolverse matricialmente para \\({\\textbf B}\\) como \\[{\\textbf B}=({\\textbf X}^T{\\textbf X})^{-1}{\\textbf X}^T{\\textbf Y}\\,,\\] donde \\({\\textbf X}\\) tiene una columna mas que el caso del ajuste de una recta, i.e. \\(k+1\\) columnas.\n{} Ejemplo (2): Ajustes por cuadrados mínimos con restricciones.\n\n{} Supongamos que queremos ajustar dos polinomios \\(f(x)\\) y \\(g(x)\\) de orden d-1 a dos conjunto de datos contínuos de \\(M\\) y \\(N\\) observaciones, respectivamente, \\(x_1, x_2, ....,x_M\\leq a\\) y \\(x_{M+1}, x_{M+2}, ....,x_N&gt;a\\), tal que, queremos minimizar:\n\\[\\sum\\limits^M_{i=1}(f(x_i)-y_i)^2 + \\sum\\limits^N_{i=M+1}(g(x_i)-y_i)^2\\,,\\]\nsujeto a las restricciones:\n\\[f(a)=g(a)\\,\\,y\\,\\,f'(a)=g'(a)\\,.\\]\n\nPrimero debemos de construir las matrices del problema lineal de cuadrados mínimos:\n\\[ {\\textbf X}=\\left(\\begin{array}{cccccccc}\n  1 & x_1   & .  .  . & x_1^{d-1}  & 0 & 0 & .  .  . & 0 \\\\\n  . & .         &  & .                  & . & . &  & . \\\\\n  . & .         &  & .                  & . & . &  & . \\\\\n  . & .         &  & .                  & . & . &  & . \\\\\n  1 & x_M & .  .  . & x_M^{d-1} & 0 & 0 & .  .  . & 0 \\\\\n  0 & 0       & .  .  . & 0                 & 1 & x_{M+1} & .  .  . & x^{d-1}_{M+1} \\\\\n  . & .         && .                  & . & . &  & . \\\\\n  . & .         & & .                  & . & . &  & . \\\\\n  . & .         &  & .                  & . & . &  & . \\\\\n  0 & 0       & .  .  . & 0                & 1  & x_N & .  .  . & x_N^{d-1} \\\\\n  \\end{array}\\right)\n  \\]\n\\[{\\textbf Y}=\\left(\\begin{array}{c}\n    y_1 \\\\ y_2 \\\\ . . . \\\\ y_M \\\\ y_{M+1}\\\\ y_{M+2}\\\\...\\\\y_N\n      \\end{array}\\right)\\,,\\]\n\\[{\\textbf G}=\\left(\\begin{array}{cccccccc}\n        1 & a & .  .  .&a^{d-1}&-1&-a& .  .  .&-a^{d-1}\\\\\n        0 & 1 & .  .  .&(d-1)a^{d-2}&0&-1& .  .  .&-(d-1)a^{d-2}\\\\\n          \\end{array}\\right)\\,,\n          {\\textbf d}=\\left(\\begin{array}{c}\n            0\\\\\n            0\\\\\n              \\end{array}\\right)\\,.\\]\nSegundo debemos de calcular los coeficientes del ajuste \\({\\textbf B}\\) con la expresión para mínimos cuadrados con restricciones.\n\\[{\\textbf B}=({\\textbf X}^T {\\textbf X})^{-1}\\left( {\\textbf X}^T {\\textbf Y} - {\\textbf G}^T\\left[{\\textbf G}({\\textbf X}^T{\\textbf X})^{-1}{\\textbf G}^T \\right]^{-1}\n           \\left[{\\textbf G}({\\textbf X}^T{\\textbf X})^{-1}{\\textbf X}^T {\\textbf Y}-{\\textbf d}\\right] \\right)\\,.\\]\nApliquemos el caso particular del ajuste de dos rectas\n\\(f(x)=b_1 + b_2 x\\) y \\(g(x)=b_3 + b_4 x\\):\n\ndonde \\(h\\) es el valor donde ambas rectas interseccionan. Por ello exigimos que ambas rectas tomen el mismo valor y sus derivadas sean iguales en \\(x=h\\). Se resuleve el sistema de ecuaciones:\n\\[{\\textbf X}\\,{\\textbf B}={\\textbf Y}\\] \\[({\\it 1})b_1+({\\it x_1})b_2 + ({\\it 0})b_3 + ({\\it 0})b_4=y_1\\] \\[({\\it 1})b_1+({\\it x_2})b_2 + ({\\it 0})b_3 + ({\\it 0})b_4=y_2\\] \\[({\\it 1})b_1+({\\it x_3})b_2 + ({\\it 0})b_3 + ({\\it 0})b_4=y_3\\] \\[({\\it 1})b_1+({\\it x_4})b_2 + ({\\it 0})b_3 + ({\\it 0})b_4=y_4\\] \\[({\\it 0})b_1+({\\it 0})b_2 + ({\\it 1})b_3 + ({\\it x_5})b_4=y_5\\] \\[({\\it 0})b_1+({\\it 0})b_2 + ({\\it 1})b_3 + ({\\it x_6})b_4=y_6\\] \\[({\\it 0})b_1+({\\it 0})b_2 + ({\\it 1})b_3 + ({\\it x_7})b_4=y_7\\] \\[({\\it 0})b_1+({\\it 0})b_2 + ({\\it 1})b_3 + ({\\it x_8})b_4=y_8\\,,\\]  \nbajo el sistema de ecuaciones de restricciones:\n\\[{\\textbf G}\\,{\\textbf B}={\\textbf d}\\] \\[({\\it 1})b_1+({\\it h})b_2 + (-{\\it 1})b_3 + (-{\\it 1})b_4=0\\] \\[({\\it 1})b_1+({\\it 0})b_2 + (-{\\it 1})b_3 + ({\\it 0})b_4=0\\]\n\nEn notación matricial:\n\n\\[{\\textbf X}=\\left(\\begin{array}{cccccccc}\n  1 & x_1  & 0 & 0 \\\\\n  1 & x_2   & 0 & 0 \\\\\n  1 & x_3   & 0 & 0\\\\\n  1 & x_4   & 0 & 0 \\\\\n  0 & 0  & 1 & x_5 \\\\\n  0 & 0   & 1 & x_6 \\\\\n  0 & 0   & 1 & x_7\\\\\n  0 & 0   & 1 & x_8 \\\\\n  \\end{array}\\right)\\,,\n{\\textbf Y}=\\left(\\begin{array}{c}\n    y_1 \\\\ y_2 \\\\ y_3 \\\\ y_4 \\\\ y_5\\\\ y_6\\\\y_7\\\\y_8\n      \\end{array}\\right)\\,,\n{\\textbf G}=\\left(\\begin{array}{cccccccc}\n        1 & h & -1&-h\\\\\n        0 & 1& 0&-1\\\\\n          \\end{array}\\right)\\,,\n{\\textbf d}=\\left(\\begin{array}{c}\n            0\\\\\n            0\\\\\n              \\end{array}\\right)\\,.\n  \\]\n\n\n\nCuadrados Mínimos no-lineales\n\nLos mínimos cuadrados no-lineales se aplican cuando queremos ajustar un conjunto de observaciones a un modelo que no es lineal en cuanto a los coeficientes.\n\nLinealización del problema no lineal En ocasiones se puede linealizar el problema y resolverlo utilizando la solución de mínimos cuadrados lineales. Por ejemplo, supongamos que queremos ajustar un conjunto de \\(N\\) observaciones a una función no-lineal exponencial: \\[\\hat{y}=a e^{bx}\\,.\\]\n\nEste ejemplo se puede linealizar: \\[\\hat{y}=a e^{bx}\\rightarrow ln(\\hat{y})=ln(a) + bx\\,,\\] y resolver el problema lineal: \\[{\\cal Y}={\\cal B}_1 + {\\cal B}_2 {\\cal X}\\,,\\] donde hemos hecho el cambio de variables: \\[{\\cal Y}=ln(\\hat{y})\\,,{\\cal B}_1=ln(a)\\,,{\\cal B}_2=b\\,\\,\\,{ y}\\,{\\cal X}=x\\,.\\]\nResolvemos para \\({\\cal B}_1\\) y \\({\\cal B}_2\\):\n\\[{\\cal B}=\\left(\\begin{array}{c}\n            {\\cal B}_1\\\\\n            {\\cal B}_2\\\\\n              \\end{array}\\right)=({\\cal X}^T {\\cal X})^{-1}\\,{\\cal X}^T{\\cal Y}\n\\,,\\] y calculamos los coeficientes originales \\(a\\) y \\(b\\) con el cambio de variables.\n\nResolución numérica del problema no lineal\nCuando no es posible linealizar la función no-lineal que queremos ajustar, debemos minimizar la suma de los errores cuadráticos medios y resolver numéricamente. La \\(SEC\\) del modelo exponencial\n\n\\[SEC=\\sum\\limits^N_{i=1}(y_i-\\hat{y_i})^2=\\sum\\limits^N_{i=1}\n              {\\underbrace{\\left(y_i - a e^{bx_i}\\right)}_{\\epsilon_i}}^2\\,\\]\nes minimizada derivando con respecto los coeficientes:\n\\[\\frac{\\partial{SEC}}{\\partial{a}}=\n      2\\sum\\limits^N_{i=1}\\left(y_i - a e^{bx_i}\\right)\\frac{\\partial{\\epsilon_i}}{\\partial{a}}=0\\]\n\\[\\frac{\\partial{SEC}}{\\partial{b}}=\n2\\sum\\limits^N_{i=1}\\left(y_i - a e^{bx_i}\\right)\\frac{\\partial{\\epsilon_i}}{\\partial{b}}=0\\,.\\]\nDebemos resolver el sistema de ecuaciones de arriba, lo cual se puede hacer con descomposiciones algebráicas tipo \\({\\textbf L}{\\textbf U}\\), minimizando \\(SEC\\) con la función de Matlab fminsearch.m (o implementando un método numérico manual), o directamente utilizando la función de mínimos cuadrados no-lineales de Matlab nlinfit.m.\nOtro ejemplo no-lineal es ajustar la siguiente función trigonométrica: \\[\\hat{y}=\\phi_1 e^{\\phi_2 x} cos(\\phi_3 x + \\phi_4)\\,,\\] donde la suma de los errores cuadráticos es:\n\\[SEC=\\sum\\limits^N_{i=1}(y_i-\\hat{y_i})^2=\\sum\\limits^N_{i=1}\n              {\\underbrace{\\left(y_i - \\phi_1 e^{\\phi_2 x_i} cos(\\phi_3 x_i + \\phi_4)\\right)}_{\\epsilon_i}}^2\\,,\\]\ny sus derivadas respecto los coeficientes \\(\\phi_1\\), \\(\\phi_2\\), \\(\\phi_3\\), y \\(\\phi_4\\) son:\n\\[\\frac{\\partial{SEC}}{\\partial{\\phi_1}}=\n     2\\sum\\limits^N_{i=1}\\left(y_i - \\phi_1 e^{\\phi_2 x_i} cos(\\phi_3 x_i + \\phi_4)\\right)\n     \\frac{\\partial{\\epsilon_i}}{\\partial{\\phi_1}}=0\\]\n\\[\\frac{\\partial{SEC}}{\\partial{\\phi_2}}=2\\sum\\limits^N_{i=1}\\left(y_i - \\phi_1 e^{\\phi_2 x_i} cos(\\phi_3 x_i + \\phi_4)\\right)\n\\frac{\\partial{\\epsilon_i}}{\\partial{\\phi_2}}=0\\]\n\\[\\frac{\\partial{SEC}}{\\partial{\\phi_3}}=\n    2\\sum\\limits^N_{i=1}\\left(y_i - \\phi_1 e^{\\phi_2 x_i} cos(\\phi_3 x_i + \\phi_4)\\right)\n    \\frac{\\partial{\\epsilon_i}}{\\partial{\\phi_3}}=0\\]\n\\[\\frac{\\partial{SEC}}{\\partial{\\phi_4}}=\n    2\\sum\\limits^N_{i=1}\\left(y_i - \\phi_1 e^{\\phi_2 x_i} cos(\\phi_3 x_i + \\phi_4)\\right)\n    \\frac{\\partial{\\epsilon_i}}{\\partial{\\phi_4}}=0\\]\nRelación entre regresión y correlación\n\nEl coeficiente de correlación, \\(r\\), nos informa de que tanto dos (o mas) variables covarian en el espacio-tiempo. Para dos variables aleatorias \\(x\\) e \\(y\\), el coeficiente de correlación es\n\\[r=\\rho_{xy}=\\frac{\\frac{1}{N-1}\\sum\\limits^N_{i=1}(x_i-\\bar{x})(y_i-\\bar{y})}\n     {\\sqrt{\\frac{1}{N-1}\\sum\\limits^N_{i=1}(x_i-\\bar{x})}\n     \\sqrt{\\frac{1}{N-1}\\sum\\limits^N_{i=1}(y_i-\\bar{y})}}=\\frac{C_{xy}}{s_x s_y}\\,,\\]\ndonde \\(C_{xy}\\) es la covarianza de \\(x\\) e \\(y\\), y \\(s_x\\) y \\(s_y\\) son las correspondientesdesviaciones estándar.\n\nPropiedades del coeficiente de correlación\n\n\n\\(r\\) es adimensional.\n\nla magnitud de \\(r\\) se encuantra acotada entre \\(-1\\) y \\(1\\), ya que es una normalización de la covarianza por el producto de la desviación estándar de las dos variables aleatorias.\n\n\nSi \\(r=\\pm1\\) entonces el ajuste es perfecto. Para \\(r=0\\) los puntos estan dispersos aleatoriamente y no existe relación alguna entre las variables. Normalmente encontramos el estadístico \\(r^2\\) en lugar de \\(r\\). \\(r^2\\) se puede reescribir como\n\\[r^2=SCR/SCT=\\frac{SCT-SEC}{SCT}=1-\\frac{SEC}{SCT}=\\frac{C^2_{xy}}{(s_x s_y)^2}\\,,\\]\nlo que nos informa del porcentaje de la varianza explicada (\\(r^2={ varianza\\,explicada}/{ varianza\\,total}\\)) como vimos anteriormente. Un valor de \\(r=0.75\\) significa que la regresión lineal de \\(y\\) sobre \\(x\\), es decir \\(\\hat{y}\\), explica \\(100*r^2=56.25\\%\\) de la varianza total de la muestra.\nFinalmente puntualizar que también podemos calcular el coeficiente de correlación utilizando los estimados de los coeficientes de regresión. Para el caso de una línea recta sabemos\n\\[\\hat{b}_1=\\frac{C_{xy}}{s^2_x}\\,.\\]\nSi sustituimos en la definición de \\(r\\) obtenemos\n\\[r=\\hat{b}_1\\frac{s^2_x}{s_x s_y}=\\hat{b}_1\\frac{s_x}{s_y}\\,.\\]\nCoeficiente de correlación ajustado\n\nCon el fin de considerar los grados de libertad del modelo de regresión lineal, el coeficiente de correlación debe ser ajustado en función del número de variables independientes \\(k\\). Para ello hay que considerar la verdadera varianza de los errores\n\\[Var(SEC)=\\frac{SEC}{N-k-1}\\,,\\]\ny de la variable dependiente\n\\[Var(SCT)=\\frac{SCT}{N-1}\\,.\\]\nPuesto que los grados de libertad no son iguales (\\(N-1\\) vs \\(N-k-1\\)), el coeficiente de correlación ajustado al cuadrado (\\(\\tilde{r}^2\\)) se define como\n\\[\\tilde{r}^2=1-\\frac{Var(SEC)}{Var(SCT)}=1-\\frac{SEC/N-k-1}{SCT/N-1}=\n1-\\frac{N-1}{N-k-1}(1-r^2)=\\] \\[=1-(1-r^2)\\frac{N-1}{N-k-1}\\,,\\]\ndonde para \\(k=0\\) obtenemos la definición clásica del coeficiente de correlación.\n\nIntervalo de confianza para el coeficiente de correlación\nPodemos calcular intervalos de confianza para el coeficiente de correlación \\(r\\) por medio de la denominada transformación Z de Fisher. Básicamente transforma \\(r\\) en una variable normal estándar \\(Z\\)\n\\[Z=\\frac{1}{2}\\frac{ln(1+r)}{ln(1-r)}\\,,\\]\ncon desviación estándar\n\\[\\sigma(Z)=\\frac{1}{\\sqrt{(N-3)}}\\,,\\]\ny media\n\\[\\mu(Z)=\\frac{1}{2}\\frac{ln(1+\\rho_0)}{ln(1-\\rho_0)}\\,,\\]\ndonde \\(\\mu(Z)\\) es la media esperada (media poblacional) del estadístico \\(Z\\).\nEl intervalo de confianza se escribe entonces como\n\\[Z-Z_{\\alpha/2}&lt;Z&lt;Z+Z_{\\alpha/2}\\]\n*Ejemplo**:\nSupongamos \\(N=21\\) y \\(r=0.8\\). Encuentra el intervalo de confianza al \\(95\\%\\) para el coeficiente de correlación poblacional \\(\\rho_0\\).\n\\[Z=\\frac{1}{2}\\frac{ln(1+0.8)}{ln(1-0.8)}=1.0986\\]\nPuesto que \\(Z\\) esta Normalmente distribuida, entonces todos los valores deben de caer dentro de 1.96 desviaciones estándar de \\(Z\\). Entonces, al \\(95\\%\\), la verdadera media \\(\\mu_Z\\) esta contenida en \\[Z-1.96\\sigma(Z) &lt; \\mu(Z) &lt; Z + 1.96\\sigma(Z)\\] \\[Z-1.96\\frac{1}{\\sqrt{(21-3)}} &lt; \\mu(Z) &lt; Z + 1.96\\frac{1}{\\sqrt{(21-3)}}\\] \\[0.6366&lt;\\mu(Z)&lt;1.5606\\]\nLos límites encontrados para \\(\\mu(Z)\\) los podemos transformar en términos de la verdadera correlación \\[\\mu(Z)=0.6366=\\frac{1}{2} \\left\\{ \\frac{1+\\rho}{1-\\rho} \\right\\} \\rightarrow  \\rho=0.56\\,,\\] donde hemos usado \\[e^{2\\mu}=\\frac{1+\\rho}{1-\\rho}\\] \\[e^{2\\mu}-1=\\rho(1+e^{2\\mu})\\] \\[\\rho=\\frac{(e^{2\\mu(Z)}-1)}{(e^{2\\mu(Z)}+1)}\\,.\\]\nPodemos afirmar con un 95% de confianza que la verdadera correlación \\(\\rho\\) esta en el intervalo \\(0.56&lt;\\rho,0.92\\), dado un tamaño muestral \\(N=21\\) y una correlación muestral \\(r=0.8\\).\n{erdaderos grados de libertad} \\ Ya hemos definido anteriormente los grados de libertad se define como el número de muestras independientes \\(N\\) menos el número de parámetros que se quieren estimar. Esta definición es un tanto incorrecta puesto que debemos de también asegurar que las \\(N\\) muestras son efectivamente independientes, es decir, no estan autocorreladas en el espacio-tiempo. Para considerar esto \\(N\\) debe reesecribirse como\n\\[N^{*}=\\frac{N}{\\left[ \\sum\\limits^{\\infty}_{\\tau=-\\infty}\nC_{xx}(\\tau)C_{yy}(\\tau) + C_{xy}(\\tau)C_{yx}(\\tau)\n\\right]/\\left[ C_{xx}(0)C_{yy}(0)\\right]}=\\]\n\\[\\frac{N}{\\left[\\sum\\limits^{\\infty}_{\\tau=-\\infty}\n               \\rho_{xx}(\\tau)\\rho_{yy}(\\tau) +\n           \\rho_{xy}(\\tau)\\rho_{yx}(\\tau)\\right]}\\,.\\]\nEn general, series de datos suelen estar correlacionados en el espacio-tiempo y \\(N^{*}&lt;&lt;N\\). Cuanto mayores las escalas de correlación espaciales-temporales, menores los \\(N^*\\). Esto nos hace pensar que es muy importante la selección de las escalas espaciales-temporales sobre las que queremos calcular un estadístico. Para extraer las escalas de interés podemos usar métodos espectrales y filtros. El proceso de filtrado se encarga de eliminar aquellas escalas que esperamos no contribuyen a la verdadera correlación pero pueden adicionar correlación artificial debido a errores instrumentales y de muestreo."
  },
  {
    "objectID": "article2_new.html#propagación-de-errores",
    "href": "article2_new.html#propagación-de-errores",
    "title": "Julio Sheinbaum",
    "section": "Propagación de errores",
    "text": "Propagación de errores\n*Regla 1** Si \\(x\\) e \\(y\\) tienen errores aleatorios independientes \\(\\delta{x}\\) y \\(\\delta{y}\\),nentonces el error en la suma \\(z=x+y\\) es\n\\[\\delta{z}=\\sqrt{\\delta{x}^2 + \\delta{y}^2}\\,.\\]\n*Regla 2**\nSi \\(x\\) e \\(y\\) tienen errores aleatorios independientes \\(\\delta{x}\\) y \\(\\delta{y}\\), entonces el error en la multiplicación \\(z=xy\\) es\n\\[\\frac{\\delta{z}}{z}=\\sqrt{\\left(\\frac{\\delta{x}^2}{x}\\right) + \\left(\\frac{\\delta{y}^2}{y}\\right)}\\,.\\]\n*Regla 3**\n\nSi \\(z=f(x)\\), donde \\(f()\\) es una función dada, entonces\n\\[\\delta{z}=|f'(x)|\\delta{x}\\,.\\]\n*Formula general para propagación del error**\nSea \\(x_1, x_2,....,x_N\\) medidas con incertidumbres \\(\\delta{x_1},\\delta{x_2},....,\\delta{x_3}\\). Supongamos que queremos determinar \\(q\\), el cual es una función de \\(x_1,x_2,...,x_N\\): \\[q=f(x_1, x_2,...,x_N)\\,.\\] El error asociado a \\(q\\) es entonces\n\\[\\delta{q}=\\sqrt{\\left( \\frac{\\partial{q}}{\\partial{x_1}}\\delta{x_1}\\right)^2 + ... +\n                  \\left( \\frac{\\partial{q}}{\\partial{x_N}}\\delta{x_N}\\right)^2}\\] Si \\(q=x_1+x_2\\) entonces obtenemos la regla 1:\n\\[\\frac{\\partial{q}}{\\partial{x_1}}=1\\,,\\] \\[\\frac{\\partial{q}}{\\partial{x_2}}=1\\,,\\]\n\\[\\delta{q}=\\sqrt{\\delta{x_1}^2 + \\delta{x_2}^2}\\,.\\]\nSi \\(q=x_1x_2\\) entonces obtenemos la regla 2:\n\\[\\frac{\\partial{q}}{\\partial{x_1}}=x_2\\,,\\] \\[\\frac{\\partial{q}}{\\partial{x_2}}=x_1\\,,\\]\n\\[\\delta{q}=\\sqrt{x^2_2\\delta{x_1}^2 + x^2_1\\delta{x_2}^2}=\\sqrt{q^2\\left[ \\left(\\frac{\\partial{x_1}}{x_1}\\right)^2 + \\left(\\frac{\\partial{x_2}}{x_2}\\right)^2\\right]}\\,.\\]\n\\[\\frac{\\delta{q}}{q}=\\sqrt{\\left(\\frac{\\delta{x_1}^2}{x_1}\\right) + \\left(\\frac{\\delta{x_2}^2}{x_2}\\right)}\\,.\\]\nDemstración:*\nQueremos calcular la desviación estándar de la función\n\\[q=q(x_1, x_2,....,x_N)\\,,\\]\nque depende de \\(N\\) variables independientes \\(x_1\\), \\(x_2\\), ….,\\(x_N\\). El desarrollo de Taylor de la función \\(q\\) alrededor de la media \\(\\bar{q}\\) se puede escribir: \\[q - \\bar{q}=\\left(x_1 - \\bar{x}_1\\right)\\frac{\\partial{q}}{\\partial{x_1}} +\n                          \\left(x_2 - \\bar{x}_2\\right)\\frac{\\partial{q}}{\\partial{x_2}}+  \\,.\\,.\\,.\\,\n                          + \\left(x_N - \\bar{x}_N\\right)\\frac{\\partial{q}}{\\partial{x_N}}\\,.\\]\nLa varianza de la función \\(q\\) es:\n\\[s^2_q=\\frac{1}{N-1}\\sum\\limits^N_{i=1}\\left( q_i-\\bar{q} \\right)^2=\n               \\frac{1}{N-1}\\left[\n               \\left(x_1 - \\bar{x}_1\\right)\\frac{\\partial{q}}{\\partial{x_1}} +\n               \\left(x_2 - \\bar{x}_2\\right)\\frac{\\partial{q}}{\\partial{x_2}} +\n              \\,.\\,.\\,.\\, +\n               \\left(x_N - \\bar{x}_N\\right)\\frac{\\partial{q}}{\\partial{x_N}}\n               \\right]^2=\\]\n\\[=\\frac{1}{N-1}\\left[\n\\left(x_1 - \\bar{x}_1\\right)^2\\left(\\frac{\\partial{q}}{\\partial{x_1}}\\right)^2 +\n\\left(x_2 - \\bar{x}_2\\right)^2\\left(\\frac{\\partial{q}}{\\partial{x_2}}\\right)^2 +\n2\\left(x_1 - \\bar{x}_1\\right)\\left(x_2 - \\bar{x}_2\\right)\n\\frac{\\partial{q}}{\\partial{x_1}}\\frac{\\partial{q}}{\\partial{x_2}}+\\,.\\,.\\,.\\right]=\\]\n\\[=s^2_{x_1}\\left(\\frac{\\partial{q}}{\\partial{x_1}}\\right)^2 +\n   s^2_{x_2}\\left(\\frac{\\partial{q}}{\\partial{x_2}}\\right)^2+\n   2 s_{{x_1}{x_2}}\n   \\frac{\\partial{q}}{\\partial{x_1}}\\frac{\\partial{q}}{\\partial{x_2}}+\\,.\\,.\\,.\\]\ny finalmente la expresión general para la propagación del error es:\n\\[s_{q}=\\sqrt{s^2_{x_1}\\left(\\frac{\\partial{q}}{\\partial{x_1}}\\right)^2 +\n               s^2_{x_2}\\left(\\frac{\\partial{q}}{\\partial{x_2}}\\right)^2+\n                2 s_{{x_1}{x_2}}\n                \\frac{\\partial{q}}{\\partial{x_1}}\\frac{\\partial{q}}{\\partial{x_2}}+\\,.\\,.\\,.}\\]\nEjemplo:\nLa ecuación para el cálculo de la salinidad a partir de la conductividad (C) y temperatura (T) es (Unesco EOS-80)\n\\[S=a_0+a_1 R_T^{1/2}+a_2 R_T+a_3 R_T^{3/2}+a_4 R_T^{2}+a_5 R_T^{5/2}+\\Delta{S}\\,,\\]\ndonde \\[R_T=\\frac{R}{R_p r_t}\\,\\,\\,,\\,\\,R=\\frac{C(S,T,0)}{C(35,15,0)}\\,,\\]\n\\(C(35,15,0)\\) es la conductividad de un agua de salinidad práctica 35 a los \\(15\\,^\\circ{ C}\\), \\[r_t=c_0 + c_1 T + c_2 T^2 + c_3T^3 + c_4T^4\\,,\\]\n\\[R_p=1+\\frac{P(e_1+e_2P+e_3P^2)}{(1+d_1T+d_2T^2+(d_3 + d_4T)R)}\\,,\\] y \\[\\Delta{S}=\\frac{T-15}{1+k(T-15)}(b_0+b_1 R_T^{1/2}+b_2 R_T+b_3 R_T^{3/2}+b_4 R_T^{2}+b_5 R_T^{5/2})\\,,\\]\ncon los coeficientes \\(a_i\\,,b_i\\,,c_i\\,,d_i\\,,\\,\\,y\\,e_i\\) \n\\(a_0=0.0080\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,b_0=0.0005\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,c_0=0.6766097\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,d_1=3.426\\,e^{-2}\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,e_1=2.070\\,e^{-5}\\)  \\(a_1=-0.1692\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,b_1=-0.0056\\,\\,\\,\\,\\,\\,\\,\\,\\,c_1=2.00564\\,e^{-2}\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,d_2=4.464\\,e^{-4}\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,e_2=-6.370\\,e^{-10}\\)  \n\\(a_2=25.3851\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,b=-0.0066\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,c_2=1.104259\\,e^{-4}\\,\\,\\,\\,\\,\\,\\,\\,\\,d_3=4.215\\,e^{-1}\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,e_3=3.989\\,e^{-15}\\)\n\n\\(a_3=14.0941\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,b=-0.0375\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,c_3=-6.9698\\,e^{-7}\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,d_4=-3.107\\,e^{-3}\\)   \\(a_4=-7.0261\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,b_4=0.0636\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,c_4=1.0031\\,e^{-9}\\)\n\n\\(a_5=2.7081\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,b_5=-0.0144\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\)\n\n\\(\\sum a_i=35.0000\\,\\,\\,\\,\\sum b_i=0.0000\\)\n\n\\(k=0.0162\\)\n\nSi el error de precisión del termistor y de la celda de conductividad del CTD es \\(\\delta{T}=0.001\\,^\\circ{ C}\\) y \\(\\delta{C}=0.001\\,{ S}{ m}^{-1}\\), respectivamente, calcule la incertidumbre asociada al cálculo de la salinidad \\(S\\) a partir de la formula general de propagación del error para \\(C=5\\,{ S}\\,{ m}^{-1}\\), \\(T=28\\,^\\circ\\,{ C}\\), y \\(P=50\\,{ dbar}\\). Suponga que \\(\\delta{P}=0\\), es decir, el altímetro no tiene errores de precisión.\n\nResultado:\nEl error estándar asociado a la salinidad es\n\\[\\delta{S}=\\sqrt{\\left( \\frac{\\partial{S}}{\\partial{T}}\\delta{T} \\right)^2 + \\left( \\frac{\\partial{S}}{\\partial{C}}\\delta{C}\\right)^2}\\,,\\]\ndonde\n\\[\\frac{\\delta{S}}{\\delta{T}}=\n      \\left[(1+k(T-15))^{-1} - (k(T-15))(1+k(T-15))^{-2}\\right]\\] \\[\\left (b_0+b_1 R_T^{1/2}+b_2 R_T+b_3 R_T^{3/2}+b_4 R_T^{2}+b_5 R_T^{5/2}\\right)\\,,\\] y\n\\[\\frac{\\delta{S}}{\\delta{C}}=\n                 \\frac{1}{2} a_1 \\frac{R_T^{-1/2}}{C(35,T,0)} +\n         a_2 \\frac{1}{C(35,T,0)} +\n             \\frac{3}{2}a_3 \\frac{R_T^{1/2}}{C(35,T,0)}+ \\]\n\\[+2 a_4 \\frac{R_T}{C(35,T,0)} +\\frac{5}{2} a_5 \\frac{R_T^{3/2}}{C(35,T,0)}+\\frac{T-15}{(1+k(T-15))}\\]\n\\[\\left(b_1 \\frac{R_T^{-1/2}}{C(35,T,0)} +\n         b_2 \\frac{1}{C(35,T,0)} +\n             \\frac{3}{2}b_3 \\frac{R_T^{1/2}}{C(35,T,0)}+\n               2 b_4 \\frac{R_T}{C(35,T,0)} +\n     +\\frac{5}{2} b_5 \\frac{R_T^{3/2}}{C(35,T,0)}\n     \\right)\\,.\\]\nPor lo tanto, dada una medida de conductividad \\(C\\) y temperatura \\(T\\) podemos calcular \\({\\delta{S}}/{\\delta{T}}\\) y \\({\\delta{S}}/{\\delta{C}}\\) y obtener la desviación estándar asociado al cálculo de la salinidad \\(\\delta{S}\\).\nEjercicio:\nCalcule el error estándar asociado a la densidad referenciada a la superfície que se obtiene al usar el algoritmo del estado del agua de mar (Unesco EOS-80) \\[\\rho(S,T,0)=\\rho_w + \\left(b_0+b_1 T+b_2 T^2+b_3 T^{3}+b_4 T^{4}\\right) S +\\]\n\\[+\\left(c_0+c_1 T+c_2 T^2\\right) S^{3/2} + d_0 S^2\\,,\\]\ncon los coeficientes \\(b_i\\), \\(c_i\\), y \\(d_0\\)\n\\(b_0=8.24493\\,e^{-1}\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,c_0=-5.72466\\,e^{-3}\\)  \\(b_1=-4.0899\\,e^{-3}\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,c_1=1.0227\\,e^{-4}\\)\n\\(b_2=7.6438\\,e^{-5}\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,c_2=-1.6546\\,e^{-6}\\)\n\\(b_3=-8.2467\\,e^{-7}\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\)   \\(b_4=5.3875\\,e^{-9}\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,d_0=4.8314\\,e^{-4}\\),,\n\ny la densidad de referencia para agua pura (\\(\\rho_w\\)) definida como:\n\\[\\rho_w=a_0+a_1 T+a_2 T^2+a_3 T^{3}+a_4 T^{4}+a_5 T^{5}\\,,\\]\ndonde los coeficientes \\(a_i\\) son:\n\\(a_0=999.842594\\)  \\(a_1=6.793952\\,e^{-2}\\)  \\(a_2=-9.095290\\,e^{-3}\\)  \\(a_3=1.001685\\,e^{-4}\\)  \\(a_4=-1.120083\\,e^{-6}\\) \\(a_5=6.536332\\,e^{-9}\\)."
  },
  {
    "objectID": "article2_new.html#métodos-de-interpolación",
    "href": "article2_new.html#métodos-de-interpolación",
    "title": "Julio Sheinbaum",
    "section": "Métodos de Interpolación",
    "text": "Métodos de Interpolación\nInterpolación es el procedimiento para el cual obtenemos valores de propiedades en posiciones o tiempos que nunca fueron muestreados a partir de observaciones existentes en otras localizaciones o tiempos. En oceanografía necesitamos interpolar (i) para rellenar huecos cuando el instrumento dejo de medir, (ii) para obtener mapas espaciales de contornos (2D), (iii) para calcular alguna propiedad derivada en un punto concreto\n\nInterpolación Lineal\nLa interpolación mas sencilla, pero no por ello peor, es la interpolación lineal. Esta interpolación se basa en ajustar una línea recta entre los puntos conocidos, e interpolar cualquier punto intermedio como un punto a lo largo de la recta. Este tipo de interpolación puede ser usado para rellenar huecos en nuestras series temporales. La interpolación lineal de unas serie y(t) se puede escribir como \\[y(t_i)=y(t_0)+\\frac{y(t_1)-y(t_0)}{t_1 - t_0}(t_i-t_0)\\,.\\]\n\n\nInterpolación polinómica\nEn el caso que queramos interpolar entre mas de dos puntos simultaneamente, debemos de usar polinomios de orden superior a la recta (orden 1). Es decir, \\[y(x)=a_0 + a_1 x + a_2 x^2 + ... + a_{n-1}x^{n-1} + a_n x_n\\,.\\] Si tomamos \\(N\\) observaciones (\\(y_i(x)\\,\\,;i=0,2,...,N-1\\)) obtenemos un sistema de \\(N\\) ecuaciones\n\\[\\left(\\begin{array}{ccccc}\n  1 & x_0 & ... & x_0^{N-1} & x_0^N\\\\\n  1 & x_1 & ... & x_1^{N-1} & x_1^N\\\\\n  . & . & . & . & .\\\\\n  . & . & . & . & .\\\\\n  . & . & . & . & .\\\\\n  1 & x_N & ... & x_N^{N-1} & x_N^N\\\\\n     \\end{array}\\right)\n    \\left(\\begin{array}{c}\n  a_0 \\\\\n  a_1 \\\\\n  . \\\\\n  . \\\\\n  . \\\\\n  a_N \\\\\n     \\end{array}\\right)=\n     \\left(\\begin{array}{c}\n  y_0 \\\\\n  y_1 \\\\\n  . \\\\\n  . \\\\\n  . \\\\\n  y_N \\\\\n     \\end{array}\\right)\\,,\\] lo cual se puede resolver con eliminación Gauss-Jordan. Este método es muy lento y por ello se usa el método de Lagrange.\n{ étodo de Lagrange}\nDe nuevo asumimos \\[p(x)=a_0 + a_1 x + a_2 x^2 + ... + a_{n-1}x^{n-1} + a_n x_n=\\sum\\limits^{N}_{k=0} a_k x^k\n=\\sum^{N+1}_{i=1} y_i {\\cal L}_i(x)\\,,\\] donde \\[{\\cal L}_i(x)=\\prod^{N+1}_{\\substack{k=1\\\\ k\\ne i}} \\frac{x-x_k}{x_i-x_k}\\,,\\] son los denominados polinomios de Lagrange, y \\(\\prod\\) es el operador producto. Puesto que este operador cuando \\(k\\ne i\\) no incluye el producto, a pesar que varíe de \\(1\\) a \\(N+1\\), obtendremos un polinomio de orden \\(N\\).\nEsta suma de polinomios de Lagrange es el polinomio de menor grado que interpola un conjunto de datos, es decir, \\[p(x_j)=\\sum^{N+1}_{i=1} y_i {\\cal L}_i(x_j)=y_j\\,.\\] {emostración:}\nLos polinomios de Lagrange para \\(i\\ne j\\) son iguales a cero, y para \\(i=j\\) son iguales a 1. Veamos esto:\n\\[{ (1)}\\,\\,\\,\\,\\,{ Para}\\,\\,\\,{i \\ne j}:\\,\\,\\,\\,\\,{\\cal L}_i(x_j)=\\prod^{N+1}_{\\substack{k=1\\\\ k\\ne i}} \\frac{x_j-x_k}{x_i-x_k}=\\] \\[=\\frac{x_j-x_1}{x_i-x_1} \\frac{x_j-x_2}{x_i-x_2} ...\\frac{x_j-x_j}{x_i-x_j}....\\frac{x_j-x_{N+1}}{x_i-x_{N+1}}\\]\n\\[{ (2)}\\,\\,\\,\\,\\,{ Para}\\,\\,\\,{i = j}:\\,\\,\\,\\,\\,{\\cal L}_i(x_j)=\\prod^{N+1}_{\\substack{k=1\\\\ k\\ne i}} \\frac{x_j-x_k}{x_j-x_k}=1\\]\nDe modo que \\[{\\cal L}_i(x_j)=\\delta_{ij}=\\begin{cases}\n\\begin{array}{c}\n   1 \\,\\,\\,\\,\\text{si}\\,\\,\\,\\,i= j\\\\\n   0 \\,\\,\\,\\,\\text{si}\\,\\,\\,\\,i\\ne j\\\\\n\\end{array}\n\\end{cases}\\,.\\]\nFinalmente podemos concluir entonces que \\[p(x_j)=\\sum^{N+1}_{i=1} y_i {\\cal L}_i(x_j)=\\sum^{N+1}_{i=1} y_i \\delta_{ij}=y_j\\,.\\] es un polinomio de grado no mayor a \\(N\\) y que \\(p(x_j)=y_j\\).\nEl polinomio se puede reescribir en terminos de la función \\(Q_i\\) como \\[p(x)=\\sum^{N+1}_{i=1} y_i[Q_i(x)/Q_i(x_i)]\\,,\\] donde \\[Q_i(x)=(x-x_1)(x-x_2)....(x-x_{i-1})(x-x_{i+1})...(x-x_{N-1})\\,,\\] es el producto de todas las diferencias excepto la posición \\(i\\) (i.e., \\(x-x_{i}\\)). Si expandemos \\(p(x)\\) \\[p(x)=y_1\\frac{(x-x_2)(x-x_3)...(x-x_{N+1})}{(x_1-x_2)(x_1-x_3)...(x_1-x_{N+1})} +\n       y_2\\frac{(x-x_1)(x-x_3)...(x-x_{N+1})}{(x_2-x_1)(x_2-x_3)...(x_2-x_{N+1})} +\\] \\[+...+y_{N+1}\\frac{(x-x_1)(x-x_2)...(x-x_{N})}{(x_{N+1}-x_1)(x_{N+1}-x_2)...(x_{N+1}-x_{N})}\\,.\\]\n{ jemplo:}\nConsidere los puntos (0,2), (1,2), (2,0) y (3,0) para los cuales queremos ajustar un polinomio de orden 3 \\[y(x)=2\\frac{(x-1)(x-2)(x-3)}{(0-1)(0-2)(0-3)} + 2\\frac{(x-0)(x-2)(x-3)}{(1-0)(1-2)(1-3)} + 0\n+ 0=\\] \\[=\\frac{2}{3}x^3 -3x^2 +\\frac{7}{3}x +2\\,.\\] \n\n\nSpline cúbico\nLa interpolación por spline cúbicos es un método de ajuste de polinomios de orden 3 por segmentos. Veamos este método con un ejemplo.\nSupongamos que queremos interpolar los siguientes datos \\((x_i,y_i)=[(2,-1), (3,2), (5,-7)]\\) con una spline cúbica. Primero definimos un polinomio cúbico para cada intervalo: \\[s(x)=a_1x^3 + b_1 x^2 +c_1x +d_1\\,\\,\\,\\,\\,{ si}\\,\\,\\,\\,\\,x\\in[2,3]\\] \\[s(x)=a_2x^3 + b_2 x^2 +c_2x +d_2\\,\\,\\,\\,\\,{ si}\\,\\,\\,\\,\\,x\\in[3,5]\\]\nA continuación debemos de asegurar que los polinomios pasan por los puntos del problema: \\[s(2)=8a_1 + 4b_1  +2c_1 +d_1=-1\\] \\[s(3)=27a_1 + 9b_1  +3c_1 +d_1=2\\] \\[s(3)=27a_2 + 9b_2  +3c_2 +d_2=2\\] \\[s(5)=125a_2 + 25b_2  +5c_2 +d_2=-7\\]\nAhora calculamos la primera y segundas derivadas para cada intervalo \\[s'(x)=3a_1x^2+2b_1x+c_1\\,\\,\\,\\,\\,{ si}\\,\\,\\,\\,\\,x\\in[2,3]\\] \\[s'(x)=3a_2x^2+2b_2x+c_2\\,\\,\\,\\,\\,{ si}\\,\\,\\,\\,\\,x\\in[3,5]\\] \\[s''(x)=6a_1x+2b_1\\,\\,\\,\\,\\,{ si}\\,\\,\\,\\,\\,x\\in[2,3]\\] \\[s''(x)=6a_2x+2b_2\\,\\,\\,\\,\\,{ si}\\,\\,\\,\\,\\,x\\in[3,5]\\] y aseguramos que sean contínuas. Para ello debemos de igualar las derivadas entre intervalos de manera que no hayan discontinuidades \\[3a_1(3)^2+2b_1(3)+c_1=3a_2(3)^2+2b_2(3)+c_2\\rightarrow 27a_1+6b_1+c_1=27a_2+6b_2+c_2\\] \\[6a_1(3)+2b_1=6a_2(3)+2b_2\\rightarrow 18a_1+2_b1=18a_2+2_b2\\]\nEn este momento tenemos 6 equaciones y 8 incógnitas. Debemos por lo tanto encontrar dos ecuaciones mas. Para ello vamos a forzar que en los extremos la segunda derivada sea nula, es decir, no haya curvatura \\[s''(x_0)=s''(2)=0\\rightarrow 6a_1(2)+2b_1=0 \\rightarrow 12a_1 + 2b_1=0\\] \\[s''(x_N)=s''(5)=0\\rightarrow 6a_2(5)+2b_2=0 \\rightarrow 30a_2+2b_2=0\\]\nAhora ya tenemos un sistema determinado, es decir, 8 ecuaciones y 8 incógnitas \\[8a_1 + 4b_1  +2c_1 +d_1=-1\\] \\[27a_1 + 9b_1  +3c_1 +d_1=2\\] \\[27a_2 + 9b_2  +3c_2 +d_2=2\\] \\[125a_2 + 25b_2  +5c_2 +d_2=-7\\] \\[27a_1+6b_1+c_1=27a_2+6b_2+c_2\\] \\[18a_1+2b_1=18a_2+2b_2\\] \\[12a_1 + 2b_1=0\\] \\[30a_2+2b_2=0\\]\nLo cual en notación matricial se puede escribir\n\\[\\left(\\begin{array}{cccccccc}\n  8 & 4 & 2 & 1 & 0 & 0 & 0 & 0\\\\\n  27 & 9 & 3 & 1 & 0 & 0 & 0 & 0\\\\\n  0 & 0 & 0 & 0 & 27 & 9 & 3 & 1\\\\\n  0 & 0 & 0 & 0 & 125 & 25 & 5 & 1\\\\\n  27 & 6 & 1 & 0 & -27 & -6 & -1 & 0\\\\\n  18 & 2 & 0 & 0 & -18 & -2 & 0 & 0\\\\\n  12 & 2 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n  0 & 0 & 0 & 0 & 30 & 2 & 0 & 0\\\\\n    \\end{array}\\right)\\left(\\begin{array}{c}\n  a_1 \\\\\n  b_1 \\\\\n  c_1 \\\\\n  d_1 \\\\\n  a_2 \\\\\n  b_2 \\\\\n  c_2 \\\\\n  d_2 \\\\\n    \\end{array}\\right)=\n    \\left(\\begin{array}{c}\n  -1 \\\\\n  2 \\\\\n  2 \\\\\n  -7 \\\\\n  0 \\\\\n  0 \\\\\n  0 \\\\\n  0 \\\\\n    \\end{array}\\right)\\]\nEste sistema se puede resolver facilmente si la matriz de datos es invertible. Los coeficientes resultantes son \\[a_1=-1.25\\,\\,\\,\\,\\,b_1=7.5\\,\\,\\,\\,c_1=-10.75\\,\\,\\,\\,d_1=0.5\\] \\[a_1=0.625\\,\\,\\,\\,\\,b_1=-9.375\\,\\,\\,\\,c_1=39.875\\,\\,\\,\\,d_1=-50.125\\,,\\] y los polinomios de nuestra spline cúbica es \\[s(x)=-1.25x^3+7.5x^2-10.75x+0.5\\,\\,\\,\\,\\,{ si}\\,\\,\\,\\,\\,x\\in[2,3]\\] \\[s(x)=0.625x^3+-9.375x^2-39.875x+50.125\\,\\,\\,\\,\\,{ si}\\,\\,\\,\\,\\,x\\in[3,5]\\]"
  },
  {
    "objectID": "article2_new.html#interpolación-objetiva",
    "href": "article2_new.html#interpolación-objetiva",
    "title": "Julio Sheinbaum",
    "section": "Interpolación Objetiva",
    "text": "Interpolación Objetiva\nUn mapa objetivo se obtiene como una regresión múltiple (donde el error cuadrático medio es mínimo) de un conjunto de observaciones discretas. Se utiliza en oceanografía para obtener mapas continuos (mallas regulares) a partir de datos discretos distribuidos irregularmente en el espacio. Las variables representadas en el mapa objetivo pueden modificarse de una realización a otra, con lo que deben de considerarse las anomalías de las variables en lugar de las variables en si mismas. Se debe de definir un ensemble (promedio), climatología, o candidato y extraerlo a cada variable para obtener anomalías. Este proceso de elección de la media es una parte delicada de la interpolación objetiva. En general, para el océano, la media es desconocida ya que solamente tenemos pocas realizaciones de nuestro muestreo. Una forma de operar es estimar la media ajustando un polinomio de bajo orden a nuestros datos discretos, extraer esta a los datos y proceder con la interpolación objetiva. La media se añade de nuevo despues de calcular el mapa objetivo de las fluctuaciones. \\\nTípicamente se debe asumir dos condiciones: \\\n\nEl error (o ruido) asociado a la interpolación no esta correlacionado con nuestra señal (variable a interpolar) \\[&lt;\\phi_i\\epsilon_i&gt;=0\\] \\\nEl error no esta correlacionado de una estación a la otra \\[&lt;\\epsilon_i \\epsilon_j&gt;=\n\\begin{cases}\n\\begin{array}{c}\n   &lt;\\epsilon^2&gt; \\,\\,\\,\\,\\text{si} \\,\\,\\,\\,i=j\\\\\n   0    \\,\\,\\,\\,        \\text{si} \\,\\,\\,\\,i\\ne j\\\\\n\\end{array}\n\\end{cases}\\]\n\nSupongamos entonces fluctuaciones respecto un estado climatológico o media \\[\\phi_i'=\\phi_i-\\bar{\\phi}\\,; i=1,2,...,N\\] Ahora vamos a intentar aproximar el valor de \\(\\phi'\\) en un punto de una malla, \\(\\phi_g\\), en términos de una combinación lineal de los valores en estaciones vecinas \\(\\phi_i\\) (señal). Entonces el problema de mínimos cuadrados es \\[\\phi'_g=\\sum\\limits^N_{i=1}b_i\\phi_i'\\] donde \\(\\phi'_g\\) son anonmalías en la malla regular y \\(\\phi_i'\\) son anomalías en las estaciones. Los mejores coeficientes son aquellos que minimizan el error cuadrático medio, es decir,\n\\[SEC=\\sum\\limits^N_{i=1}\\left(\\phi'_g-\\sum\\limits_{i=1}^N b_i\\phi'_i \\right)^2\n     =\\sum\\limits^N_{i=1}\\left(\\phi'_g-\\sum\\limits_{i=1}^N b_i\\phi'_i \\right)\\left(\\phi'_g-\\sum\\limits_{i=1}^N b_i\\phi'_i \\right)=\\] \\[\\sum\\limits^N_{i=1}\\left[\\phi'_g \\phi'_g -2\\sum\\limits_{i=1}^N b_i\\phi'_g\\phi'_i\n      +\\sum\\limits^N_{i=1}\\sum\\limits^N_{j=1} b_i b_j \\phi'_i \\phi'_j \\right]=\\] \\[=\\sum\\limits^N_{i=1}\\phi'_g \\phi'_g -2 \\sum\\limits^N_{i=1} b_i \\sum\\limits^N_{i=1} \\phi'_g\\phi'_i + \\sum\\limits^N_{i=1}\\sum\\limits^N_{j=1}\nb_i b_j \\sum\\limits^N_{i=1} \\phi'_i \\phi'_j\\,.\\]\nEl error normalizado se puede escribir como\n\\[\\epsilon=\\frac{SEC}{\\sum\\limits^N_{i=1}\\phi'_g \\phi'_g}=1-2 \\sum\\limits^N_{i=1} b_i \\frac{\\sum\\limits^N_{i=1} \\phi'_g\\phi'_i}{\\sum\\limits^N_{i=1}\\phi'_g \\phi'_g} + \\sum\\limits^N_{i=1}\\sum\\limits^N_{j=1} b_i b_j \\frac{\\sum\\limits^N_{i=1} \\phi'_i \\phi'_j}{\\sum\\limits^N_{i=1}\\phi'_g \\phi'_g}=\\] \\[=1-2 \\sum\\limits^N_{i=1} b_i r_{gi} + \\sum\\limits^N_{i=1}\\sum\\limits^N_{j=1} b_i b_j r_{ij}\\,,\\]\ndonde\n\\[r_{gi}=\\frac{\\sum\\limits^N_{i=1} \\phi'_g\\phi'_i}{\\sum\\limits^N_{i=1}\\phi'_g \\phi'_g}\\,\\,\\,\\,\\,{ y}\\,\\,\\,\\,\\,r_{ij}=\\frac{\\sum\\limits^N_{i=1} \\phi'_i \\phi'_j}{\\sum\\limits^N_{i=1}\\phi'_g \\phi'_g}\\,,\\]\nson matrices de correlación entre el punto de malla y las estaciones, y entre las estaciones, respectivamente.\nSi derivamos \\(\\epsilon\\) respecto los coeficientes, obtenemos la condición de minimización:\n\\[\\frac{\\partial{\\epsilon}}{\\partial{b_i}}=-2 r_{gi} + 2\\sum\\limits^N_{j=1}b_j r_{ij}=0\\,\\,\\,\\,\\,\\,\\,\\,\\,i=1,2,...,N\\,.\\] \\[r_{gi}=\\sum\\limits^N_{j=1}b_j r_{ij}\\,,\\] y los coeficientes en notación índice son\n\\[b_j=(r_{ij})^{-1}r_{gi}\\,\\]\nFinalmente, el valor de la medida en el punto de malla es\n\\[\\phi'_g=r_{gi}(r_{ij})^{-1}\\phi_i'\\]\no en notación matricial para un único punto de malla\n\\[\\phi'_g=r_{gs}(r_{ss})^{-1}\\phi'\\,,\\]\ndonde \\(\\phi'_g\\) es el valor de la anomalía en un punto de malla (matriz elemento, \\(1\\times 1\\)), \\(r_{gs}\\) es un vector fila compuesto por las correlaciones entre el punto de malla y las estaciones de medida, \\(r_{ss}\\) es una matriz de correlaciones entre todas las estaciones, y \\(\\phi'\\) es un vector columna con las anomalías en las estaciones.\nEn el caso de que tengamos \\(N\\) puntos de malla y \\(k\\) estaciones de medida, las ecuaciones básicas de la interpolación objetiva se pueden escribir en notación matricial como\n\\[\\left(\\begin{array}{c}\n  \\phi'_{g_1} \\\\\n  \\phi'_{g_2} \\\\\n  . \\\\\n  . \\\\\n  . \\\\\n  \\phi'_{g_N} \\\\\n     \\end{array}\\right)=\n\\left(\\begin{array}{ccccc}\n  r_{g_11} & r_{g_12} &... &  r_{g_1k}\\\\\n  r_{g_21} & r_{g_22} &... &  r_{g_2k}\\\\\n  . & . & . & . \\\\\n  . & . & . & . \\\\\n  . & . & . & . \\\\\n  r_{g_N1} & r_{g_N2} & ... & r_{g_Nk}\\\\\n     \\end{array}\\right)\n\\left(\\begin{array}{ccccc}\n  r_{11} & r_{12} &... &  r_{1k}\\\\\n  r_{21} & r_{22} &... &  r_{2k}\\\\\n  . & . & . & . \\\\\n  . & . & . & . \\\\\n  . & . & . & . \\\\\n  r_{k1} & r_{k2} & ... & r_{kk}\\\\\n     \\end{array}\\right)^{-1}       \\left(\\begin{array}{c}\n  \\phi'_1 \\\\\n  \\phi'_2 \\\\\n  . \\\\\n  . \\\\\n  . \\\\\n  \\phi'_k \\\\\n     \\end{array}\\right)\n     \\,.\\] \\[\\left( N \\times 1 \\right)\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\n  \\left( N \\times k \\right)\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\n  \\,\\,\\,\\,\\,\\,\\,\\,\\left( k \\times k \\right)\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\n  \\left( k \\times 1 \\right)\\,\\,\\,\\,\\,\\] \\\nEl error normalizado \\(\\epsilon\\) asociado al mapa interpolado es \\[\\epsilon=1-2 \\sum\\limits^N_{j=1} b_j r_{gi} + \\sum\\limits^N_{i=1}\\sum\\limits^N_{j=1} b_i b_j r_{ij}=\n           1-2 \\sum\\limits^N_{j=1} b_j \\sum\\limits^N_{j=1} b_j r_{ij} + \\sum\\limits^N_{i=1}\\sum\\limits^N_{j=1} b_i b_j r_{ij}=\\] \\[=1-2\\sum\\limits^N_{i=1}\\sum\\limits^N_{j=1} b_i b_j r_{ij} + \\sum\\limits^N_{i=1}\\sum\\limits^N_{j=1} b_i b_j r_{ij} =1-\\sum\\limits^N_{i=1}\\sum\\limits^N_{j=1} b_i b_j r_{ij}=\\] \\[= 1- \\sum\\limits^N_{j=1} b_j \\sum\\limits^N_{j=1} b_j r_{ij}  =1-\\sum\\limits^N_{i=1}r_{gi}b_i\\,,\\]\no en notación matricial para un único punto de malla \\[\\epsilon=1-r_{gs}(r_{ss})^{-1}r^T_{gs}\\,.\\]\nEn el caso de \\(N\\) puntos de malla y \\(k\\) estaciones \\[\\left(\\begin{array}{ccc}\n  \\epsilon_{1} \\\\\n  \\epsilon_{2} \\\\\n  . \\\\\n  . \\\\\n  . \\\\\n  \\epsilon_{N} \\\\\n     \\end{array}\\right)=\\text{Diag}\\left[\n     {\\textbf I}-\n\\left(\\begin{array}{ccccc}\n  r_{g_11} & r_{g_12} &... &  r_{g_1k}\\\\\n  r_{g_21} & r_{g_22} &... &  r_{g_2k}\\\\\n  . & . & . & . \\\\\n  . & . & . & . \\\\\n  . & . & . & . \\\\\n  r_{g_N1} & r_{g_N2} & ... & r_{g_Nk}\\\\\n     \\end{array}\\right)\n\\left(\\begin{array}{ccccc}\n  r_{11} & r_{12} &... &  r_{1k}\\\\\n  r_{21} & r_{22} &... &  r_{2k}\\\\\n  . & . & . & . \\\\\n  . & . & . & . \\\\\n  . & . & . & . \\\\\n  r_{k1} & r_{k2} & ... & r_{kk}\\\\\n     \\end{array}\\right)^{-1}\n\\left(\\begin{array}{ccccc}\n  r_{g_11} & r_{g_21} &... &  r_{g_N1}\\\\\n  r_{g_12} & r_{g_22} &... &  r_{g_N2}\\\\\n  . & . & . & . \\\\\n  . & . & . & . \\\\\n  . & . & . & . \\\\\n  r_{g_1k} & r_{g_2k} & ... & r_{g_Nk}\\\\\n     \\end{array}\\right) \\right]\n     \\,.\\] \\[\\left( N \\times 1 \\right)\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\n  \\left( N \\times k \\right)\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\n  \\,\\,\\,\\,\\,\\,\\,\\,\\left( k \\times k \\right)\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\n  \\left( k \\times N \\right)\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\]\nEste error es tal vez la característica mas importante de la interpolación objetiva. En general solo se muestra el mapa interpolado en las regiones donde \\(\\epsilon\\) emenor que un cierto valor. El error solo depende en las localizaciones de las estaciones y no en un valor particular de la medida. Es por ello que esta técnica puede ser usada para el diseño de experimentos. Es decir, nos sirve para saber cual debe ser la distribución óptima de las estaciones de medida para obtener el error mínimo en el mapa.\n\nNota: Si un punto de observación, \\(i=k\\), coincide con un punto de malla, entonces \\(r_{gk}=r_{kk}=1\\), y esperamos que el método de regresión nos de \\(b_k=1\\) y todos los demas pesos sean igual a cero. En este caso el valor interpolado en el punto de malla es igual al valor medido en la estación \\(\\phi'*g = r*{gk} (r\\_{kk})\\^{-1} \\phi'\\_k =1(1)\\^{-1}\\phi'\\_k=\\phi'\\_k\\) . En este caso el error es cero, \\(\\epsilon=1-r_{gk}(r_{kk})^{-1}r^T_{gk}=1-1(1)^{-1}1=0\\), ya que hemos asumido que los datos son perfectos. Si los puntos de las estaciones estan decorrelacionados con el punto de malla en cuestión (i.e., muy lejos de las estaciones), entonces \\(b_i=0\\) y \\(\\epsilon=1\\), y recuperamos la media o climatología\n\\[\\phi'_g=\\sum\\limits^N_{i=1}b_i\\phi'_i=0\\,\\,\\rightarrow\\,\\,\\phi'_g=\\phi_g-\\bar{\\phi}=0\\,\\,\\,\\,\\,\\text{and}\\,\\,\\,\\,\\,\\phi_g=\\bar{\\phi}\\]\nError observacional:\nVamos asumir ahora que las mediciones en las estaciones no son perfectas, es decir,\n\\[\\phi'_i=E[\\phi'_i] + \\delta_i\\,.\\]\nIgual que anteriormente asumimos que el error de instrumentación \\(\\delta_i\\) no esta correlacionado con la señal verdadera \\[&lt;E[\\phi_i]\\delta_i&gt;=0\\,,\\] y que el error instrumental entre estaciones tampoco esta correlacionado\n\\[&lt;\\epsilon_i \\epsilon_j&gt;=\n\\begin{cases}\n\\begin{array}{c}\n   &lt;\\delta^2&gt; \\,\\,\\,\\,\\text{si}\\,\\,\\,\\,i=j\\\\\n        0     \\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\text{si}\\,\\,\\,\\,i\\ne j\\\\\n\\end{array}\n\\end{cases}\\]\nEn este caso obtenemos\n\\[\\epsilon=1-2 \\sum\\limits^N_{i=1} b_i r_{gi} + \\sum\\limits^N_{i=1}\\sum\\limits^N_{j=1} b_i b_j r_{ij} + \\eta\\sum\\limits_{i=1}^N b_i^2\\,,\\]\ndonde \\(\\eta\\) es el cociente entre la varianza del error (ruido) y la varianza de las medidas, es decir el cociente ruido-señal \\[\\eta=\\frac{&lt;\\delta^2&gt;}{&lt;\\phi'_g\\phi'_g&gt;}\\,.\\]\nLa minimización del error nos da la condición \\[r_{gi}=\\sum\\limits^N_{j=1}b_j r_{ij}+\\eta b_i\\,,\\] y los coeficientes en notación índice son \\[b_j=(r_{ij} + \\eta I_{ij})^{-1}r_{gi}\\,,\\] donde \\(I_{ij}\\) es la matriz identidad. De esta expresión se deduce que cuando \\(\\eta\\) es grande (mucho ruido en la medida) entonces los coeficientes son mas pequeños respecto al caso de observaciones perfectas. Consecuentemente nuestro mapa interpolado se acerca mas a la climatología ya que la anomalía es menor para coeficientes mas pequeños\n\\[\\downarrow \\sum\\limits^N_{i=1}b_i\\phi'_i \\,\\,\\,\\,\\,\\rightarrow \\,\\,\\,\\,\\,\\downarrow\\phi'_g\\,\\,\\,\\,\\,\\,\\text{y}\\,\\,\\,\\,\\,\\phi_g\\rightarrow\\bar{\\phi}\\,.\\]\nIncluyendo errores observacionales, la interpolación objetiva tendera a la climatología o media y las nuevas observaciones serán incluidas pero con menos pesos. También podemos añadir diferentes errores ruido-señal en la diagonal principal para darle menos peso a las estaciones de medida que tienen mas incertidumbre asociada. Es conveniente entonces añadir errores observacionales al esquema de interpolación objetiva. Otra razón para ello es el caso en que existan dos estaciones que coincidan exactamente con un punto de malla. En este caso, si no hemos añadido error observacional la matriz de correlaciones entre las estaciones se convierte singular y el esquema de interpolación objetiva no se puede resolver. Por ejemplo para la interpolación en un único punto de malla a partir de dos estaciones de medida, la matriz de correlaciones sería singular\n\\[r_{ss}=\n       \\left(\\begin{array}{cc}\n  1 & 1 \\\\\n  1 & 1\n     \\end{array}\\right)\\,.\\]\nLa anomalía interpolada en notación matricial en un punto de malla es ahora \\[\\phi'_g=r_{gs}(r_{ss}+\\eta I)^{-1}\\phi'\\,,\\]\ny el error asociado al mapa interpolado en notación matricial en un único punto de malla es\n\\[\\epsilon=1-r_{gs}(r_{ss}+\\eta I)^{-1}r^T_{gs}\\,.\\]\nPara el caso de \\(N\\) puntos de malla y \\(k\\) estaciones obtenemos el mismo sistema que para el caso de medidas perfectas pero añadiendo el error ruido-señal en la diagonal principal de la matriz de correlaciones entre estaciones\n\\[\\left(\\begin{array}{ccc}\n  \\epsilon_{1} \\\\\n  \\epsilon_{2} \\\\\n  . \\\\\n  . \\\\\n  . \\\\\n  \\epsilon_{N} \\\\\n     \\end{array}\\right)=\\text{Diag}\\left[\n     {\\textbf I}-\n\\left(\\begin{array}{ccccc}\n  r_{g_11} & r_{g_12} &... &  r_{g_1k}\\\\\n  r_{g_21} & r_{g_22} &... &  r_{g_2k}\\\\\n  . & . & . & . \\\\\n  . & . & . & . \\\\\n  . & . & . & . \\\\\n  r_{g_N1} & r_{g_N2} & ... & r_{g_Nk}\\\\\n     \\end{array}\\right)\n\\left(\\begin{array}{ccccc}\n  r_{11}+\\eta & r_{12} &... &  r_{1k}\\\\\n  r_{21} & r_{22}+\\eta &... &  r_{2k}\\\\\n  . & . & . & . \\\\\n  . & . & . & . \\\\\n  . & . & . & . \\\\\n  r_{k1} & r_{k2} & ... & r_{kk}+\\eta\\\\\n     \\end{array}\\right)^{-1}\n\\left(\\begin{array}{ccccc}\n  r_{g_11} & r_{g_21} &... &  r_{g_N1}\\\\\n  r_{g_12} & r_{g_22} &... &  r_{g_N2}\\\\\n  . & . & . & . \\\\\n  . & . & . & . \\\\\n  . & . & . & . \\\\\n  r_{g_1k} & r_{g_2k} & ... & r_{g_Nk}\\\\\n     \\end{array}\\right) \\right]\n     \\,.\\]\n\\[\\left( N \\times 1 \\right)\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\n  \\left( N \\times k \\right)\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\n  \\,\\,\\,\\,\\,\\,\\,\\,\\left( k \\times k \\right)\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\n  \\left( k \\times N \\right)\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\]"
  },
  {
    "objectID": "article2_new.html#funciones-empíricas-ortogonales-feos",
    "href": "article2_new.html#funciones-empíricas-ortogonales-feos",
    "title": "Julio Sheinbaum",
    "section": "Funciones Empíricas Ortogonales (FEOs)",
    "text": "Funciones Empíricas Ortogonales (FEOs)\n\nInterpretación de los sistemas `propios’\nAntes de entrar en la teoría para las FEOs, vamos a ver que los vectores propios son equivalentes a modos de oscilación de sistemas físicos. Imaginemos oscilaciones verticales de bolas en una cuerda. Las bolas tienen masa \\(m\\) y estan separadas por cuerdas elásticas de longitud \\({\\cal d}\\) en el equilibrio. Supongamos que los desplazamientos \\(y_n\\) son tan pequeños que la tensión de la cuerda \\({\\textbf T}\\) se puede considerar constante. El ángulo de cada cuerda es \\(\\theta\\) como se ilustra en la figura. Entonces, la ecuación del movimiento para la bola \\(n\\) es\n\\[m\\frac{d^2y_n}{dt^2}=-T sen\\theta_{n-1} - T sen\\theta_{n}\\,.\\]\nBajo la suposición de que los desplazamientos son pequeños, el \\(sen\\theta_n=tan\\theta_n\\), es decir, \\(sen\\theta_{n-1}=tan\\theta_{n-1}=y_n-y_{n-1}/d\\) y \\(sen\\theta_n=tan\\theta_n=(y_n-y_{n+1})/d\\). Entonces la ecuación queda \\[m\\frac{d^2y_n}{dt^2}=-T\\frac{y_n-y_{n-1}}{d} - T\\frac{y_n-y_{n+1}}{d}\\,,\\] y si reagrupamos \\[m\\frac{d^2y_n}{dt^2}=\\frac{T}{d}(y_{n-1}-2y_n+y_{n+1})\\,.\\] Vamos ahora a substituir una solución oscilatoria del tipo \\[y_n=Y_n e^{i\\omega t}\\] en la ecuación del movimiento \\[-m\\omega^2 Y_n e^{i\\omega t}=\\frac{T}{d}(Y_{n-1} e^{i\\omega t}-2Y_n e^{i\\omega t}+Y_{n+1} e^{i\\omega t})\\] \\[\\frac{-m\\omega^2 d}{T} Y_n e^{i\\omega t}=\\frac{T}{d}(Y_{n-1} e^{i\\omega t}-2Y_n e^{i\\omega t}+Y_{n+1} e^{i\\omega t})\\,.\\] Si definimos \\[\\lambda=\\frac{m\\omega^2 d}{T}\\,,\\] el sistema de ecuaciones a resolver es \\[-\\lambda Y_n=(Y_{n-1}-2Y_n +Y_{n+1})\\] \\[Y_n(2-\\lambda)-Y_{n-1}-Y_{n+1}=0\\,,\\] con las condiciones de frontera \\(Y_0=Y_{n+1}=0\\) en las paredes.\nVamos a suponer ahora el caso de dos bolas. Para la primera bola \\(n=1\\) \\[Y_1(2-\\lambda)-Y_0-Y_2=0\\,,\\] y para la bola \\(n=2\\) \\[Y_2(2-\\lambda)-Y_1-Y_3=0\\,.\\] Si aplicamos las condiciones de frontera \\(Y_0=Y_3=0\\), nos queda uns sistema “propio”, donde \\(\\lambda\\) son los autovalores.\n\\[\\left|\\begin{array}{cc}\n  2-\\lambda & -1 \\\\\n   -1 & 2-\\lambda\\\\\n        \\end{array}\\right|=0\\,.\\]\nEl polinomio característico es\n\\[\\lambda^2-4\\lambda+3=0\\,,\\]\ny tienes las raíces \\(\\lambda_1=1\\) y \\(\\lambda_2=3\\).\nResolvamos para los vectores propios:\\\n\n\\(\\lambda_1=1\\) entonces\n\n\\[Y_1-Y_2=0\\]\ny el vector propio es\n\\[{\\textbf y}=\\frac{1}{\\sqrt{2}}\\left(\\begin{array}{c}\n   1 \\\\\n   1 \\\\\n        \\end{array}\\right)\\] \\\n\n\\(\\lambda_2=3\\) entonces\n\n\\[-Y_1-Y_2=0\\,,\\]\ny el vector propio es\n\\[{\\textbf y}=\\frac{1}{\\sqrt{2}}\\left(\\begin{array}{c}\n   1 \\\\\n   -1 \\\\\n        \\end{array}\\right)\\,.\\]\nEstas soluciones representan los modos oscilatorios de un sistema físico de dos bolas oscilando verticalmente. Los modos de oscilación se muestran en la figura. Los modos oscilan independientemente uno de otro, y la evolución del sistema es una combinación lineal de los dos modos. De esta forma, lo que estamos haciendo al resolver el problema de las bolas en una cuerda elástica es precisamente la solución de un sistema “propio”.\n\n\nDefinición de FEOs\nUn análisis en FEOs busca estructuras en los datos que explican la mayor cantidad de varianza de un conjunto de datos bidimensional. La primera dimensión es la dimension en la que deseamos encontrar una estructura, y la otra dimension es la dimensión en la que se muestrean las diferentes realizaciones. Por ejemplo, un conjunto de series temporales de datos distribuidos espacialmente (i.e. arreglo de anclajes). La primera dimensión es espacio y la segunda es tiempo. Las estructuras en la dimensión espacial son las FEOs, mientras que las estructuras en la dimensión de muestreo se denominan Componentes Principales (CPs). \\ Tanto las FEOs como las PCs son ortogonales en sus dimensiones. Las FEOs/PCs pueden entenderse de diferentes formas: \\ (i) Transforma variables correlacionadas en un conjunto de variables no correlacionadas que expresan mejor la relación dinámica entre los datos originales. \\ (ii) Identifica y ordena los vectores ortogonales (o dimensiones) a lo largo de los cuales nuestro conjunto de datos presenta la mayor varianza. \\ (iii) Una vez definidas las FEOs, es posible encontrar la mejor aproximación de los datos originales con el mínimo número de vectores ortogonales.\nEn general, aplicaremos el análisis en FEOs para describir de manera mas sencilla conjuntos de datos organizados en matrices \\(M\\times N\\):}\n\n\nUna matriz espacio-tiempo: Medidas de una variable en \\(M\\) localizaciones y \\(N\\) tiempos.\n\nUna matriz parámetro-tiempo: Medidas de \\(M\\) variables en una localización y \\(N\\) tiempos.\n\nUna matriz parámetro-espacio: Medidas de \\(M\\) variables tomadas en \\(N\\) localizaciones al mismo tiempo.\n\n\nNota: Un error común es considerar que las FEOs se corresponden con modos de físicos oceánicos. Eso no es cierto!. Los modos físicos en el océano son modos de oscilación que se obtienen considerando las ecuaciones que rigen el movimiento y condiciones de frontera; las FEOs son simplemente funciones ortogonales que explican la mayor cantidad de varianza de un conjunto de datos.\nAunque los procesos físicos dominantes son representados por los primeros modos de oscilación, no existe una correspondencia uno a uno entre modos físicos y FEOs.\n\n\nTeoría\nSupongamos M localizaciones de medición con series temporales de temperatura de N elementos. Queremos descomponer la serie temporal de temperatura en una localización dada \\(k\\) como una combinación lineal de \\(M\\) funciones ortogonales \\({\\textbf b}_i\\) cuyas amplitudes son pesadas con \\(M\\) coeficientes dependientes del tiempo, es decir,\n\\[{\\textbf T}(t)=\\sum\\limits^M_{i=1}[\\alpha_i(t){\\textbf b}_{i}]\\,,\\]\ndonde \\(\\alpha_i(t)\\) es la amplitud del modo ortogonal \\(i\\) al tiempo \\(t=t_n(1\\le n\\le N)\\). Los coeficientes \\(\\alpha_i(t)\\) nos informan de como varian los modos \\({\\textbf b}_{i}\\) con el tiempo. Necesitamos tantas funciones ortogonales como estaciones con series temporales tenemos para poder describir la varianza total de los datos originales de temperatura a cada tiempo. Sin mebargo, en términos prácticos podemos explicar una gran cantidad de varianza de los datos originales con las primeras FEOs. Podemos ver el problema al revés, es decir, tenemos \\(N\\) funciones temporales cuyas amplitudes son pesadas por \\(M\\) coeficientes que varian en el espacio. En este caso hablamos de PCs. Ya sea la reducción de los datos en funciones ortogonales espaciales (FEOs) o temporales (PCs), obtenemos los mismos resultados.\nPuesto que queremos \\({\\textbf b}_{i}\\) ser ortogonal, requerimos \\[{\\textbf b}^T_i {\\textbf b}_{j}=\\delta_{ij}\\,,\\] y los coeficientes temporales \\(\\alpha_i={\\textbf b}^T_{i}{\\textbf T}(t)\\). Son precisamente estos coeficientes temporales las CPs, es decir, la proyección de los datos originales sobre las FEOs o la expresión de los datos originales en la nueva base de vectores ortogonales (el nuevo sistema de coordenadas).\nEl objetivo del análisis es encontrar una base de vectores ortogonales tal que las funciones \\(\\alpha_i(t)\\) no esten correlacionadas \\[&lt;\\alpha_i \\alpha_j&gt;=&lt;{\\textbf b}^T_{i}{\\textbf T}{\\textbf b}^T_{j}{\\textbf T}&gt;=&lt;{\\textbf b}^T_{i}{\\textbf T}{\\textbf T}^T{\\textbf b}_{j}&gt;={\\textbf b}^T_{i}&lt;{\\textbf T} {\\textbf T}^T&gt;{\\textbf b}_{j}=\\delta_{ij}&lt;\\alpha^2_i&gt;\\,,\\] donde \\[&lt;\\alpha_i^2&gt;=\\frac{1}{N}\\sum\\limits^N_{n=1}=\\alpha^2_i(t_n)\\,.\\] Es decir, la matriz de covarianza de \\(\\alpha_i(t)\\) será una matriz diagonal \\({\\textbf D}\\). Para \\(M\\) posiciones se puede reescribir como\n\\[{\\textbf B}^T&lt;{\\textbf T}{\\textbf T}^T&gt;{\\textbf B}={\\textbf D}\\,\\,\\,\\,\\,\\,\\,\\,\\,{ o}\\,\\,\\,\\,\\,\\,\\,\\,{\\textbf B}^T{\\textbf T}{\\textbf T}^T{\\textbf B}=N{\\textbf D}\\]\ndonde \\({\\textbf B}\\) es una matriz ortogonal cuyas columnas son los vectores ortogonales \\({\\textbf b}_i\\) y \\({\\textbf D}\\) es una matriz diagonal compuesta por las varianzas de las funciones temporales \\(\\alpha_i(t)\\). Si multiplicamos por \\({\\textbf B}\\) llegamos a un sistema propio\n\\[&lt;{\\textbf T}{\\textbf T}^T&gt;{\\textbf B}={\\textbf B}{\\textbf D}\\,.\\]\nEste tipo de sistema propio es conocido. La diagonal de \\({\\textbf D}\\) esta compuesta por valores propios y las columnas de \\({\\textbf B}\\) son los vectores propios. Los vectores propios son denominados FEOs, y los valores propios son las varianzas de las amplitudes \\(\\alpha_i\\). Básicamente hemos realizado una transformación de coordenadas de tal forma que los vectores propios \\({\\textbf b}_i\\)\nindican combinaciones lineales de datos que no estan correlacionados (i.e., \\(&lt;\\alpha_i \\alpha_j&gt;=\\delta_{ij}&lt;\\alpha^2_i&gt;\\)). Esta descomposición de los datos en FEOs es óptima en el sentido de mínimos cuadrados. Imaginemos que queremos un conjunto de \\(K\\) vectores que mejor aproxima \\({\\textbf T}\\)\n\\[&lt;(\\hat{\\textbf T}-{\\textbf T})^T(\\hat{\\textbf T}-{\\textbf T})&gt;=&lt;{\\textbf T}^T{\\textbf T}&gt;\n  -\\sum\\limits^K_{i=1}&lt;\\alpha^2_i&gt;\\,.\\]\nEl problema en mínimos cuadrados, bajo las restricciones que las funciones \\({\\textbf b}_i\\) sean ortogonales, se puede escribir a partir de los multiplicadores de Lagrange\n\\[{\\textbf \\cal L}=\\sum\\limits^K_{i=1}\\left[{\\textbf b}^T_i&lt;{\\textbf T}{\\textbf T}^T&gt;{\\textbf b}_i-\n            \\lambda_i({\\textbf b}_i^T{\\textbf b}_i-1)\\right]\\,.\\]\nSi derivamos \\({\\textbf \\cal L}\\) respecto de \\({\\textbf b}_i\\) obtenemos el sistema propio\n\\[&lt;{\\textbf T}{\\textbf T}^T&gt;{\\textbf b}_i=\\lambda_i{\\textbf b}_i\\,.\\]\nDe este análisis deducimos que las primeras \\(K\\) funciones ortogonales o FEOs son las mejores funciones que explican la máxima varianza de los datos originales, donde los valores porpios estan ordenados de mayor a menor. Es decir, no existe un subset de datos mas pequeño que \\(K\\) funciones ortogonales que produce el error cuadrático medio menor. En este sentido las FEOs son los mejores `descriptores’ de la varianza de los datos.\nUna matriz de datos de \\(M\\) localizaciones y \\(N\\) tiempos se puede descomponer\n\\[{\\textbf C}{\\textbf B}={\\textbf B}{\\textbf \\Lambda}\\,,\\]\ndonde\n\\[{\\textbf C}={\\textbf T}{\\textbf T}^T=\\left(\\begin{array}{cccc}\n   \\sum\\limits_{i=1}^N{T_{1}(t_i)T_{1}(t_i)} & \\sum\\limits_{i=1}^N{T_{1}(t_i)T_{2}(t_i)} & ... & \\sum\\limits_{i=1}^N{T_{1}(t_i)T_{M}(t_i)}\\\\\n   \\sum\\limits_{i=1}^N{T_{2}(t_i)T_{1}(t_i)} & \\sum\\limits_{i=1}^N{T_{2}(t_i)T_{2}(t_i)} & ... & \\sum\\limits_{i=1}^N{T_{2}(t_i)T_{M}(t_i)}\\\\\n                . & . & . \\\\\n        . & . & . \\\\\n        . & . & . \\\\\n   \\sum\\limits_{i=1}^N{T_{M}(t_i)T_{1}(t_)} & \\sum\\limits_{i=1}^N{T_{M}(t_i)T_{2}(t_i)} & ... & \\sum\\limits_{i=1}^N{T_{M}(t_i)T_{M}(t_i)}\\\\\n        \\end{array}\\right)\\,,\\]\nes la matriz de covarianza entre las series temporales de temperatura en localizaciones espaciales, unas con otras;\n\\[{\\textbf T}=\\left(\\begin{array}{cccc}\n   T_{1}(t_1) & T_{1}(t_2) & ... & T_{1}(t_N)\\\\\n   T_{2}(t_1) & T_{2}(t_2) & ... & T_{2}(t_N)\\\\\n                . & . & . \\\\\n        . & . & . \\\\\n        . & . & . \\\\\n   T_{M}(t_1) & T_{M}(t_2) & ... & T_{M}(t_N)\\\\\n        \\end{array}\\right)\\] es la\nmatriz de datos de temperatura;\n\\[{\\textbf B}=\\left(\\begin{array}{cccc}\n   b_{11} & b_{12} & ... & b_{1M}\\\\\n   b_{21} & b_{22} & ... & b_{2M}\\\\\n                . & . & . \\\\\n        . & . & . \\\\\n        . & . & . \\\\\n   b_{M1} & b_{M2} & ... & b_{MM}\\\\\n        \\end{array}\\right)\\,,\\] es la matriz con los vectores propios \\({\\textbf b}_i\\) como columnas, y \\[{\\textbf \\Lambda}=\\left(\\begin{array}{cccc}\n   \\lambda_1 & 0 & ... & 0\\\\\n  0 & \\lambda_2 & ... &0\\\\\n                . & . & . \\\\\n        . & . & . \\\\\n        . & . & . \\\\\n   0 & 0 & ... & \\lambda_M\\\\\n        \\end{array}\\right)\\,,\\]\nes la matriz diagonal compuesta por los valores propios \\(\\lambda_i=&lt;\\alpha_i^2&gt;\\)\n\nOtra forma de entender las FEOs (PCs) consiste en un análisis en valores propios de las matrices de dispersión de nuestras matrices de datos. La matriz de dispersión es el producto matricial de la matriz con su transpuesta, o a la inversa. La primera matriz de dispersión es\n\\[{\\textbf C}={\\textbf T}{\\textbf T}^T=\\left(\\begin{array}{cccc}\n   T_{1}(t_1) & T_{1}(t_2) & ... & T_{1}(t_N)\\\\\n   T_{2}(t_1) & T_{2}(t_2) & ... & T_{2}(t_N)\\\\\n                . & . & . \\\\\n        . & . & . \\\\\n        . & . & . \\\\\n   T_{M}(t_1) & T_{M}(t_2) & ... & T_{M}(t_N)\\\\\n        \\end{array}\\right)\n\\left(\\begin{array}{cccc}\n   T_{1}(t_1) & T_{2}(t_1) & ... & T_{M}(t_1)\\\\\n   T_{1}(t_2) & T_{2}(t_2) & ... & T_{M}(t_2)\\\\\n                . & . & . \\\\\n        . & . & . \\\\\n        . & . & . \\\\\n   T_{1}(t_N) & T_{2}(t_N) & ... & T_{M}(t_N)\\\\\n        \\end{array}\\right)=\\]\n\\[=\\left(\\begin{array}{cccc}\n   \\sum\\limits_{i=1}^N{T_{1}(t_i)T_{1}(t_i)} & \\sum\\limits_{i=1}^N{T_{1}(t_i)T_{2}(t_i)} & ... & \\sum\\limits_{i=1}^N{T_{1}(t_i)T_{M}(t_i)}\\\\\n   \\sum\\limits_{i=1}^N{T_{2}(t_i)T_{1}(t_i)} & \\sum\\limits_{i=1}^N{T_{2}(t_i)T_{2}(t_i)} & ... & \\sum\\limits_{i=1}^N{T_{2}(t_i)T_{M}(t_i)}\\\\\n                . & . & . \\\\\n        . & . & . \\\\\n        . & . & . \\\\\n   \\sum\\limits_{i=1}^N{T_{M}(t_i)T_{1}(t_i)} & \\sum\\limits_{i=1}^N{T_{M}(t_i)T_{2}(t_i)} & ... & \\sum\\limits_{i=1}^N{T_{M}(t_i)T_{M}(t_i)}\\\\\n        \\end{array}\\right)\\,,\\]\n\\[\\left( M \\times M \\right)\\]\ny la segunda es\n\\[{\\textbf C}={\\textbf T}^T{\\textbf T}=\n\\left(\\begin{array}{cccc}\n   T_{1}(t_1) & T_{2}(t_1) & ... & T_{M}(t_1)\\\\\n   T_{1}(t_2) & T_{2}(t_2) & ... & T_{M}(t_2)\\\\\n                . & . & . \\\\\n        . & . & . \\\\\n        . & . & . \\\\\n   T_{1}(t_N) & T_{2}(t_N) & ... & T_{M}(t_N)\\\\\n        \\end{array}\\right)\\left(\\begin{array}{cccc}\n   T_{1}(t_1) & T_{1}(t_2) & ... & T_{1}(t_N)\\\\\n   T_{2}(t_1) & T_{2}(t_2) & ... & T_{2}(t_N)\\\\\n                . & . & . \\\\\n        . & . & . \\\\\n        . & . & . \\\\\n   T_{M}(t_1) & T_{M}(t_2) & ... & T_{M}(t_N)\\\\\n\\end{array}\\right)=\\]\n\\[=\\left(\\begin{array}{cccc}\n   \\sum\\limits_{i=1}^M{T_{i}(t_1)T_{i}(t_1)} & \\sum\\limits_{i=1}^M{T_{i}(t_1)T_{i}(t_2)} & ... & \\sum\\limits_{i=1}^i{T_{1}(t_1)T_{i}(t_N)}\\\\\n   \\sum\\limits_{i=1}^M{T_{i}(t_2)T_{i}(t_1)} & \\sum\\limits_{i=1}^M{T_{i}(t_2)T_{i}(t_2)} & ... & \\sum\\limits_{i=1}^M{T_{i}(t_2)T_{i}(t_N)}\\\\\n                . & . & . \\\\\n        . & . & . \\\\\n        . & . & . \\\\\n   \\sum\\limits_{i=1}^M{T_{i}(t_N)T_{i}(t_1)} & \\sum\\limits_{i=1}^M{T_{i}(t_N)T_{i}(t_2)} & ... & \\sum\\limits_{i=1}^M{T_{i}(t_N)T_{i}(t_N)}\\\\\n        \\end{array}\\right)\\,.\\] \\[\\left( N \\times N \\right)\\]\nAmbas matrices de dispersión obtenidas del producto de la matriz de datos consigo misma son matrices de covarianza simétricas. La primera matriz de dispersión \\({\\textbf T}{\\textbf T}^T\\) es una matriz \\(M \\times M\\) con lo que hemos eliminado la dimensión temporal o dimensión de muestreo. En este caso, la matriz de dispersión es una matriz de covarianzas temporales (determinadas por sus variaciones temporales) de las estaciones unas con otras. En la segunda matriz \\({\\textbf T}^T{\\textbf T}\\) se invierten las dimensiones, la matriz resultante es \\(N\\times N\\), y es una matriz de covarianzas espaciales (determinadas por sus variaciones espaciales) entre los diferenetes tiempos.\n\nEjemplo: Un ejmplo sencillo viene dado por el diagrama de dispersión de la figura. La primera EOF o vector propio que explica la mayor varianza sería la recta que se ajusta al conjunto de puntos y la segunda EOF sería la línea perpendicular a la ajustada.\nRelación entre FEOs y CPs\nComo ya hemos visto nuestro sistema propio es\n\\[{\\textbf B}^T&lt;{\\textbf T}{\\textbf T}^T&gt;{\\textbf B}={\\textbf D}\\,\\,\\,\\,\\,\\,\\,\\,\\,{ o}\\,\\,\\,\\,\\,\\,\\,\\,{\\textbf B}^T{\\textbf T}{\\textbf T}^T{\\textbf B}=N{\\textbf D}\\,,\\]\ndonde \\({\\textbf B}\\) es una matriz cuyas columnas son los vectores propios y \\({\\textbf D}\\) es una matriz cuadrada con los \\(M\\) valores propios en la diagonal principal. Si queremos expresar los datos originales en términos de los vectores propios, entonces debemos usar la definición\n\\[\\alpha_i(t)={\\textbf b}^T_{i}{\\textbf T}(t)\\,,\\]\nque en notación matricial se puede expresar como\n\\[{\\textbf Z}={\\textbf B}^T{\\textbf T}\\,,\\]\ny finalmente para recuperar los datos originales a partir de la base de FEOs usamos\n\\[{\\textbf T}={\\textbf B}{\\textbf Z}\\,,\\]\nya que \\({\\textbf B}{\\textbf B}^T={\\textbf I}\\). La matriz \\({\\textbf Z}\\) contiene los vectores de las CPs, que no son mas que las amplitudes por las cuales multiplicamos las FEOs para obtener los datos originales de vuelta. De esta forma podemos ir de un espacio (vectores propios) al otro (datos originales) con la matriz de FEOs. Supongamos que tenemos un conjunto de vectores propios ortogonales y normalizados. El primero de ellos por ejemplo sería\n\\[{\\textbf e}=\\left(\\begin{array}{c}\n   e_{11} \\\\\n   e_{21} \\\\\n         .\\\\\n         .\\\\\n     .\\\\\n   e_{M1} \\\\\n        \\end{array}\\right)\\,.\\]\nSi ponemos todos los vectores propios en columna obtenemos la matriz cuadrada \\({\\textbf B}\\), la cual es ortonormal \\({\\textbf B}^T{\\textbf B}={\\textbf I}\\). Si queremos proyectar un vector propio sobre los datos originales y obtener la amplitud de este vector propio en cada tiempo, debemos de hacer\n\\[{\\textbf e}^T{\\textbf T}=\\left(\\begin{array}{ccccccc}\n   e_{11} &\n   e_{21} & . & . & . & e_{M1} \\\\\n        \\end{array}\\right)\n    \\left(\\begin{array}{cccc}\n   T_{1}(t_1) & T_{1}(t_2) & ... & T_{1}(t_N)\\\\\n   T_{2}(t_1) & T_{2}(t_2) & ... & T_{2}(t_N)\\\\\n                . & . & . \\\\\n        . & . & . \\\\\n        . & . & . \\\\\n   T_{M}(t_1) & T_{M}(t_2) & ... & T_{M}(t_N)\\\\\n        \\end{array}\\right)=\n    \\left(\\begin{array}{ccccccc}\n   z_{11} &\n   z_{12} & . & . & . & z_{1N} \\\\\n        \\end{array}\\right)\\,,\\]\ndonde, por ejemplo, \\(z_{11}=e_{11}T_1(t_1)+ e_{21}T_2(t_1) + ... + e_{M1}T_M(t_1)\\,.\\) Si hacemos lo mismo para todos los otros vectores propios, obtenemos series temporales de longitud \\(N\\) para cada FEO, lo cual se denominan componentes principales de cada EOF\n\\[{\\textbf Z}={\\textbf B}^T{\\textbf T}\\,.\\] \\[M\\times N\\]\nLas CPs también son ortogonales. Si substituimos \\({\\textbf T}={\\textbf B}{\\textbf Z}\\), en \\({\\textbf B}^T{\\textbf T}{\\textbf T}^T{\\textbf B}=N{\\textbf D}\\) obtenemos\n\\[{\\textbf B}^T{\\textbf B}{\\textbf Z}({\\textbf B}{\\textbf Z})^T{\\textbf B}=N{\\textbf D}\\]\n\\[{\\textbf I}{\\textbf Z}{\\textbf Z}^T{\\textbf B}^T{\\textbf B}=\nN{\\textbf D}\\]\n\\[{\\textbf I}{\\textbf Z}{\\textbf Z}^T{\\textbf I}=N{\\textbf D}\\]\n\\[{\\textbf Z}{\\textbf Z}^T=N{\\textbf D}\\,\\,\\,\\,\\,\\,{ o}\\,\\,\\,\\,\\,\\,&lt;{\\textbf Z}{\\textbf Z}^T&gt;={\\textbf D}\\,.\\]\nPor lo tanto no solo las FEOs sino también las CPs son ortogonales.\nEquivalencia con descomposición en valores singulares (SVD)\nSupongamos la matriz de datos compuesta por \\(M\\) series temporales de temperatura de longitud \\(N\\) \\[{\\textbf T}=\\left(\\begin{array}{cccc}\n   T_{1}(t_1) & T_{1}(t_2) & ... & T_{1}(t_N)\\\\\n   T_{2}(t_1) & T_{2}(t_2) & ... & T_{2}(t_N)\\\\\n                . & . & . \\\\\n        . & . & . \\\\\n        . & . & . \\\\\n   T_{M}(t_1) & T_{M}(t_2) & ... & T_{M}(t_N)\\\\\n        \\end{array}\\right)\\,.\\]\nLa matriz de dispersión es \\[&lt;{\\textbf T}{\\textbf T}^T&gt;=\\frac{1}{N} {\\textbf T}{\\textbf T}^T\\,.\\]\nSabemos que la matriz \\({\\textbf T}\\) puede descomponerse en una matriz ortogonal \\({\\textbf U}\\), una matriz diagonal \\({\\textbf S}\\), y la transpuesta de una matriz ortogonal \\({\\textbf V}\\). Esto es \\[{\\textbf T}={\\textbf U}{\\textbf S}{\\textbf V}^T\\,,\\] donde el número de valores singulares no nulos indican el rango de \\({\\textbf T}\\). Si \\(K&lt;N\\) y el número de filas (i.e., los datos) son linealmente independientes entonces el rango será \\(K\\). Ahora la matriz de covarianza es\n\\[\\frac{1}{N} {\\textbf T}{\\textbf T}^T = \\frac{1}{N} ({\\textbf U}{\\textbf S}{\\textbf V}^T)({\\textbf U}{\\textbf S}{\\textbf V}^T)^T=\n\\frac{1}{N}{\\textbf U}{\\textbf S}{\\textbf V}^T{\\textbf V}({\\textbf U}{\\textbf S})^T=\\frac{1}{N} {\\textbf U}{\\textbf S}{\\textbf V}^T{\\textbf V}{\\textbf S}^T{\\textbf U}^T=\\frac{1}{N} {\\textbf U}{\\textbf S}{\\textbf S}^T{\\textbf U}^T\\,.\\]\nLa derecha del igual es la descomposición en valores propios de la matriz de covarianza, donde \\({\\textbf S}{\\textbf S}^T\\) es cuadrada y diagonal con los elementos igual a \\(N\\lambda_i\\); y las columnas de \\({\\textbf U}\\) son las FEOs. Las amplitudes de las FEOs vienen dadas por las filas de la matriz\n\\[{\\textbf U}^T{\\textbf T}={\\textbf S}{\\textbf V}\\,,\\]\nasociado con valores singulares no nulos.\nInterpretación de las FEOs\nComo comentario final hay que decir que las FEOs no son muy fáciles de interpretar. Matemáticamente son estructuras que representan la mayor cantidad de varianza de los datos originales y que son ortogonales entre ellas. En ocasiones estas estructuras nos dan estructuras con sentido físico en un conjunto de datos, en otras no. Las estructuras particulares encontradas dependerán de cómo hemos acomodado nuestra matriz bidimensional de datos. Algunas sugerencias para detectar si las FEOs tienen sentido físico son las siguientes:\n\n\n¿La varianza de tu FEO es mas grande que lo que esperabas si los datos originales no tenían estructura alguna?\n\n¿Existe una explicación apriori para las estructuras que has encontrado? ?`Se pueden explicar las estructuras en términos de alguna teoría? ¿Las estructuras se comportan consistentemente con la teoría apriori?\n\n¿Cuán robustas son las estructuras a la elección del dominio de la estructura? ¿Si cambias el dominio del análisis, esas estructuras cambian significantemente? ¿Si las estructuras estan definidas en un espacio geográfico, y cambias el tamaño de la región, las estructuras cambian significativamente? ¿Si las estructuras estan definidas en el espacio de parámetros y añades o eliminas un parámetros, los resultados cambian de forma suave o aleatoriamente?\n\n\n(4)¿Cuán robustas son las estructuras a los datos usados? ¿Si divides los datos originales en fracciones menores y haces el analisis de cada fracción, obtienes las mismas estructuras?\nEjemplo de descomposición en valores singulares\n  Supongamos la matriz\n\\[{\\textbf A}=\\left(\\begin{array}{ccc}\n  3 & 1 & 1\\\\\n  -1 & 3 & 1\\\\\n      \\end{array}\\right)\\,.\\]\nPara encontrar \\({\\textbf U}\\) debemos resolver el problema en vectores y valores propios de la matriz \\({\\textbf A}{\\textbf A}^T\\)\n\\[{\\textbf A}{\\textbf A}^T=\\left(\\begin{array}{ccc}\n  3 & 1 & 1\\\\\n  -1 & 3 & 1\\\\\n      \\end{array}\\right)\n      \\left(\\begin{array}{ccc}\n  3 & -1 \\\\\n  1 & 3 \\\\\n  1 & 1 \\\\\n      \\end{array}\\right)=\\left(\\begin{array}{cc}\n  11 & 1 \\\\\n  1 & 11 \\\\\n      \\end{array}\\right)\\]\nEl sistema de ecuaciones `propio’ es\n\\[\\left(\\begin{array}{cc}\n  11 & 1 \\\\\n  1 & 11 \\\\\n      \\end{array}\\right)\\left(\\begin{array}{cc}\n  x_1 \\\\\n  x_2 \\\\\n      \\end{array}\\right)=\\lambda\\left(\\begin{array}{cc}\n  x_1 \\\\\n  x_2 \\\\\n      \\end{array}\\right)\\]\nSi resolvemos para \\(\\lambda\\)\n\\[\\left|\\begin{array}{cc}\n  11-\\lambda & 1 \\\\\n  1 & 11-\\lambda \\\\\n      \\end{array}\\right|=0\\,,\\]\nlo que deja el polinomio característico\n\\[(\\lambda-10)(\\lambda-12)=0\\,,\\]\ncon raices (valores propios) \\(\\lambda_1=12\\) y \\(\\lambda_2=10\\).\nSi sustituimos en el sistema de ecuaciones `propio’ el primer valor propio \\(\\lambda_1=12\\) \\[(11-12)x_1 + x_2=0\\]\n\\[x_1=x_2\\]\nPara \\(x_1=1\\) obtenemos que \\(x_2=1\\). Entonces obtenemos el vector propio \\({\\textbf v}_1=[1,1]\\)“”\nSi sustituimos en el sistema de ecuaciones “propio” el primer valor propio \\(\\lambda_1=10\\) \\[(11-10)x_1 + x_2=0\\]\n\\[x_1=-x_2\\] Para \\(x_1=1\\)\nobtenemos que \\(x_2=-1\\). Entonces obtenemos el vector propio \\({\\textbf v}_2=[1,-1]\\).\nSi organizamos la matriz con columnas correspondientes a los vectores propios asociados a los valores propios de mayor a menor, obtenemos\n\\[\\left(\\begin{array}{cc}\n  1 & 1 \\\\\n  1 & -1 \\\\\n      \\end{array}\\right)\\]\nFinalmente, sabemos que \\({\\textbf U}\\) tiene que ser ortonormal. Vamos a usar el proceso de Gram-Schmidt para ortonormalizar las columnas de \\({\\textbf U}\\). Empezamos normalizando la primera columna\n\\[{\\textbf u}_1=\\frac{{\\textbf v}_1}{|{\\textbf v}_1|}=\\frac{[1,1]}{\\sqrt{2}}=\\left[ \\frac{1}{\\sqrt{2}}, \\frac{1}{\\sqrt{2}} \\right]\\,,\\]\ny calculamos el vector ortonormal como\n\\[{\\textbf w}_2={\\textbf v}_2-{\\textbf u}_1\\cdot{\\textbf v}_2{\\textbf u}_1=[1,-1]-\\left[ \\frac{1}{\\sqrt{2}}, \\frac{1}{\\sqrt{2}} \\right]\n\\cdot[1,-1]\\left[ \\frac{1}{\\sqrt{2}}, \\frac{1}{\\sqrt{2}} \\right]=[1,-1]-[0,0]=[1,-1]\\,.\\]\nSi lo normalizamos\n\\[{\\textbf u}_2=\\frac{{\\textbf w}_2}{|{\\textbf w}_2|}=\\left[ \\frac{1}{\\sqrt{2}}, \\frac{-1}{\\sqrt{2}} \\right]\\,,\\]\ndejando la matriz\n\\[{\\textbf U}=\\left(\\begin{array}{cc}\n  \\frac{1}{\\sqrt{2}} & \\frac{1}{\\sqrt{2}} \\\\\n  \\frac{1}{\\sqrt{2}} & -\\frac{1}{\\sqrt{2}} \\\\\n\\end{array}\\right)\\,.\\]\nSimilarmente, para calcular la matriz \\({\\textbf V}\\) debemos resolver el problema en vectores y valores propios de la matriz \\({\\textbf A}^T{\\textbf A}\\)\n\\[{\\textbf A}^T{\\textbf A}=\\left(\\begin{array}{ccc}\n  3 & -1 \\\\\n  1 & 3 \\\\\n  1 & 1 \\\\\n      \\end{array}\\right)\\left(\\begin{array}{ccc}\n  3 & 1 & 1\\\\\n  -1 & 3 & 1\\\\\n      \\end{array}\\right)=\n      \\left(\\begin{array}{ccc}\n  10 & 0 & 2 \\\\\n  0 & 10 & 4\\\\\n  2 & 4 & 2\\\\\n      \\end{array}\\right)\n      \\]\nEl sistema de ecuaciones `propio’ es\n\\[\\left(\\begin{array}{ccc}\n  10 & 0 & 2 \\\\\n  0 & 10 & 4\\\\\n  2 & 4 & 2\\\\\n      \\end{array}\\right)\\left(\\begin{array}{c}\n  x_1 \\\\\n  x_2 \\\\\n  x_3 \\\\\n      \\end{array}\\right)=\\lambda \\left(\\begin{array}{c}\n  x_1 \\\\\n  x_2 \\\\\n  x_3 \\\\\n      \\end{array}\\right)\\]\n\\[(10-\\lambda)x_1 + 2x_3=0\\] \\[(10-\\lambda)x_2 + 4x_3=0\\] \\[2x_1 + 4x_2 + (2-\\lambda)x_3=0\\]\nSi resolvemos para \\(\\lambda\\)\n\\[\\left|\\begin{array}{ccc}\n  10-\\lambda & 0 & 2 \\\\\n  0 & 10-\\lambda & 4\\\\\n  2 & 4 & 2-\\lambda\\\\\n      \\end{array}\\right|=0\\]\nlo que deja el polinomio característico\n\\[\\lambda (\\lambda-10)(\\lambda-12)=0\\,,\\]\ncon raices (valores propios) \\(\\lambda_1=12\\), \\(\\lambda_2=10\\), y \\(\\lambda_3=0\\). Substituyendo en el sistema `propio’ para \\(\\lambda_1=12\\)\n\\[(10-12)x_1 + 2x_3 = -2x_1+2x_3=0\\] \\[x_1=1;\\,\\,x_3=1\\]\n\\[(10-12)x_2 + 4x_3 = -2x_2+4x_3=0\\] \\[x_2=2x_3\\]\n\\[x_2=2\\] Entonces, para \\(\\lambda_1=12\\) obtenemos el vector propio \\({\\textbf v}_1=[1,2,1]\\).\n\nPara \\(\\lambda_2=10\\)\n\\[(10-10)x_1 + 2x_3=2x_3=0\\] \\[x_3=0\\]\n\\[2x_1+4x_2=0\\] \\[x_1=-2x_2\\]\n\\[x_1=2;\\,\\,x_2=-1\\]\ny obtenemos \\({\\textbf v}_2=[2,-1,0]\\) para \\(\\lambda_2=10\\). \\ Finalmente, para \\(\\lambda_3=0\\)\n\\[10x_1+2x_3=0\\] \\[x_3=-5\\] \\[10x_1-20=0\\]\n\\[x_2=2\\] \\[2x_1+8-10=0\\]\n\\[x_1=1\\]\nlo que implica que para \\(\\lambda_3=0\\) \\({\\textbf v}_3=[1,2,-5]\\).\n\nSi organizamos los vectores de acuerdo con el valor de los valores propios (de mayor a menor) obtenemos la matriz\n\\[\\left(\\begin{array}{ccc}\n  1 & 2 & 1 \\\\\n  2 & -1 & 2\\\\\n  1 & 0 & -5\\\\\n      \\end{array}\\right)\\]\nAhora vamos a ortonormalizarla con el proceso de Gram-schmidt\n\\[{\\textbf u}_1=\\frac{{\\textbf v}_1}{|{\\textbf v}_1|}=\\frac{[1,2,1]}{\\sqrt{6}}\\,,\\] y calculamos el vector ortonormal como\n\\[{\\textbf w}_2={\\textbf v}_2-{\\textbf u}_1\\cdot{\\textbf v}_2{\\textbf u}_1=[2,-1,0]\\]\nSi lo normalizamos\n\\[{\\textbf u}_2=\\frac{{\\textbf w}_2}{|{\\textbf w}_2|}=\\left[ \\frac{2}{\\sqrt{5}}, \\frac{-1}{\\sqrt{5}} , 0 \\right]\\]\nEl último vector ortonormal a calcular es\n\\[{\\textbf w}_3={\\textbf v}_3-{\\textbf u}_1\\cdot{\\textbf v}_3{\\textbf u}_1 - {\\textbf u}_2\\cdot{\\textbf v}_3{\\textbf u}_2=[\\frac{-2}{3},\\frac{-4}{3},\\frac{10}{3}]\\]\nSi lo normalizamos\n\\[{\\textbf u}_3=\\frac{{\\textbf w}_3}{|{\\textbf w}_3|}=\\left[ \\frac{1}{\\sqrt{30}}, \\frac{2}{\\sqrt{30}} , \\frac{-5}{\\sqrt{30}} \\right]\\]\ndejando la matriz\n\\[{\\textbf V}=\\left(\\begin{array}{ccc}\n  \\frac{1}{\\sqrt{6}} & \\frac{2}{\\sqrt{5}} & \\frac{1}{\\sqrt{30}}\\\\\n  \\frac{2}{\\sqrt{6}} & -\\frac{1}{\\sqrt{5}} & \\frac{2}{\\sqrt{30}}\\\\\n  \\frac{1}{\\sqrt{6}} & 0 & \\frac{-5}{\\sqrt{30}}\n      \\end{array}\\right)\\,.\\]\nFinalmente, lo que realmente queremos es\n\\[{\\textbf V}^T=\\left(\\begin{array}{ccc}\n  \\frac{1}{\\sqrt{6}} & \\frac{2}{\\sqrt{6}} & \\frac{1}{\\sqrt{6}}\\\\\n  \\frac{2}{\\sqrt{5}} & -\\frac{1}{\\sqrt{5}} & 0\\\\\n  \\frac{1}{\\sqrt{30}} & \\frac{2}{\\sqrt{30}} & \\frac{-5}{\\sqrt{30}}\\\\\n      \\end{array}\\right)\\,.\\]\nPara calcular \\({\\textbf S}\\) debemos tomar las raices cuadradas de los valores propios diferentes de cero (\\(\\lambda_i\\ne0\\)) y colocarlos en la diagonal principal en orden descendente. Es decir, el valor propio mayor en la posición \\(s_{11}\\), el siguiente mas grande en \\(s_{22}\\), y así sucesivamente. Los valores propios diferentes de cero son iguales para \\({\\textbf U}\\) y \\({\\textbf V}\\) con lo que no importa de cual los tomemos. Puesto que solo hay dos valores propios diferentes de cero y el orden de las matrices \\({\\textbf U}\\) y \\({\\textbf V}\\) es \\(3\\times3\\), debemos añadir una columna de ceros a \\({\\textbf S}\\)\n\\[{\\textbf S}=\\left(\\begin{array}{ccc}\n  \\sqrt{12} & 0 & 0 \\\\\n  0 & \\sqrt{10} & 0\\\\\n      \\end{array}\\right)\\,.\\]\nAhora ya tenemos todas las matrices de la descomposición en valores singulares:\n\\[{\\textbf A}={\\textbf U}{\\textbf S}{\\textbf V}^T=\\left(\\begin{array}{cc}\n  \\frac{1}{\\sqrt{2}} & \\frac{1}{\\sqrt{2}} \\\\\n  \\frac{1}{\\sqrt{2}} & -\\frac{1}{\\sqrt{2}} \\\\\n      \\end{array}\\right)\n      \\left(\\begin{array}{ccc}\n  \\sqrt{12} & 0 & 0 \\\\\n  0 & \\sqrt{10} & 0\\\\\n      \\end{array}\\right)\n      \\left(\\begin{array}{ccc}\n  \\frac{1}{\\sqrt{6}} & \\frac{2}{\\sqrt{5}} & \\frac{1}{\\sqrt{30}}\\\\\n  \\frac{2}{\\sqrt{6}} & -\\frac{1}{\\sqrt{5}} & \\frac{2}{\\sqrt{30}}\\\\\n  \\frac{1}{\\sqrt{6}} & 0 & \\frac{-5}{\\sqrt{30}}\n      \\end{array}\\right)=\\]\n\\[=\\left(\\begin{array}{ccc}\n  \\frac{12}{\\sqrt{2}} & \\frac{\\sqrt{10}}{\\sqrt{2}} & 0\\\\\n  \\frac{12}{\\sqrt{2}} & -\\frac{\\sqrt{10}}{\\sqrt{2}} & 0\\\\\n      \\end{array}\\right)\n      \\left(\\begin{array}{ccc}\n  \\frac{1}{\\sqrt{6}} & \\frac{2}{\\sqrt{5}} & \\frac{1}{\\sqrt{30}}\\\\\n  \\frac{2}{\\sqrt{6}} & -\\frac{1}{\\sqrt{5}} & \\frac{2}{\\sqrt{30}}\\\\\n  \\frac{1}{\\sqrt{6}} & 0 & \\frac{-5}{\\sqrt{30}}\n      \\end{array}\\right)=\n      \\left(\\begin{array}{ccc}\n  3 & 1 & 1 \\\\\n  -1 & 3 & 1\\\\\n      \\end{array}\\right)\\,.\\]"
  },
  {
    "objectID": "article2_new.html#análisis-espectral-o-análisis-de-fourier",
    "href": "article2_new.html#análisis-espectral-o-análisis-de-fourier",
    "title": "Julio Sheinbaum",
    "section": "Análisis espectral o análisis de Fourier",
    "text": "Análisis espectral o análisis de Fourier\n\nIntroducción\nUna función periódica es aquella cuyos valores se repiten a intervalos regulares. El tiempo entre las sucesivas repeticiones se denomina periodo \\(\\tau\\). Normalmente lo definimos entre sucesivas crestas. Matemáticamente, una función es periódica si \\(f(t)=f(t+T)\\) para todo valor de \\(T\\).\nLa frecuencia de una función periódica se define como el inverso del periodo, \\(f=1/\\tau\\), es decir el número de ciclos por unidad de tiempo (si es por segundo hablamos de Hercios, Hz). Si un ciclo equivale a \\(2\\pi\\) radianes, entonces el número de radianes por segundo es lo que se concoce por la frecuencia angular fundamental: \\[\\omega=\\frac{2\\pi}{T}\\,.\\]\nLas funciones periódicas también se pueden definir en el espacio. Entonces el periodo se define como \\[\\tau=\\lambda/v_p\\,,\\] donde \\(\\lambda\\) es la longitud de onda y \\(v_p=\\lambda/\\tau=\\omega/k\\) es la velocidad de fase. La longitud de onda es una distancia entre estados de la onda que se repiten, e.j. entre dos crestas. El número de onda \\(k\\) es el número de ondas contenidas en una unidad de distancia \\[k=\\frac{2\\pi}{\\lambda}=\\frac{\\omega}{v_p}\\]\nEl valor promedio de una función periódica es: \\[f_m=\\frac{1}{\\tau}\\int\\limits^{\\tau}_0 f(t) dt\\,,\\] y su valor cuadrático medio (o RMS, en inglés) es: \\[f_{rms}=\\sqrt{\\frac{1}{\\tau}\\int\\limits^{\\tau}_0 f^2(t) dt}\\,,\\] donde las integrales se han definido en el intervalo 0,\\(\\tau\\), aunque se pueden definir en cualquier intervalo que abarque un periodo, e.j. de -\\(\\tau\\)/2 a \\(\\tau\\)/2.\nUna de las ondas periódicas mas utilizadas es la sinusoidal o cosenosoidal. \\[f(t)=A sen(\\omega t + \\theta)\\,,\\] siendo \\(A\\) la amplitud y \\(\\theta\\) su fase inicial. En este caso el valor medio es cero y su rms es \\(A/\\sqrt(2)\\). % Recordar que existen dos frecuencias básicas: (i) Frecuencia de Nyquist \\(f_N=1/(2\\Delta t)\\) (la frecuencia mas alta que podemos resolver) y (ii) Frecuencia fundamental \\(f_0=1/(\\Delta t N)=1/T\\) (la frecuencia mas baja que podemos resolver).\n\n\nSerie de Fourier\nEl principio básico del análisis de Fourier es que cualquier función periódica \\(f(t)\\) definida en el intervalo \\([0,T]\\) se puede descomponer en suma de funciones simples, sinusoidales y cosinusoidales, o series de Fourier de la forma \\[f(t)=\\bar{f(t)} + \\sum\\limits_p [A_p cos(\\omega_p t) + B_p sin (\\omega_p t)]\\,,\\] donde \\(\\overline{f(t)}\\) es el valor medio de la serie temporal, \\(A_p\\) y \\(B_p\\) son constantes denominados coeficientes de Fourier, y \\(\\omega_p=2 \\pi p f_0=2\\pi p/\\tau\\) es múltiplo de la frecuencia angular fundamental.\nSi tenemos suficientes componentes de Fourier cada valor de la serie original se puede reconstruir. La contribución que cada componente tiene sobre la varianza de la serie temporal es una medida de la importancia de una frecuencia particular en la serie original. El punto clave aqui es que el conjunto de coeficientes de Fourier con amplitudes \\(A_p\\) y \\(B_p\\) forman un espectro el cual define la contribución de cada componente oscilatoria \\(\\omega_p\\) sobre la `energía’ total de la señal original. En concreto, el spectro de potencia (Power spectrum) define la energía por unidad de banda de frecuencia de una serie temporal. Puesto que debemos definir dos amplitudes \\(A_p\\) y \\(B_p\\), hay dos grados de libertad por estimación espectral.\nComo ya hemos dicho el primer armónico (\\(p=1\\)) oscila con frecuencia fundamental \\(\\omega_1=2\\pi f_1\\). El armónico \\(N/2\\), el cual nos da la componente con la frecuencia más alta que puede ser resuelta tiene frecuencia \\(f_N=N/2/N\\Delta t=1/(2\\Delta T)\\) ciclos por unidad de tiempo y un periodo de \\(2\\Delta t\\). Esta es la frecuencia de Nyquist.\nLas series de Fourier se definen como \\[f(t)=\\frac{1}{2}A_0 +\\sum\\limits^\\infty_{p=1}[A_p cos(\\omega_p t) + B_p sen(\\omega_p t)]\\,,\\] en la cual \\[\\omega_p=2\\pi f_p=2\\pi p f_1=2\\pi p /T;\\,\\,\\,\\,p=1,2,...\\,,\\] es la frecuencia de la componente \\(p\\)ésima en radianes por unidad de tiempo (\\(f_p\\) es en ciclos por unidad de tiempo) y \\(A_0/2\\) es la media de la serie temporal.\nPara obtener los coeficientes \\(A_p\\), debemos multiplicar la expresión de la descomposición de Fourier por \\(cos(\\omega_r t)\\) e integrar sobre la serie completa. \\[\\int\\limits^{T}_{0}f(t)cos(\\omega_r t)dt=\\frac{1}{2}A_0\\int\\limits^{T}_{0} cos(\\omega_r t) dt + \\] \\[+\\sum\\limits^\\infty_{p=1} A_p \\int\\limits^{T}_{0}cos(\\omega_p t)cos(\\omega_r t) dt +\\] \\[+\\sum\\limits^\\infty_{p=1} B_p \\int\\limits^{T}_{0}sin(\\omega_p t)cos(\\omega_r t) dt\\,.\\]\nSi usamos las siguientes condiciones de ortogonalidad: \\[\\int\\limits^{T}_{0}sin(\\omega_p t)cos(\\omega_r t) dt=0\\] \\[\\int\\limits^{T}_{0}cos(\\omega_p t)cos(\\omega_r t) dt=\n\\left\\lbrace\n  \\begin{array}{l}\n     T, p=r=0 \\\\\n     T/2,p=r&gt;0 \\\\\n     0, p\\ne r \\\\\n  \\end{array}\n  \\right.\\] \\[\\int\\limits^{T}_{0}sin(\\omega_p t)sin(\\omega_r t) dt=\n\\left\\lbrace\n  \\begin{array}{l}\n     0, p=r=0 \\\\\n     T/2,p=r&gt;0 \\\\\n     0, p\\ne r \\\\\n  \\end{array}\n  \\right.\\] entonces encontramos que para \\(r=0;p\\ne r\\) la ecuación de arriba se reduce a \\[\\int\\limits^{T}_{0}f(t)dt=\\frac{A_0}{2}T\\,,\\] es decir, \\[A_0=\\frac{2}{T}\\int\\limits^{T}_{0}f(t)dt=2\\overline{f(t)}\\,,\\] dos veces el valor medio de la serie \\(f(t)\\). Es por ello que se añade el factor de \\(1/2\\) en la serie de Fourier. Es decir, para que el primer término de la serie de Fourier sea igual a la media de la serie temporal \\(\\overline{f(t)}=1/2A_0\\).\nCuando \\(p\\ne0\\) el único término no despreciable de la derecha de la expresión de arriba sucede cuando \\(r=p\\) \\[\\int\\limits^{T}_{0}f(t)cos(\\omega_p t)dt=\\frac{A_p}{2}T\\,,\\] y entonces \\[A_p=\\frac{2}{T}\\int\\limits_0^{T} f(t) cos(\\omega_p t) dt,\\,\\,\\,\\,p=1,2,...\\]\nLos otros coeficientes \\(B_p\\) son obtenidos igualmente multiplicando por \\(sen(\\omega_r t)\\) en lugar de \\(cos(\\omega_r t)\\) \\[B_p=\\frac{2}{T}\\int\\limits_0^{T} f(t) sen(\\omega_p t) dt,\\,\\,\\,\\,p=1,2,...\\,.\\]\nTambién podemos representar la serie de Fourier en notación compacta como: \\[f(t)=\\frac{1}{2}C_0 +\\sum\\limits^\\infty_{p=1}C_p cos(\\omega_p t - \\theta_p)\\,,\\] en la cual la amplitud de la \\(p\\)ésima componente es \\[C_p=\\sqrt{A_p^2+B_p^2}\\,,\\,\\,\\,\\,p=1,2,....\\] donde \\(C_0=A_0 (B_0=0)\\) es dos veces el valor promedio de la serie y \\[\\theta_p=arctg[B_p/A_p]\\,,\\,\\,\\,\\,p=1,2,...\\] es el ángulo de fase de la componente al tiempo \\(t=0\\). El ángulo de fase nos informa del “desfase” lag relativo de las componente en radianes (o grados) medido en el sentido contrario a las agujas del reloj desde el eje real definido por \\(B_p=0, A_p&gt;0\\). El correspondiente tiempo de desfase para la componente \\(p\\)ésima es \\(t_p=\\theta_p/2\\pi f_p\\) en el cual \\(\\theta_p\\) esta medida en radianes. La energía espectral se define como las amplitudes de los coeficientes de Fourier al cuadrado, lo cual representa la varianza y entonces la energía \\[C^2_p=A_p^2 + B_p^2\\,.\\] \\ \\ De igual forma con las relaciones trigonométricas de arriba se puede expresar las series de Fourier en notación compleja. Usando \\[sen\\omega_p t=\\frac{e^{i\\omega_pt}-e^{-i\\omega_pt}}{2i}\\,\\,\\,\\,\\,\\,\\,y\\,\\,\\,\\,\\,\\,\\,\ncos\\omega_p t=\\frac{e^{i\\omega_pt}+e^{-i\\omega_pt}}{2}\\,,\\] obtenemos \\[f(x)=\\frac{1}{2}A_0 +\\sum\\limits^\\infty_{p=1}[A_p cos(\\omega_p t) + B_p sen(\\omega_p t)]=\\] \\[=\\frac{1}{2}A_0 +\\sum\\limits^\\infty_{p=1}\\left[ A_p \\frac{e^{i\\omega_pt}+e^{-i\\omega_pt}}{2} + B_p \\frac{e^{i\\omega_pt}-e^{-i\\omega_pt}}{2i}\\right]=\\] \\[=\\frac{1}{2}A_0+\\sum\\limits^\\infty_{p=1}\\left[\\frac{A_p e^{i\\omega_p t}}{2} + \\frac{A_p e^{-i\\omega_p t}}{2} -\n\\frac{iB_p e^{i\\omega_p t}}{2} + \\frac{iB_p e^{-i\\omega_p t}}{2} \\right]=\\] \\[=\\frac{1}{2}A_0+\\sum\\limits^\\infty_{p=1} e^{i\\omega_p t}\\frac{A_p-iB_p}{2} +\n                  \\sum\\limits^\\infty_{p=1} e^{-i\\omega_p t}\\frac{A_p+iB_p}{2}=\\] \\[=C^*_0 + \\sum\\limits^\\infty_{p=1}C^*_pe^{i\\omega_p t} + \\sum\\limits^\\infty_{p=1}C^*_{-p} e^{-i\\omega_p t}=\n\\sum\\limits^\\infty_{p=-\\infty} C^*_p e^{i\\omega_p t}\\,,\\] donde hemos definido las siguientes relaciones entre los coeficientes de Fourier complejos y reales: \\[C^*_0=\\frac{1}{2}A_0\\,,\\] \\[C^*_p=\\frac{1}{2}(A_p-iB_p)\\,,\\] \\[C^*_{-p}=\\frac{1}{2}(A_p+iB_p)\\,.\\]\nEn resumen, podemos reconstruir la serie periódica \\(f(t)\\) con la transformada de Fourier \\[f(x)=\\sum\\limits^\\infty_{p=-\\infty} C^*_p e^{i\\omega_p t}\\,,\\] e inversamente podemos calcular los coeficientes de Fourier \\(C^*_p\\) a partir de la \\(f(t)\\) \\[C^*_p=\\frac{1}{2}(A_p-iB_p)=\\frac{1}{2}\\left(\\frac{2}{T}\\int\\limits_0^{T} f(t) cos(\\omega_p t) dt -\n                                             i \\frac{2}{T}\\int\\limits_0^{T} f(t) sen(\\omega_p t) dt \\right)=\\] \\[=\\frac{1}{T}\\int\\limits_0^{T} f(t) [cos(\\omega_p t) - isen(\\omega_p t) ]dt = \\frac{1}{T}\\int\\limits_0^{T} f(t) e^{-i \\omega_p t} dt\\,,\\] es decir, podemos pasar del espacio temporal \\(f(t)\\) al espacio espectral o de Fourier \\(C^*_p\\) e inversamente regresar al espacio temporal de nuevo.\nEl teorema de Parseval es precisamente el que demuestra que la el valor cuadrático medio de la serie de Fourier es igual al error cuadrático medio de los coeficientes de Fourier. La varianza de la serie de Fourier es \\[\\frac{1}{T}\\int\\limits^{T}_0 f^2(t) dt=\\frac{1}{T}\\int\\limits^{T}_0\n\\left(\\frac{1}{2}A_0 +\\sum\\limits^\\infty_{p=1}[A_p cos(\\omega_p t) + B_p sen(\\omega_p t)]\\right) dt=\\] \\[=\\frac{1}{T}\\frac{1}{4} A^2_0 \\int\\limits^{T}_0 dt + \\frac{1}{T}A_0 \\sum\\limits^\\infty_{p=1} A_p \\int\\limits^{T}_0 cos(\\omega_p t)dt + \\frac{1}{T}A_0\\sum\\limits^\\infty_{p=1}B_p \\int\\limits^{T}_0 sen(\\omega_p t)dt +\\] \\[+\\frac{1}{T}\\sum\\limits^\\infty_{p=1}A^2_p \\int\\limits^{T}_0 cos^2(\\omega_p t)dt +\n\\frac{1}{T}\\sum\\limits^\\infty_{p=1}B^2_p \\int\\limits^{T}_0 sen^2(\\omega_p t)dt+\\] \\[+\\frac{1}{T}2\\sum\\limits^\\infty_{p=1}A_p \\sum\\limits^\\infty_{p=1}B_p\\int\\limits^{T}_0 sen(\\omega_p t)cos(\\omega_p t)dt=\n\\frac{1}{T}\\frac{1}{4}A^2_0 T + \\frac{1}{T}\\sum\\limits^\\infty_{p=1}A^2_p\\left[ \\frac{t}{2} + \\frac{sen(2\\omega_p t)}{4\\omega_p}\\right]^T_0+\\] \\[+\\frac{1}{T}\\sum\\limits^\\infty_{p=1}B^2_p\\left[ \\frac{t}{2} - \\frac{sen(2\\omega_p t)}{4\\omega_p}\\right]^T_0=\n\\frac{1}{4}A^2_0 + \\frac{1}{2}\\sum\\limits^\\infty_{p=1}A_p^2 + \\frac{1}{2}\\sum\\limits^\\infty_{p=1}B_p^2=\\] \\[=\\frac{1}{4}A^2_0 + \\frac{1}{2}\\sum\\limits^\\infty_{p=1} A_p^2 + B_p^2\\,.\\]\nUtilizando las siguientes identidades \\[|C^*_p|^2=|C^*_{-p}|^2=\\frac{1}{4}(A_p^2 + B_p^2)\\,,\\] \\[C^*_0=\\frac{1}{2}A_0\\,,\\] el teorema de Parseval en términos de los coeficientes de Fourier complejos es \\[\\frac{1}{T}\\int\\limits^{T}_0 f^2(t) dt=(C^*_0)^2 + \\frac{1}{2}\\sum\\limits^\\infty_{p=1} 4|C^*_p|^2=\n(C^*_0)^2 + \\sum\\limits^\\infty_{p=1} |C^*_p|^2 + \\sum\\limits^\\infty_{p=1} |C^*_p|^2=\\] \\[=(C^*_0)^2 + \\sum\\limits^\\infty_{p=1} |C^*_p|^2 + \\sum\\limits^\\infty_{p=1} |C^*_{-p}|^2=\n\\sum\\limits^\\infty_{p=-\\infty} |C^*_p|^2\\]\nEsto da lugar a la relación entre la amplitud de las componentes de Fourier en el dominio espectral (de frecuencia) y la varianza de la serie en el dominio temporal.\n\nNOTA: En el análisis de Fourier es importante recalcar que debemos de eliminar la tendencia de la serie antes de calcular los coeficientes. Sino lo hacemos, el análisis de Fourier pondrá erroneamente la varianza de la tendencia en las componentes de baja frecuencia de la expansión de Fourier. En Matlab eso lo podemos hacer con el comando detrend.m o bien simplemente extrayendo el promedio temporal.\n{jemplo de cálculo de coeficientes de Fourier}\nImaginemos la siguiente onda cuadrada representada por la función \\[f(t)=\n\\left\\lbrace\n  \\begin{array}{l}\n     -1,\\text{para} -\\frac{1}{2}T\\le t&lt; 0 \\\\\n     +1,\\text{para} 0\\le t&lt; \\frac{1}{2}T  \\\\\n  \\end{array}\n  \\right.\\]\nPuesto que función de arriba es impar, es decir cumple la condición de simetría \\(-f(t)=f(-t)\\), entonces la serie de Fourier resultante solamente contendrá componentes sinusoidales. Entonces \\[B_p=\\frac{2}{T}\\int\\limits_{-T/2}^{T/2} f(t) sen(\\omega_p t) dt=\\] \\[=\\frac{2}{T}\\int\\limits_{-T/2}^{0} f(t) sen(\\omega_p t) dt + \\frac{2}{T}\\int\\limits_{0}^{T/2} f(t) sen(\\omega_p t) dt=\\] \\[\\frac{2}{T}\\left[ (-1) \\int\\limits_{-T/2}^{0} sen(\\omega_p t) dt +\n                   (1) \\int\\limits_{0}^{T/2} sen(\\omega_p t) dt \\right]=\\]\n\\[=\\frac{2}{T}\\left[ \\left|\\frac{cos(\\omega_p t)}{\\omega_p}\\right|_{-T/2}^{0}\n                     -\\left|\\frac{cos(\\omega_p t)}{\\omega_p}\\right|_{0}^{T/2} \\right]=\\] \\[=\\frac{2}{\\omega_p T}\\left[ 1 - cos(\\omega_p T/2) - cos(\\omega_p T/2) + 1  \\right]=\n\\frac{2}{\\omega_p T}\\left[2 -2cos(\\omega_p T/2)\\right]=\\] \\[=\\frac{4}{\\omega_p T}\\left[1-cos(\\omega_p T/2)\\right]=\n\\frac{2}{\\pi p}\\left[1-cos(\\pi p)\\right]\\] \\ \\ Los coeficientes son cero (\\(B_p=0\\)) si \\(p\\) es par y \\(B_p=4/\\pi p\\) si \\(p\\) es impar. Finalmente nuestra serie de Fourier es \\[f(t)=\\sum\\limits^\\infty_{p=1}B_p sen(\\omega_p t)=\\] \\[=\\sum\\limits^\\infty_{p=1}B_p sen\\left(\\frac{2\\pi p}{T} t\\right)=\nB_1 sen\\left(\\frac{2\\pi 1}{T} t\\right) + B_3 sen\\left(\\frac{2\\pi 3}{T} t\\right) +\\] \\[+B_5 sen\\left(\\frac{2\\pi 5}{T} t\\right) + ... =\\frac{4}{\\pi}sen\\left(\\frac{2\\pi 1}{T} t\\right) +\\] \\[+\\frac{4}{3\\pi}sen\\left(\\frac{2\\pi 3}{T} t\\right) +\\frac{4}{5\\pi}sen\\left(\\frac{2\\pi 5}{T} t\\right)=\\] \\[=\\frac{4}{\\pi}\\left( \\frac{sen{\\omega_1 t}}{1} + \\frac{sen{3\\omega_1 t}}{3} + \\frac{sen{5\\omega_1 t}}{5}\\right)+...\\]\n{Series de Fourier Discretas}\nEn general, vamos a muestrear de forma discreta el océano y consecuentemente las series temporales que obtenemos son discretas en el tiempo. Segun el teorema de Parseval, la varianza de estas series discretas \\[\\sigma^2=\\frac{1}{N-1}\\sum\\limits^N_{t=1}(f(t)-\\overline{f(t)})^2\\] se puede obtener sumando las contribuciones individuales de los armónicos de Fourier. La descomposición de series temporales discretas en armónicos específicos da lugar al concepto de espectro de Fourier. Para encontrar el espectro de Fourier debemos calcular los coeficientes \\(A_p,B_p\\) o, equivalentemente, las amplitudes \\(C_p\\) y el ángulo de fase \\(\\theta_p\\).\nSupongamos la serie de Fourier para un registro finito de longitud par \\(N\\) definido en los tiempos \\(t_1, t_2,....,t_N\\) \\[f(t_n)=\\frac{1}{2}A_0+\\sum\\limits^{N/2}_{p=1}[A_p cos(\\omega_p t_n) + B_p sen(\\omega_p t_n)]\\,,\\] donde ya sabemos \\(\\omega_p=2\\pi f_p=2\\pi p/T\\). Sabiendo que \\(t_n=n \\Delta t\\), esta serie se puede reescribir como \\[f(t_n)=\\frac{1}{2}A_0+\\sum\\limits^{N/2}_{p=1}[A_p cos(2\\pi p n \\Delta t / \\Delta t N) + B_p sen(2\\pi p n \\Delta t / \\Delta t N)]=\\] \\[=\\frac{1}{2}A_0+\\sum\\limits^{N/2}_{p=1}[A_p cos(2\\pi p n  / N) + B_p sen(2\\pi p n / N)]=\n\\frac{1}{2}C_0 + \\sum\\limits^{N/2}_{p=1}[C_p cos[(2\\pi p n  / N) - \\theta_p]\\,,\\] donde los términos \\(A_0/2\\) y \\(C_0/2\\) son los valores medios de toda la serie \\(f(t)\\). Los coeficientes se calculan de igual forma usando las condiciones de ortogonalidad. La única diferencia es que en lugar de tratar con integrales (serie contínua) tratamos con sumatorios (serie discreta) \\[A_p=\\frac{2}{N}\\sum\\limits_{n=1}^{N} f(t_n) cos(2\\pi p n /N),\\,\\,\\,\\,p=1,2,...,N/2\\] \\[A_0=\\frac{2}{N}\\sum\\limits_{n=1}^{N} f(t_n),\\,\\,\\,\\,B_0=0\\] \\[A_{N/2}=\\frac{1}{N}\\sum\\limits_{n=1}^{N} f(t_n)cos(n\\pi),\\,\\,\\,\\,B_{N/2}=0\\] \\[B_p=\\frac{2}{N}\\sum\\limits_{n=1}^{N} f(t_n) sen(2\\pi p n /N),\\,\\,\\,\\,p=1,2,...,N/2\\]\nEl \\(N/2\\) se debe a que es el armónico con mayor frecuencia que podemos resolver, es decir, aquel que oscila con frecuencia de Nyquist. Para \\(p&gt;N/2\\) las funciones trigonométricas simplemente darán coeficientes de Fourier repetidos ya obtenidos en el intervalo \\(1\\le p\\le N/2\\). Para calcular la serie discreta de Fourier, primero debemos calcular los argumentos de las funciones trigonométricas \\(2\\pi n p / N\\) para cada entero \\(p\\) y \\(n\\). Segundo, evaluamos las funciones \\(cos(2\\pi n p / N)\\) y \\(sen(2\\pi n p / N)\\), y sumamos para los términos \\(f(t_n)cos(2\\pi n p / N)\\) y \\(f(t_n)sen(2\\pi n p / N)\\). Por último, incrementamos \\(p\\) y repetimos los dos pasos anteriores.\n{jemplo de Series Temporales Discretas (modificado de Emery and Thompson, p387)}\nConsidera la serie temporal de temperatura promedia mensual por un periodo de tres años (ver tabla y Figura).\n\n\n\nUtilizando las expresiones de arriba podemos calcular las frecuencias \\(f_p\\), amplitudes \\(A_p\\), \\(B_p\\), \\(C_p\\), las fases \\(\\theta_p\\) y finalmente la serie de Fourier \\(f(t)\\). Los valores para las primeras 8 componentes estan reflejados en la tabla\n\n{Serie de Fourier para variables vectoriales (complejas)}\nEn este caso la transformada de Fourier se aplica a una cantidad vectorial en lugar de una cantidad escalar como temperatura, salinidad, densidad, etc. Supongamos que tenemos las dos componentes de la velocidad \\(u\\) y \\(v\\) las cuales expandemos en series de Fourier \\[u(t)=\\frac{1}{2}A_0+\\sum\\limits^{N/2}_{p=1}[A_p cos(\\omega_p t_n) + B_p sen(\\omega_p t_n)]\\] \\[v(t)=\\frac{1}{2}C_0+\\sum\\limits^{N/2}_{p=1}[C_p cos(\\omega_p t_n) + D_p sen(\\omega_p t_n)]\\,,\\] lo cual se puede escribir en versión compleja como \\[R(t)=u(t)+i v(t)=\\frac{1}{2}A_0+\\sum\\limits^{N/2}_{p=1}[A_p cos(\\omega_p t_n) + B_p sen(\\omega_p t_n)]\n+\\] \\[+ i \\left(\\frac{1}{2}C_0+\\sum\\limits^{N/2}_{p=1}[C_p cos(\\omega_p t_n) + D_p sen(\\omega_p t_n)]\\right)=\\] \\[=\\left[\\frac{1}{2}A_0 + i \\frac{1}{2}C_0\\right] + \\sum\\limits^{N/2}_{p=1} \\left[ (A_p + iC_p) cos(\\omega_p t_n) + (B_p + i D_p) sen(\\omega_p t_n) \\right]\\,,\\] donde \\(\\frac{1}{2}A_0 + i \\frac{1}{2}C_0=\\overline{u(t)} +i \\overline{v(t)}\\) es la velocidad media, \\(\\omega_p=2\\pi f_p=2\\pi p/N\\Delta t\\) la frecuencia angular, \\(t_n=n\\Delta t\\) es el eje de tiempo, y \\((A_p\\,,B_p\\,,C_p\\,,D_p)\\) y son las amplitudes y fases de cada componente de Fourier, tanto las reales como las imaginarias. A diferencia de la serie de Fourier real en este caso las componentes van de \\(p=1\\) hasta \\(p=N\\) y, por lo tanto, estamos cubriendo ambas frecuencias positivas y negativas.\nSi extraemos la velocidad media\n\\[R'(t)=R(t)-[\\overline{u(t)} +i \\overline{v(t)}]=\\sum\\limits^{N/2}_{p=1} \\left[ (A_p + iC_p) cos(\\omega_p t_n) + (B_p + i D_p) sen(\\omega_p t_n) \\right]\\,.\\]\nAhora vamos a escribir la anomalía de la serie compleja \\(R'(t)\\) en términos de dos componentes rotatorias ortogonales, es decir, una componente que gira en el sentido de las agujas del reloj con amplitud \\(R^{-}\\) y otra que gira en el sentido opuesto a las agujas del reloj y amplitud \\(R^{+}\\) \\[R'(t)=\\sum\\limits^{N/2}_{p=1} \\left[ e^{i\\omega_p t} + e^{-i\\omega_p t}\\right]=\\] \\[    =\\sum\\limits^{N/2}_{p=1}  R_p^{+}\\left[cos\\left(\\omega_p t_n\\right) + i sen\\left(\\omega_p t_n\\right )\\right] +\n       \\sum\\limits^{N/2}_{p=1}  R_p^{-}\\left[cos\\left(\\omega_p t_n\\right) - i sen\\left(\\omega_p t_n\\right))\\right]=\\] \\[=\\sum\\limits^{N/2}_{p=1}\\left[ (R_p^+ + R_p^{-})cos\\left(\\omega_p t_n\\right) + (R_p^+ - R_p^{-})i sen\\left(\\omega_p t_n\\right) \\right]\\]\nNote que \\(e^{i\\omega_p t}=cos\\left(\\omega_p t_n\\right) + i sen\\left(\\omega_p t_n\\right)\\) rota en el sentido contrario a las agujas del reloj y \\(e^{-i\\omega_p t}=cos\\left(\\omega_p t_n\\right) - i sen\\left(\\omega_p t_n\\right)\\) rota en el sentido de las agujas del reloj. Si comparamos las dos expresiones obtenemos las siguientes identidades \\[A_{p} + i C_{p}=R_p^+ + R_p^{-}\\] \\[B_{p} + i D_{p}=(R_p^+ - R_p^{-})i\\] y de ahí obtenemos que \\[R_p^+=\\frac{1}{2}\\left[ A_{p} + D_{p} +i(C_{p}-B_{p})\\right]\\] \\[R_p^-=\\frac{1}{2}\\left[ A_{p} - D_{p} +i(C_{p}+B_{p})\\right]\\,,\\] y las magnitudes de las componentes rotatorias es \\[|R_p^+|=\\frac{1}{2}\\left[ (A_{p} + D_{p})^2 +(C_{p}-B_{p})^2\\right]^{1/2}\\] \\[|R_p^-|=\\frac{1}{2}\\left[ (A_{p} - D_{p})^2 +(C_{p}+B_{p})^2\\right]^{1/2}\\,,\\] y las fases de las componentes rotatorias son \\[\\epsilon_p^+=actan\\left(\\frac{C_{p}-B_{p}}{A_{p} + D_{p}}\\right)\\,,\\] \\[\\epsilon_p^-=actan\\left(\\frac{C_{p}+B_{p}}{A_{p} - D_{p}}\\right)\\,.\\]\nLa rotación de las componentes clockwise y anticlockwise dibujan una elipse en el plano \\(u\\) vs \\(v\\). Puesto que ambas componentes rotan en sentido contrario pero con la misma frecuencia, habrán momentos que ambás apuntaran en la misma dirección (aditivas), otras ocasiones en dirección opuesta (cancelativas). Esos tiempos de adición y cancelación definen el eje mayor de la elipse \\(L_E=R_p^+ + R_p^-\\) y el eje menor de la elipse \\(L_e=R_p^+ - R_p^-\\). La orientación (inclinación) y la fase de estas elipses a \\(t=0\\) es \\[\\theta_e=\\frac{1}{2}(\\epsilon_p^+ + \\epsilon_p^-)\\,,\\] \\[\\phi_e=\\frac{1}{2}(\\epsilon_p^+ - \\epsilon_p^-)\\,.\\]\nUna propiedad interesante es el coeficiente rotatorio \\[r(\\omega)=\\frac{R^+_p - R^-_p}{R^+_p + R^-_p}\\,,\\] que toma valores entre 0 y 1. Para \\(r=-11\\) tenemos movimiento en el sentido de las agujas del reloj, para \\(r=0\\) tenemos un flujo unidireccional, y para \\(r=+1\\) tenemos movimiento en el sentido contrario de las agujas del reloj.\n{Transformada Rápida de Fourier y espectros de potencia}\nLa FFT (por sus siglas en inglés) es un algoritmo para calcular la serie de Fourier discreta de forma mas eficiente computacionalmente hablando. En este caso la FFT se debería aplicar a series temporales con longitudes múltiples de \\(2\\). En caso contrario, es útil rellenar de ceros nuestra serie para obtener longitudes múltiplos de \\(2\\). A eso se le llama `padding’. Básicamente, el algoritmo obtiene los coeficientes de la serie discreta de Fourier\n\\[f(t)=\\sum\\limits^\\infty_{p=-\\infty} C^*_p e^{i\\omega_p t_n}\\,\\,\\,\\,\\,\\,\\,;\\,\\,\\,\\,\\,\\,\\,\\omega_p=2 \\pi p f_0=2\\pi p/T\\,,\\] \\[F(p)=C^*_p=\\frac{1}{2}(A_p-iB_p)=\\frac{1}{N} \\sum\\limits^{N-1}_{n=0} f(t) e^{-i \\omega_p t_n} =\n        \\frac{1}{N} \\sum\\limits^{N-1}_{n=0}f(t) [cos(\\omega_p t_n) - isen(\\omega_p t_n) ]\\,.\\]\nLa parte real de la FFT me da las amplitudes \\(A_p\\) y la parte imaginaria me da las amplitudes \\(-B_p\\) \\[Re [F(p)]=A_p=\\frac{2}{N}\\sum\\limits_{n=0}^{N-1} f(t_n) cos(2\\pi p n /N),\\,\\,\\,\\,p=0,1,...,N/2\\] \\[Im [F(p)]=-B_p=-\\frac{2}{N}\\sum\\limits_{n=0}^{N-1} f(t_n) sen(2\\pi p n /N),\\,\\,\\,\\,p=0,1,...,N/2\\]\nEn general, debemos de normalizar las amplitudes \\(A_p\\) y \\(B_p\\) por la longitud del registro \\(N\\). Así que en Matlab la amplitud de la FFT es \\[{ abs}({ fft}(f(t)))/N\\,,\\] y la potencia de la FFT es \\[{ abs}({ fft}(f(t)).^2/N^2\\,.\\]\nLa FFT ha descompuesto una señal de \\(N\\) elementos, \\(f(t)\\), en un conjunto de \\(N/2 +1\\) ondas cosinusoidales y \\(N/2 + 1\\) ondas sinusoidales, con las frecuencias definidas por el índice \\(p=0,1,...,N/2\\), i.e. $_p= 2f_p = 2p / T = 2p / N t $. Las amplitudes de los cosenos estan contenidas en \\(Re [F(p)]\\) y las amplitudes de los senos en \\(Im [F(p)]\\). Note que las frecuencias son siempre positivas, es decir, los índices \\(k\\) siempre van de cero a \\(N/2\\). Las frecuencias entre \\(N/2\\) y \\(N-1\\) son negativas. Recuerda que el espectro frecuencial de una señal discreta es periódico, y entonces las frecuencias son negativas entre \\(N/2\\) y \\(N-1\\) al igual que en el intervalo \\(-N/2\\) y \\(-1\\). Los puntos \\(0\\) y \\(N/2\\) separan las frecuencias negativas de las positivas. Es por ello que, generalmente, solamente centramos nuestra atención en la parte positiva del espectro. La magnitud (o norma) de la transformada de Fourier discreta es\n\\[{ Magnitud}=|F(p)|=\\sqrt{Re [F(p)]^2 + Im[F(p)]^2}\\,,\\]\ny la fase es\n\\[Phase=tan^{-1}\\left( \\frac{Im[F(p)]}{Re [F(p)]} \\right)\\,.\\]\nLa FFT organiza los coeficientes de Fourier (imaginarios y reales) en frecuencias negativas y positivas y reparte la varianza de la señal equitativamente entre ellas. En \\(p=0\\) tenemos la media de la serie temporal, aunque debido a que hemos eliminado la media y la tendencia de la serie temporal no debemos de preocuparnos por ella. Entre \\(p=1,...,N/2\\) tenemos los valores de los coeficientes de Fourier reales y entre \\(p=N/2+1,...,N-1\\) tenemos los complejos conjugados de los primeros \\(N/2\\) coeficientes. Si calculamos el valor absoluto de la transformada de Fourier (en Matlab \\({ abs}({ fft}(f(t)))/N\\)) estamos calculando \\(A_p^2 + B_p^2\\) y si solo nos quedamos con los primeros \\(N/2\\) elementos de la FFT, debemos de multiplicar por un factor de \\(2\\) para conservar la energía espectral.\n\nEstimaciones espectrales o autoespectros\n\nEspectro de amplitud\n\nLa gráfica de la magnitud de los coeficientes complejos \\(|C_p^*|\\) de la serie de Fourier \\[f(t)=\\sum\\limits^\\infty_{p=-\\infty} C^*_p e^{i\\omega_p t_n}\\] frente a (versus) la frecuencia \\(\\omega_p\\) se denomina espectro de amplitud de la función periódica \\(f(t)\\). En Matlab (para las primeras \\(N/2\\) componentes), \\[{ Amplitud}=2*|C^*_p|=abs({ fft}(f(t)))/N\\,.\\]\n\nEspectro de densidad de potencia (Power Spectral Density, PSD)\n\nEl espectro de densidad de potencia (PSD, por sus siglas en inglés) es la potencia de la FFT por unidad de frecuencia \\[{\\it PSD}(p)=2*|C^*_p|^2/\\Delta f\\] donde \\(\\Delta f=1/N\\Delta t\\) es la frecuencia fundamental.\nLa gráfica de \\({ PSD}(p)\\) de la frente a (versus) la frecuencia \\(\\omega_p\\) se denomina espectro de densidad de potencia de la función periódica \\(f(t)\\). Si solo nos quedamos con las \\(N/2\\) primeras componentes en Matlab se escribe \\[{\\it PSD}(p)=2*abs({ fft}(f(t)))^2/N^2/\\Delta f\\,.\\]\nEsta normalización tiene su fundamento en el cumplimiento del teorema de Parseval, de tal forma que la energía total de la señal en el dominio temporal \\(f(t)\\) (por unidad de tiempo) sea igual a la energía total de la señal en el dominio frecuencial definido por \\(C^*_p\\): \\[\\frac{1}{T}\\sum\\limits^N_{n=1}|f(t_n)|^2 \\Delta t =\\frac{1}{N}\\sum\\limits^N_{n=1}|f(t_n)|^2 =\nvar(f(t))=\\sum\\limits^{\\infty}_{p=-\\infty} |C^*_p|^2=\\sum\\limits^{N/2}_{p=0}PSD(p)*\\Delta f\\,.\\]\n\nEste teorema de conservacón de energía nos informa de que la integral bajo la curva espectral \\({ PSD}(p)\\) debe ser igual a la varianza total de la serie temporal.\nEfectos de los extremos en estimaciones espectrales\nEn general, para calcular un espectro promedio debemos fragmentar nuestra serie temporal en bloques de igual tamaño que contengan las frecuencias de interés, realizar espectros individuales de dichos fragmentos, y promediar todos ellos. Este método se le conoce como Welch. Los fragmentos pueden ser únicos, es decir, sin superposición o bien pueden ser recursivos, es decir, cuando utilizamos superposición de fragmentos. Por ejemplo, una superposición del 50% significa que cada fragmento empieza en la mitad del fragmento anterior. Además de eliminar el ruido, el método de Welch también reduce la transferencia de energía de las frecuencias pico hacia frecuencias colindantes (leakage' en inglés). El método de Welch reduce el ruido causado por el uso de datos imperfectos y por el efectoleakage’. Aqui les muestro un ejemplo de como promediar un espectro de densidad espectral con fregmentos de tamaño \\(M\\):\nEste problema de transferencia de energía o `leakage’ es intrínseco al problema de que las series temporales oceanográficas son finitas y, por lo tanto, no son necesariamenteperiódicas, condición necesaria en el análisis de Fourier. Veamos esto con un ejemplo de una onda cosinusoidal periódica y no-periódica.\nPodemos observar como en el caso de la serie no-periódica existe una transferencia de energía del pico espectral hacia las frecuencias colindantes de forma que se reduce la amplitud del pico de interés.\nCorrelación\ny \\(s_y\\) son las varianzas de las variables \\(x\\) e \\(y\\), respectivamente. \\end{framed}\nKundu (1976b) define la función de correlación cruzada desfasada entre dos series de velocidad en las profundidades 1 y 2 como \\[\\rho_{\\tau}=\\frac{\\overline{u_1'(t)u_2'(t-\\tau)}}\n{\\left[ \\overline{u_1'(t)^2}\\,\\, \\overline{u_2'(t)^2} \\right]^{1/2}}\\,,\\] donde las primas \\('\\) indican anomalias. Esta función fue utilizada para estudiar la propagación vertical de ondas inercio-gravitatorias y la velocidad de fase de estas ondas \\(c=\\Delta_{12}/\\tau\\). Kundu (1976a) introduce el coeficiente de correlación complejo\n\\[\\rho=\\frac{\\overline{w_1^*(t)w_2(t)}}\n{\\left[ \\overline{w_1^*(t)w_1^*(t)}\\,\\, \\overline{w_2^*(t)w_2(t)} \\right]^{1/2}}\\,,\\]\ndonde \\(w=u+i v\\), los asteriscos \\(*\\) indican complejo conjugados, y los subíndices \\(1\\) y \\(2\\) se refieren a dos estaciones de medida. La cantidad \\(\\rho\\) es un número complejo cuya magnitud (\\(\\le1\\)) nos da una medida de correlación promedia y cuyo ángulo de fase da el angulo promedio, medido en el sentido contrario a las agujas del reloj, del segundo vector con respecto del primero. Por ejemplo, un ángulo de fase negativo entre las profundidades \\(50\\) y \\(100\\,{ m}\\) implica que la señal llega primero a \\(z=-100\\,{ m}\\) y luego a \\(z=-50\\,{ m}\\), es decir, podría tratarse de una onda interna cuyas fases se propagan hacia arriba.\n\n\nEspectro cruzado\nCon análisis de espectros cruzados pretendemos comprender la relación entre dos series temporales en función de la frecuencia. Por ejemplo, observamos en dos localizaciones espectros con picos en las mismas frecuencias y queremos saber si dichos armónicos estan relacionados.\nSupongamos dos series de Fourier \\(x(t)\\) e \\(y(t)\\) \\[x(t)=\\bar{x}+\\sum\\limits^{N/2}_{p=1} A_{xk} cos(\\omega_p t_n) + B_{xp}sen(\\omega_p t_n)\\,,\\] \\[y(t)=\\bar{y}+\\sum\\limits^{N/2}_{p=1} A_{yk}cos(\\omega_p t_n) + B_{yp}sen(\\omega_p t_n)\\,.\\] Utilizando las condiciones ortogonalidad entre las funciones sinusoidales y cosinusoidal, la covarianza entre las variables \\(x\\) e \\(y\\) es \\[\\overline{x'y'}=\\sum\\limits^{N/2}_{p=1} \\frac{1}{2}( A_{xp}A_{yp}  + B_{xp}B_{yp})=\\sum\\limits^{N/2}_{p=1}Co(p)\\,,\\] donde \\(Co(p)\\) es el co-espectro de \\(x\\) e \\(y\\).\nSupongamos dos series de Fourier \\(x(t)\\) e \\(y(t)\\) definidas en la forma compleja (el asterisco ha sido eliminado en esta notación) \\[x(t)=\\bar{x}+\\sum\\limits^{N/2}_{p=1}\n  C_{xp} e^{i\\omega_p t_n}=\\bar{x}+\n  \\sum\\limits^{N/2}_{p=1}\\frac{1}{2}\\left(A_{xp} - iB_{xp} \\right)e^{i\\omega_p t_n}=\\bar{x}+\n  \\sum\\limits^{N/2}_{p=1}F_x(p)\\] \\[y(t)=\\bar{y}+\\sum\\limits^{N/2}_{p=1}\n  C_{yk} e^{i\\omega_p t_n}=\\bar{y}+\n  \\sum\\limits^{N/2}_{p=1}\\frac{1}{2}\\left(A_{yp} - iB_{yp} \\right)e^{i\\omega_p t_n}=\\bar{y}+\n  \\sum\\limits^{N/2}_{p=1}F_y(p)\\,.\\]\nSi ahora calculamos las varianzas \\[\\overline{x'^2}=\\sum\\limits^{N/2}_{p=-N/2}F_{xx}(p)\\,,\\]\ndonde \\[F_{xx}(p)=2\\frac{1}{2}\\left(A_{xp} - iB_{xp} \\right)e^{i\\omega_p t_n}\\frac{1}{2}\\left(A_{xp} + iB_{xp} \\right)e^{-i\\omega_p t_n}\n=2F_x(p) F^*_x(p)=|C_{xp}|^2\\,,\\]\ny el asterisco indica complejo conjugado. Para la variable \\(y\\) de igual forma obtenemos:\n\\[\\overline{y'^2}=\\sum\\limits^{N/2}_{p=-N/2}F_{yy}\\,;\\,\\,\\,\\,\\,F_{yy}(p)=2F_y(p) F^*_y(p)=|C_{yp}|^2\\]\nDe las expresiones anteriores se deduce que covarianza se puede calcular en el espacio espectral como\n\\[\\overline{x'y'}=Re\\left[\\sum\\limits^{N/2}_{p=-N/2}F_{xy}(p)\\right]\\,,\\] donde \\[F_{xy}(p)=2\\frac{1}{2}\\left(A_{xp} - iB_{xp} \\right)e^{i\\omega_p t_n}\\frac{1}{2}\\left(A_{yp} + iB_{yp} \\right)e^{-i\\omega_p t_n}\n=2F_x(p) F^*_y(p)=|C_{xp}||C_{yp}|e^{i(\\theta_{xp}-\\theta_{yp})}\\,.\\]\nEl factor \\(e^{i(\\theta_{xp}-\\theta_{yp})}\\) aparece para considerar que ambas series periódicas no estan en fase.\n\nEspectro cruzado complejo\n\nSi escribimos \\(F_{xy}(p)\\) en términos de los coeficientes de Fourier reales \\[F_x(p) F^*_y(p)=\\frac{1}{2}\\left(A_{xp} - iB_{xp} \\right)e^{i\\omega_p t_n}\\frac{1}{2}\\left(A_{yp} + iB_{yp} \\right)e^{-i\\omega_p t_n}=\\] \\[=\\frac{1}{4}\\left[A_{xk}A_{yk}  + B_{xk}B_{yp} + i\\left(A_{xp}B_{yp} - A_{yp}B_{xp}\\right)\\right]\\,.\\] Para el caso de series \\(x(t)\\) e \\(y(t)\\) reales sabemos que las frecuencias negativas son los complejos conjugados de las frecuencias positivas y entonces \\[A_k=A_{-k}\\,\\,\\,\\,\\,{ y}\\,\\,\\,\\,\\,B_k=B_{-k}\\,,\\] y \\[F_x(p)F^*_y(p)=F_x(-p)F_y^*(-p)\\,,\\] y como conclusión\n\\[F_{xy}(p)+F_{xy}(-p)=\\frac{1}{2}\\left[ A_{xp}A_{yp}  + B_{xp}B_{yp} + i\\left(A_{xp}B_{yp} - A_{yp}B_{xp}\\right)\\right]\\,,\\]\nque es espectro cruzado de \\(x\\) e \\(y\\) para el armónico \\(p\\). De esta expresión encontramos que\n\\[F_{xy}(p)+F_{xy}(-p)=2F_{xy}(p)=Co(p) + i Q(p)\\,,\\] donde \\(Co(p)=\\frac{1}{2}( A_{xp}A_{yp} + B_{xp}B_{yp})\\) es el co-espectro del armónico p\ny \\(Q(p)=\\frac{1}{2}(A_{xp}B_{yp} - A_{yp}B_{xp})\\) es el espectro de cuadratura del armónico \\(k\\).\nEn notación compleja el espectro cruzado \\[F_{xy}(p)=C_{xp}C_{yp}e^{i(\\theta_{xp}-\\theta_{yp})}=C_{xp}C_{yp}\\left(cos(\\theta_{xp}-\\theta_{yp})+ isen(\\theta_{xp}-\\theta_{yp}) \\right)\\,.\\]\n\\[\\theta_{xp}=\\theta_{yp}\\,\\,\\,\\,\\,\\text{entonces}\\,\\,\\,\\,F_{xy}(p) \\text{es real}\\] \\[\\theta_{xp}\\ne\\theta_{yp}=\\pm\\frac{\\pi}{2}\\,\\,\\,\\,\\,\\text{entonces}\\,\\,\\,\\,F_{xy}(p) \\text{es complejo}\\]\nEntonces el co-espectro (la parte real del espectro cruzado) esta en fase con la señal y el espectro de cuadratura esta totalmente desfasado.\n\nEspectro de coherencia\n\nPara una única componente \\(p\\), el espectro de coherencia al cuadrado entre dos series \\(x\\) e \\(y\\) se define\n\\[Coh^2(p)=\\frac{|F_{xy}(p)|^2}{F_{xx}F_{yy}}=\\frac{|C_{xp}C_{yp}|^2}{C_{xp}^2C_{yp}^2}\\,,\\]\ndonde \\(|Coh^2(p)|^{1/2}\\) es su magnitud y \\(\\phi_{xy}(p)\\) es el ángulo de desfase entre las dos componentes \\(p\\) de \\(x\\) e \\(y\\).\nEl espectro de coherencia al cuadrado nos indica el grado de correlación existente entre dos señales. Dos señales estan altamente correlacionadas si la magnitud del espectro de coherencia al cuadrado es \\(\\simeq 1\\) y su fase es \\(\\phi_{xy}(p)\\simeq 0\\)."
  },
  {
    "objectID": "article2_new.html#métodos-de-filtrado-y-suavizado",
    "href": "article2_new.html#métodos-de-filtrado-y-suavizado",
    "title": "Julio Sheinbaum",
    "section": "Métodos de filtrado y suavizado",
    "text": "Métodos de filtrado y suavizado\n\nConvolución y funciones respuesta (ventanas espectrales)\nLa convolución de dos funciones \\(f(t)\\) y \\(g(t)\\) sobre un registro finito \\([0,T]\\) se define como\n\\[[f*g](t)=\\frac{1}{T}\\int\\limits^{T}_{0}f(\\tau)g(t-\\tau) d\\tau\\,.\\]\nO también se puede expresar sobre un registro infinito como\n\\[[f*g](t)=\\int\\limits^{\\infty}_{-\\infty}f(\\tau)g(t-\\tau) d\\tau = \\int\\limits^{\\infty}_{-\\infty}g(\\tau)f(t-\\tau) d\\tau\\,.\\]\nLa convolución satisface las siguientes propiedades\n\n\\[f*g=g*f\\]\n\\[f*(g*h)=(f*g)*h\\]\n\\[f*(g+h)=(f*g)+(f*h)\\]\n\nAhora retomemos las definiciones de serie de Fourier y la transformada de Fourier:\nVamos ahora a deducir el teorema de la convolución. Para ello vamos a partir de la definición de convolución:\n\\[f*g=\\frac{1}{T}\\int\\limits^{T}_{0}f(\\tau)g(t-\\tau) d\\tau=\n      \\frac{1}{T}\\int\\limits^{T}_{0}f(\\tau)\\sum\\limits^{\\infty}_{p=-\\infty} G(\\omega_p) e^{i\\omega_p (t-\\tau)} d\\tau=\\]\n\\[=\\sum\\limits^{\\infty}_{p=-\\infty} G(\\omega_p) \\left[\\frac{1}{T}\\int\\limits^{T}_{0}f(\\tau)e^{-i\\omega_p \\tau} d\\tau \\right]\ne^{i\\omega_p t}=\\sum\\limits^{\\infty}_{p=-\\infty} G(\\omega_p)F(\\omega_p)\ne^{i\\omega_p t}={\\cal F}^{-1}[G(\\omega)F(\\omega)](t)\\,.\\]\nSi aplicamos transformada de Fourier a ambos lados del igual obtenemos:\n\\[{\\cal F}(f*g)=G(\\omega_p)F(\\omega_p)={\\cal F}[g(t)]{\\cal F}[f(t)]\\,,\\]\nes decir, la transformada de Fourier de la convolución de \\(f\\) y \\(g\\) es equivalente a multiplicar en el espacio espectral las transformadas de Fourier de las funciones individuales. La correlación cruzada desfasada de \\(f(t)\\) y \\(g(t)\\) en forma integral se puede definir como\n\\[C_{fg}(\\tau)=\\frac{1}{T}\\int\\limits^{T}_{0} f(\\tau) g(t+\\tau) d\\tau={\\cal F}^{-1}[G(\\omega)F(-\\omega)](t)=\n{\\cal F}^{-1}[G(\\omega)F^*(\\omega)](t)\\,,\\]\nes decir, si multiplicamos la transformada de Fourier de una función por el complejo conjugado de la transformada de Fourier de otra función es equivalente a la transformada de Foruier de la correlación cruzada desfasada entre ellas. Este se le conoce por el teorema de correlación. Para el caso particular que sea la misma función \\(g(t)\\) la que se correlaciona, entonces:\n\\[{\\cal F}[C_{gg}(\\tau)]=G(\\omega)G^*(\\omega)=|G(\\omega)|^2\\,,\\]\nes decir, la transformada de Fourier de la autocorrelación es igual al espectro de potencia de la función \\(g(t)\\). Este se denomina el teorema de Weiner-Khinchin.\nEl concepto de convolución es útil en el filtrado de señales periódicas. En general vamos a convolucionar nuestra señal \\(f(t)\\) con la denominada función respuesta \\(r(t)\\). La función \\(r(t)\\) es típicamente una función pico que cae a cero en ambas direcciones desde el máximo (o pico).\nPuesto que la función respuesta es mas ancha que algunas estructuras de pequeña escala de nuestra señal original, estas serán suavizadas tras realizar la convolución. \\ \\ {OTA:} Por el teorema de convolución filtrar en el dominio temporal convolucionando es equivalente a multiplicar la transformada de Fourier de la señal con la transformada de Fourier de la función respuesta.\nLas ventanas mas comunes para suavizar señales son las de: \n\n`Boxcar’ \\[\\begin{equation*}\n  r(t)=\n  \\left\\lbrace\n  \\begin{array}{l}\n1 \\text{ if } 0\\le t\\le T \\\\\n0 \\text{ if } t&gt;T \\\\\n  \\end{array}\n  \\right.\n\\end{equation*}\\]\n\nLa transformadad de Fourier es la función sinc\n\\[R(\\omega)={\\it sinc}=\\frac{sin\\left(\\frac{\\omega T}{2}\\right)}{\\frac{\\omega T}{2}}\\,.\\]\nEsta función respuesta tiende a cero cuando \\(\\omega T / 2\\) se acerca a cero, es decir, para \\(\\omega T=2 n \\pi;\\,\\,\\text{para}\\,\\,n=1,2,3,...\\). Esta no es una ventana o función respuesta recomendable debido a los lóbulos de menor amplitud alrededor del pico. En general respuestas tipo ondas sinusoidales o cosinusoidales amortiguadas a ambos lados del pico no son deseables.\n\nHanning\n\n\\[\\begin{equation*}\n  r(t)=\\left\\lbrace\n  \\begin{array}{l}\n     \\frac{1}{2}\\left( 1-cos\\frac{2\\pi t}{T}\\right)\n      \\text{ if } -T/2 \\le t \\le T/2 \\\\\n     0 \\text{ if } { contrario} \\\\\n  \\end{array}\n  \\right.\n  \\end{equation*}\\]   (3) Hamming\n\\[\\begin{equation*}\n  r(t)=\\left\\lbrace\n  \\begin{array}{l}\n     \\left(0.54  + 0.46 cos(\\frac{\\pi t}{T}) \\right) \\text{ if } -T/2 \\le t \\le T/2 \\\\\n     0 \\text{ if } { contrario} \\\\\n  \\end{array}\n  \\right.\n  \\end{equation*}\\]\nEs evidente que este tipo de filtro es un suavizado o filtro pasa bajo. Sin embargo, siempre podemos recuperar facilmente la señal de alta frecuencia (filtro pasa altas) restando a la señal original la serie suavizada con convolución.\nImaginemos que tenemos una serie temporal \\(u(t)\\) con un paso temporal de \\(dt=1\\,{ h}\\). Entonces, para suavizar \\(u(t)\\) de tal forma que se eliminen las señales con periodos menores de \\(T=48\\,{ h}\\), es decir, un filtro pasa baja con frecuencia de corte \\(1/48\\,{ h}^{-1}\\) debemos convolucionar \\(u(t)\\) con una función de resouesta \\(r(t)\\) o ventana. En este ejemplo Matlab se muestra como programar un suavizado\nYa hemos visto que podemos suavizar una señal simplemente con la convolución en el dominio temporal de la señal con una ventana o función respuesta (Boxcar, Hanning, Hamming,etc.). Suavizar una señal es comparable a un filtro de pasa baja, es decir, un filtro que solamente deja pasar las frecuencias bajas y elimina (pone a cero) las altas frecuencias. De forma ideal los filtros en el dominio frecuencial los representamos como: \\\n\nPasa baja (filtra las altas frecuencias), \\[\\begin{equation*}\n|R(\\omega)|=\n  \\left\n  \\lbrace\n  \\begin{array}{l}\n1 \\text{ si } |\\omega| \\le \\omega_c \\\\\n0 \\text{ si } \\omega_c \\le \\omega   \\\\\n  \\end{array}\n  \\right.\n  \\end{equation*}\\] \\\nPasa banda (filtra las frecuencias fuera de la banda) \\[\\begin{equation*}\n|R(\\omega)|=\\left\\lbrace\n  \\begin{array}{l}\n1 \\text{ si } \\omega_{c1}\\le|\\omega|\\le \\omega_{c2} \\\\\n0 \\text{ si } \\text({ lo}\\,\\,\\,{ contrario}) \\\\\n  \\end{array}\n  \\right.\n  \\end{equation*}\\] \\\nPasa alta (filtra las bajas frecuencias) \\[\\begin{equation*}\n|R(\\omega)|=\\left\\lbrace\n  \\begin{array}{l}\n0 \\text{ si } |\\omega|\\le \\omega_c \\\\\n1 \\text{ si } \\omega_c\\le\\omega \\\\\n  \\end{array}\n  \\right.\n  \\end{equation*}\\]\n\n\n\nPromedio corrido\nVeamos primero un ejemplo sencillo de filtrado paso bajo con un promedio corrido de dos puntos. Este sería el caso de filtrar utilizando la función smooth.m de Matlab. Para dos puntos sería \\(&gt;&gt; f_{suavizada}=smooth(f,2);\\). \\ Promedio corrido con dos puntos es simplemente el valor promedio \\[y(n)=\\frac{s(n) + s(n-1)}{2}\\,,\\] donde \\(s(n)\\) es una señal periódica, \\(s(-1)=s(N)\\), \\(N\\ge 2\\). La función \\(y(n)\\) es una versión suavizada con altas frecuencias eliminadas y bajas frecuencias mantenidas. Para ver esto definimos \\(s(n)=sen(2\\pi f n/N)\\), y entonces \\[y(n)=\\frac{1}{2} s(n) + \\frac{1}{2} s(n-1)=\\frac{1}{2} sen(2\\pi f n/N) + \\frac{1}{2} sen(2\\pi f (n-1)/N)=\\] \\[=\\frac{1}{2}sen(2\\pi f n/N) + \\frac{1}{2}\\left[ sen(2\\pi f n/N)cos(2\\pi f/N) - cos(2\\pi f n/N)sen(2\\pi f/N)\\right]=\\] \\[=\\frac{1}{2}\\left[ 1+cos(2\\pi f/N)\\right]sen(2\\pi f n/N) - \\frac{1}{2} cos(2\\pi f n/N)sen(2\\pi f/N)=\\] \\[=A_1sen(2\\pi f n/N) - A_2cos(2\\pi f n/N)\\,,\\] donde \\[A_1=\\frac{1}{2}\\left[ 1+cos(2\\pi f/N)\\right]\\,\\,;\\,\\,\\,\\,\\,A_2=sen(2\\pi f/N)\\,.\\]\nPara bajas frecuencias, es decir, \\(f\\sim 0\\) se cumple que \\(A_1\\sim 1\\) y \\(A_2 \\sim 0\\) y entonces \\[y(n)\\sim sen(2\\pi f n/N)=s(n)\\] y las bajas frecuencias son prácticamente mantenidas. Por el contrario para altas frecuencias, es decir, \\(f\\sim N/2\\), \\(A_1\\sim 0\\), \\(A_2\\sim0\\), y entonces \\(y(n)\\sim0\\) y consecuentemente las altas frecuencias son prácticamente eliminadas.\nLa fórmula general para el promedio corrido es \\[y(n)=\\frac{1}{M}\\sum\\limits^{(M-1)/2}_{p=-(M-1)/2} s(n+p)\\,,\\] donde \\(y( )\\) es el valor de la serie filtrada y \\(s( )\\) es la serie original sin filtrar, \\(M\\) es el número de puntos usados en el promedio. Por ejemplo, en un promedio corrido de \\(5\\) puntos, el valor en el punto \\(30\\) será \\[y(30)=\\frac{s(28)+s(29)+s(30)+s(31)+s(32)}{5}\\,.\\]\nEs evidente que este tipo de filtro es un suavizado o filtro pasa bajo. Sin embargo, podemos recuperar facilmente la señal de alta frecuencia restando a la señal original la serie filtrada.\n\n\nFiltros generales coseno\nSupongamos un simple filtro simétrico obtenido como la convolución entre una función de pesos \\(r(t)\\) y la señal \\(x(t)\\)\n\\[y_n=\\sum\\limits^{\\infty}_{p=-\\infty} r_p x_{n-p}\\,\\,\\,\\,{ donde}\\,\\,\\,\\,r_p=r_{-p}\\,,\\]\nson pesos elegidos adecuadamente. El efecto de filtrado se observa mejor en el dominio frecuencial. Queremos calcular la transformada de Fourier de una serie temporal \\(f(t)\\), la cual ha sido desfasada un tiempo \\(\\Delta t = a\\):\n\\[f(t\\pm a)=\\int\\limits^{\\infty}_{-\\infty} F(\\omega)e^{i\\omega (t\\pm a)} d{\\omega}=\n\\int\\limits^{\\infty}_{-\\infty}  \\left[F(\\omega) e^{i \\omega t}\\right]e^{\\pm i \\omega a} d{\\omega}\\]\nDe esta expresión deducimos que la transformada de Fourier de una serie desfasada por un intervalo de tiempo \\(\\Delta t\\) es igual a la transformada de Fourier de la serie no desfasada multiplicada por un factor \\[e^{\\pm i\\omega \\Delta t}\\,.\\] Usando este resultado, la transformada de Fourier de \\(y_n\\) se puede escribir como \\[Y(\\omega)={\\cal F}[y_n]=\\sum\\limits^{\\infty}_{p=-\\infty} r_p e^{-i\\omega_p \\Delta t} X(\\omega)\\,,\\] donde \\(X(\\omega)\\) y \\(Y(\\omega)\\) son la transformada de Fourier de \\(y(t)\\) y \\(x(t)\\), y la función respuesta en el dominio frecuencial es\n\\[R(\\omega)=\\frac{Y(\\omega)}{X(\\omega)}=\\sum\\limits^{\\infty}_{p=-\\infty} r_p e^{-i\\omega_p \\Delta t}\\,.\\]\n{}Puesto que \\(r_p=r_{-p}\\) y\n\\[{ cos}(x)=\\frac{e^{ix} + e^{-ix}}{2}\\,,\\]\npodemos escribir la función respuesta del filtrado deseado como\n\\[R(\\omega)=\\sum\\limits^{\\infty}_{p=-\\infty} r_p e^{-i\\omega_p \\Delta t}=\nr_0 + \\sum\\limits^{\\infty}_{p=1} r_p e^{i\\omega_p \\Delta t}\n+ \\sum\\limits^{\\infty}_{p=1} r_{-p} e^{-i\\omega_p \\Delta t} =\nr_0 + \\sum\\limits^{\\infty}_{p=1} r_p \\left[ e^{i\\omega_p \\Delta t} + e^{-i\\omega_p \\Delta t}\\right]=\n\\] \\[=r_0 + 2\\sum\\limits^{\\infty}_{p=1} r_p \\left[ \\frac{e^{i\\omega_p \\Delta t} + e^{-i\\omega_p t}}{2}\\right]=\nr_0 + 2\\sum\\limits^{\\infty}_{p=1}r_p cos(\\omega_p \\Delta t)\\]\nEn general los pesos \\(r_p\\) se van a calcular utilizando la siguiente expresión: \\[r_p=\\frac{1}{\\omega_N}\\int\\limits^{\\omega_N}_0 R(\\omega_p) cos(\\omega_p \\Delta t\n)d{\\omega}\\]\nPor ejemplo para un filtro pasa bajo \\(R(\\omega)=1\\) para \\(0&lt;|\\omega_p|\\le \\omega_c\\) y la integral para calcular los pesos queda \\[r_p=\\frac{1}{\\omega_N}\\int\\limits^{\\omega_c}_0 R(\\omega_p) cos(\\omega_p \\Delta t\n)d{\\omega}=\\frac{\\omega_c}{\\omega_N}\\frac{sen(\\omega_c p \\Delta t)}{\\omega_c p \\Delta t}=\n      \\frac{1}{\\omega_N}\\frac{sen(\\pi p \\omega_c / \\omega_N)}{\\pi p / \\omega_N}=\\] \\[=\\frac{sen(\\pi p \\omega_c / \\omega_N)}{\\pi p}\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,p=1,2,...,N\\]\nPara \\(p=0\\) entonces\n\\[r_0=\\frac{1}{\\omega_N}\\int\\limits^{\\omega_c}_0 R(\\omega_p) cos(\\omega_p \\Delta t\n)d{\\omega}=\\frac{1}{\\omega_N}\\int\\limits^{\\omega_c}_0 d{\\omega}=\\frac{\\omega_c}{\\omega_N}\\,.\\]\nY la función respuesta es\n\\[R(\\omega)=\\frac{\\omega_c}{\\omega_N} + 2\\sum\\limits^{\\infty}_{p=1}\n\\frac{sen(\\pi p \\omega_c / \\omega_N)}{\\pi p}cos(\\pi p \\omega / \\omega_N)\\]\nVeamos que forma tiene este filtro en el dominio frecuencial, asumiendo un número finito de \\(N\\) coeficientes de Fourier, i.e., \\(p=1,2,...,N\\), frecuencia de Nyquist \\(f_N=1\\) y frecuencia de corte \\(f_c=1\\):\nEn la figura observamos oscilaciones con longitud de onda \\[\\lambda=\\frac{4f_N}{2N+1}\\,,\\]\nEsta longitud de onda coincide con el ancho de banda de transición del filtro, es decir, del pico hasta la base indicado en la figura por las líneas rojas. Para filtrar únicamente debemos:  \n\nmultiplicar la respuesta espectral \\(R(f)\\) por la transformada de Fourier de la señal y regresar con la transformada inversa\n\n\\[x(t)[filtrado]={\\cal F}^{-1}[R(f)X(f)]\\,,\\]\n(ii) convolucionar la respuesta en el dominio temporal \\(r(t)\\) por la serie temporal. \\[x(t)[{ filtrado}]=[r(t)*x](t)\\,.\\]\n\n{}Si queremos un filtro pasa alta, usamos \\(r_p({ pasa}\\,\\,\\,{ alto})=1-r_p\\). Y la función respuesta sería \\[R(\\omega)[{ pasa}\\,\\,\\,{ alto}]=1-R(\\omega)=1-\\frac{\\omega_c}{\\omega_N} - 2\\sum\\limits^{\\infty}_{p=1}\n\\frac{sen(\\pi p \\omega_c / \\omega_N)}{\\pi p}cos(\\pi p \\omega / \\omega_N)\\]\nVeamos ahora de nuevo el promedio corrido pero esta vez usando el método de Fourier. De nuevo decir que el promedio corrido reemplaza el valor central de la ventana por el promedio de los valores que rodean a ese punto. Para este ejemplo los pesos son siempre iguales \\(r_p=1/T\\) para el intervalo \\(-N&lt;p&lt;N\\), donde \\(T=1/(2N+1)\\) es el tamaño de la ventana `boxcar’. De esta forma  \n\\[T=2N+1=3\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\nR(\\omega)=\\frac{1}{3} + \\frac{2}{3}cos(\\omega \\Delta t)\n\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,0&lt;\\omega&lt;\\pi/\\Delta t\\]\n\\[T=2N+1=5\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\nR(\\omega)=\\frac{1}{5} + \\frac{2}{5}cos(\\omega \\Delta t)+ \\frac{2}{5}cos(2\\omega \\Delta t)\n\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,0&lt;\\omega&lt;\\pi/\\Delta t\\]\n\\[T=2N+1=7\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\nR(\\omega)=\\frac{1}{7} + \\frac{2}{7}cos(\\omega \\Delta t)+ \\frac{2}{7}cos(2\\omega \\Delta t)+ \\frac{2}{7}cos(3\\omega \\Delta t)\n\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,0&lt;\\omega&lt;\\pi/\\Delta t\\]\nComo volvemos a observar, las transformadas de Fourier de funciones de peso cuadradas (`boxcar’) no son adecuadas debido a las oscilaciones o lóbulos menores.\n\n\nFiltro Lanczos pasabaja\nSea \\(R(f)\\) la función respuesta de un filtro pasa baja, en donde \\(f\\) corresponde a la frecuencia en ciclos por unidad de tiempo, \\(f_N\\) es la frecuencia de Nyquist, y \\(fc\\) es la frecuencia de corte.\nLanzcos se dio cuenta que las oscilaciones de los filtros cosinusoidales con longitud de onda \\(\\lambda(f)=4f_N/2N+1\\) podían ser reducidas si se realiza un suavizado de la fucnión respuesta \\(H(f)\\). Para ello realizó un promedio corrido de tamaño igual a la longitud de onda de las oscilaciones, es decir, \\(\\lambda\\). Esto se puede escribir como \\[\\widetilde{R}(f)=\\frac{1}{\\lambda(f)}\\int\\limits^{f+\\lambda/2}_{f-\\lambda/2} R(f) d{f}\\,,\\] donde ya hemos visto que \\[R(f)=\\frac{f_c}{f_N} + 2\\sum\\limits^{N}_{p=1}\nr_p cos(\\pi p f / f_N)\\,.\\]\nUn filtro de media corrida no tiene efecto en el promedio, entonces \\[\\widetilde{R}(f)=\\frac{f_c}{f_N} + \\frac{1}{\\lambda}\n\\int\\limits^{f+\\lambda/2}_{f-\\lambda/2} 2 \\sum\\limits^{N}_{p=1}\nr_pcos(\\pi p f / f_N)d{f}=\\]\n\\[=\\frac{f_c}{f_N} + \\frac{2}{\\lambda}\\sum\\limits^{N}_{p=1} r_p\n\\left[\\frac{1}{\\pi p /f_N} sen\\left( \\frac{\\pi p\nf}{f_N}\\right)\\right]^{f+\\lambda/2}_{f-\\lambda/2}=\\]\n\\[=\\frac{f_c}{f_N}+ \\frac{2}{\\lambda}\\sum\\limits^{N}_{p=1}\nr_p\\frac{f_N}{\\pi p}\\left[sen\\left( \\frac{\\pi p\n(f+\\lambda/2)}{f_N}\\right)-sen\\left( \\frac{\\pi p\n(f-\\lambda/2)}{f_N}\\right) \\right]=\\]\n\\[=\\frac{f_c}{f_N}+ \\frac{2}{\\lambda}\\sum\\limits^{N}_{p=1}\nr_p\\frac{f_N}{\\pi p}\\left[2cos\\left( \\frac{\\pi p\nf}{f_N}\\right)sen\\left( \\frac{\\lambda \\pi p}{2f_N}\\right) \\right]=\n\\frac{f_c}{f_N}+ \\frac{2}{\\lambda}\\sum\\limits^{N}_{p=1}\nr_p\\frac{f_N}{\\pi p}\\left[2cos\\left( \\frac{\\pi p\nf}{f_N}\\right)sen\\left( \\frac{2 \\pi p}{2N+1}\\right) \\right]=\\]\n\\[=\\frac{f_c}{f_N}+ \\frac{2(2N+1)}{4f_N}\\sum\\limits^{N}_{p=1}\nr_p\\frac{f_N}{\\pi p}\\left[2cos\\left( \\frac{\\pi p\nf}{f_N}\\right)sen\\left( \\frac{2 \\pi p}{2N+1}\\right) \\right]=\\]\n\\[=\\frac{f_c}{f_N}+ 2\\sum\\limits^{N}_{p=1}\nr_p\\frac{2N+1}{2 \\pi p}\\left[cos\\left( \\frac{\\pi p\nf}{f_N}\\right)sen\\left( \\frac{2 \\pi p}{2N+1}\\right) \\right]=\\]\n\\[=\\frac{f_c}{f_N}+ 2\\sum\\limits^{N}_{p=1}\nr_p\\frac{1}{\\frac{2 \\pi p}{2N+1}}\\left[cos\\left( \\frac{\\pi p\nf}{f_N}\\right)sen\\left( \\frac{2 \\pi p}{2N+1}\\right) \\right]=\n\\frac{f_c}{f_N}+ 2\\sum\\limits^{N}_{p=1}\nr_p \\sigma_p cos\\left( \\frac{\\pi p\nf}{f_N}\\right)\\,,\\]\ndonde\n\\[\\sigma_p=\\frac{sen\\left( \\frac{2 \\pi p}{2N+1}\\right)}{\\frac{2 \\pi p}{2N+1}}= sinc\\left(\\frac{2 \\pi p}{2N+1} \\right)\\,,\\]\nes una función sinc como la respuesta espectral de un filtro rectangular o boxcar. A este factor de suavizado se le suele llamar peso sigma.\nEn la figura se observa que la frecuencia de corte es la la frecuencia que corta el 50% de la magnitud de la respuesta. La frecuencia efectiva es \\(f=f_c+\\lambda/2\\).\n\n\nFiltro Lanczos pasabanda\nEn el dominio de las frecuencias, el filtro de pasa-banda se obtiene convolucionando el filtro pasa-bajas con la transformada de Fourier de la función coseno: \\[\\widetilde{R}_b(f)=\\widetilde{R}(f)*\\left[\\delta(f-f_o) + \\delta(f+f_0) \\right]\\,,\\] donde * significa convolución, \\[{\\cal F}[cos(2\\pi f_0 x)](f)=\\int\\limits^{\\infty}_{-\\infty} e^{-2\\pi i f x} cos(2\\pi\nf_0 x) dx=\\int\\limits^{\\infty}_{-\\infty} e^{-2\\pi i f x}\n\\left(\\frac{e^{2\\pi i f_0 x} +e^{-2\\pi i f_0 x}}{2}\\right) dx = \\] \\[=\\frac{1}{2}\\int\\limits^{\\infty}_{-\\infty}\\left[e^{-2\\pi i (f-f_0) x} + e^{-2\\pi i (f+f_0) x} \\right] dx\\] \\[=\\frac{1}{2}\\left[ \\delta(f-f_0) + \\delta(f+f_0)\\right]\\,,\\] y la delta de dirac se define como \\[\\delta(x)={\\cal F}[1](f\\pm f_0)=\\int\\limits^{\\infty}_{-\\infty} e^{-2\\pi i (f\\pm f_0) x} dx\\]\nVemos que la transformada de Fourier del coseno se ha multiplicado por un factor de 2 para que la respuesta del filtro sea unitaria (normalización). El resultado de la convolución es\n\\[\\widetilde{R}_b(f)=\\widetilde{R}(f)*\\left[\\delta(f-f_o) + \\delta(f+f_0) \\right]=\n\\widetilde{R}(f-f_0)+\\widetilde{R}(f+f_0)=\\]\n\\[=\\frac{f_c}{f_N}+ 2\\sum\\limits^{N}_{p=1}\n{r_p} \\sigma_p cos\\left( \\frac{\\pi p (f-f_0)}{f_N}\\right) +\n\\frac{f_c}{f_N}+ 2\\sum\\limits^{N}_{p=1}\n{r_p} \\sigma_p cos\\left( \\frac{\\pi p (f+f_0)}{f_N}\\right)=\\]\n\\[=2\\frac{f_c}{f_N}+ 2\\sum\\limits^{N}_{p=1}\n{r_p} \\sigma_p \\left[ cos\\left( \\frac{\\pi p f}{f_N} - \\frac{\\pi p f_0}{f_N}\\right) +\ncos\\left( \\frac{\\pi p f}{f_N} + \\frac{\\pi p f_0}{f_N}\\right)\\right]=\\]\n\\[=2\\frac{f_c}{f_N}+ 2\\sum\\limits^{N}_{p=1} r_p \\sigma_p\\left[\n2cos\\left( \\frac{\\pi p f}{f_N}\\right) cos\\left( \\frac{\\pi p f_0}{f_N}\\right)\\right]=\\]\n\\[=2\\frac{f_c}{f_N}+ 4\\sum\\limits^{N}_{p=1} r_p \\sigma_p\\left[\ncos\\left( \\frac{\\pi p f}{f_N}\\right) cos\\left( \\frac{\\pi p f_0}{f_N}\\right)\\right]\\]\nPara el filtro pasa bajas: \\[x(t)[{ filtrado}]=[r_p \\sigma_p * x](t)\\,,\\] o \\[x(t)[{ filtrado}]={\\cal F}^{-1}[\\widetilde{R}(f)X(f)]\\,.\\]\nPara el filtro pasa-banda: \\[x(t)[{ filtrado}]=[r_p \\sigma_p cos\\left( \\frac{\\pi p f_0}{f_N}\\right)*x](t)\\,,\\] o \\[x(t)[{ filtrado}]={\\cal F}^{-1}[\\widetilde{R}_b(f)X(f)]\\,.\\]\nTodo esto se puede programar facilmente en Matlab como se muestra en el siguiente:\n% OUTPUT\\\nY una vez tenemos los coeficientes solamente tenemos que convolucionar o multiplicar:\nAqui se muestra un ejemplo sintético del uso de las subrutinas presentadas."
  },
  {
    "objectID": "article2_new.html#temas-selectos",
    "href": "article2_new.html#temas-selectos",
    "title": "Julio Sheinbaum",
    "section": "Temas selectos",
    "text": "Temas selectos\n\nAnálisis Armónico\nSe trata de un ajuste por mínimos cuadrados de una serie temporal dominada por armónicos específicos. Por ejemplo, en el caso del océano, es muy comúnencontrar en series temporales de temperatura, salinidad, velocidad, etc.. señales de las mareas que no son nada mas que corrientes periódicas generadas por fuerzas astronómicas con una frecuencia de oscilación determinada (24h, 12h, etc…).\nEl método consiste en elejir las frecuencias de los armónicos y usar cuadrados mínimos para ajustarlos a la serie temporal. Supongamos \\(M\\) armónicos a ajustar\n\\[y(t_n) = \\overline{y(t)} + \\sum\\limits^{M}_{q=1}C_q cos(\\omega_q t_n -\\theta_q)+y_r(t_n)\\,,\\]\ndonde \\(\\overline{y(t)}\\) es el promedio de la serie, y \\(y_r(t_n)\\) es el residuo de la serie temporal (donde hay el resto de armónicos presentes en la serie), \\(\\omega_q=2\\pi q/N\\Delta t\\). En términos de las amplitudes \\(A_q\\) y \\(B_q\\)\n\\[y(t_n) = \\overline{f(t)} + \\sum\\limits^{M}_{q=1}[A_q cos(\\omega_q t_n)+B_q sen(\\omega_q t_n)] +y_r(t_n)\\,,\\] donde \\[C_q=\\sqrt{A_q^2+B_q^2}\\,,\\,\\,\\,\\,q=1,2,....\\] \\[\\theta_q=arctg[B_q/A_q]\\,,\\,\\,\\,\\,q=1,2,...\\]\nAntes de empezar el análisis debemos de extraer la media, \\(\\overline{y}\\), a la serie temporal. El método de mínimos cuadrados consiste en minimizar la suma de los errores cuadrados \\(SEC\\), es decir, \\[SEC= \\sum\\limits^N_{n=1} y_r^2(t_n) = \\sum\\limits^N_{n=1}  \\left( y(t_n) - \\left[ \\overline{y(t)} +\n\\sum\\limits^{M}_{q=1} A_q cos(\\omega_q t_n)+B_q sen(\\omega_q t_n) \\right] \\right)^2 =\\] \\[=\\sum\\limits^N_{n=1}  \\left(y(t_n) - \\left[ \\overline{y(t)} +\n\\sum\\limits^{M}_{q=1} A_q cos(2\\pi q n/ N)+B_q sen(2\\pi q n/ N) \\right] \\right)^2\\]\nComo siempre derivamos respecto los coeficientes e igualamos a cero para obtener un sistema de \\(2M+1\\) equaciones \\[\\frac{\\partial{SEC}}{\\partial{A_q}}=0=2\\sum\\limits^N_{n=1} \\left(y(t_n) - \\left[ \\overline{y(t)} +\n\\sum\\limits^{M}_{q=1} A_q cos(2\\pi q n/ N)+B_q sen(2\\pi q n/ N) \\right]-cos(2\\pi q n/ N) \\right)\\] \\[\\frac{\\partial{SEC}}{\\partial{B_q}}=0=2\\sum\\limits^N_{n=1} \\left(y(t_n) - \\left[ \\overline{y(t)} +\n\\sum\\limits^{M}_{q=1} A_q cos(2\\pi q n/ N)+B_q sen(2\\pi q n/ N) \\right]-sen(2\\pi q n/ N) \\right)\\]\nSoluciones del sistema requiere una equación matricial de la forma \\({\\textbf D}{\\textbf z}={\\textbf y}\\), donde \\[{\\textbf D}=\\left( \\begin{array}{cccccccccccccc}\n  N & c_1 & c_2 & ... & c_M & s_1 & s_2 & ... & s_M \\\\\n  c_1 & cc_{11} & cc_{12} & ... & cc_{1M} & cs_{11} & cs_{12} & ... & cs_{1M} \\\\\n  c_2 & cc_{21} & cc_{22} & ... & cc_{2M} & cs_{21} & cs_{22} & ... & cs_{2M} \\\\\n  ... & ... & ... & ... & ... & ... & ... & ... & ... \\\\\n  ... & ... & ... & ... & ... & ... & ... & ... & ... \\\\\n  c_M & cc_{M1} & cc_{M2} & ... & cc_{MM} & cs_{M1} & cs_{M2} & ... & cs_{MM} \\\\\n  s_1 & sc_{11} & sc_{12} & ... & sc_{1M} & ss_{11} & ss_{12} & ... & ss_{1M} \\\\\n  s_2 & sc_{21} & sc_{22} & ... & sc_{2M} & ss_{21} & ss_{22} & ... & ss_{2M} \\\\\n  ... & ... & ... & ... & ... & ... & ... & ... & ... \\\\\n  ... & ... & ... & ... & ... & ... & ... & ... & ... \\\\\n  s_M & sc_{M1} & sc_{M2} & ... & sc_{MM} & ss_{M1} & ss_{M2} & ... & ss_{MM} \\\\\n      \\end{array} \\right)\\] y\n\\[{\\textbf y}=\\left( \\begin{array}{ccc}\nyc_0 \\\\\nyc_1 \\\\\nyc_2 \\\\\n... \\\\\n... \\\\\nyc_M \\\\\nys_1 \\\\\nys_2 \\\\\n... \\\\\n... \\\\\nys_M \\\\\n\\end{array} \\right)\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\n{\\textbf z}=\\left( \\begin{array}{ccc}\nA_0\\\\\nA_1\\\\\nA_2\\\\\n...\\\\\n...\\\\\nA_M\\\\\nB_1\\\\\nB_2\\\\\n...\\\\\n...\\\\\nB_M\n\\end{array} \\right)\\]\nLos coeficientes de las matrices son:\n\\[yc_i=\\sum\\limits^N_{n=1}y(t_n)cos(\\omega_i t_n)\\,\\,\\,\\,\\,,\\,\\,\\,\\,\\,ys_i=\\sum\\limits^N_{n=1}y(t_n)sen(\\omega_i t_n)\\] \\[c_i=\\sum\\limits^N_{n=1}cos(\\omega_i t_n)\\,\\,\\,\\,\\,,\\,\\,\\,\\,\\,s_i=\\sum\\limits^N_{n=1}sen(\\omega_i t_n)\\] \\[cc_{ij}=cc_{ji}=\\sum\\limits^N_{n=1}[cos(\\omega_i t_n)cos(\\omega_j t_n)]\\] \\[ss_{ij}=ss_{ji}=\\sum\\limits^N_{n=1}[sen(\\omega_i t_n)sen(\\omega_j t_n)]\\] \\[cs_{ij}=sc_{ji}=\\sum\\limits^N_{n=1}[cos(\\omega_i t_n)sen(\\omega_j t_n)]\\,,\\] donde \\(t_n= n \\Delta\\), \\(\\omega_i = 2\\pi f_i\\) es la frecuencia angular de las componentes de interés \\(i\\), y \\(\\phi_i(n)=\\omega_i t_n\\) es el argumento de las funciones de Fourier.\n{Ejemplo de Ajuste de armónicos (Emery and Thompson, p395)}\nAsumamos la siguiente serie temporal de temperatura promedia mensual. \\\n\\ \\\nDeseamos encontrar las componentes mareales dominantes en la serie temporal de temperatura. A simple vista podemos ver que existe una frecuencia dominante semianual. Por tanto, vamos buscar las amplitudes y frecuencias de interés, es decir, de las componentes anual y semianual que tienen unas frecuencias de \\(f_1=1/12\\) meses (\\(=0.0833\\,{ cpm}\\)) y \\(f_2=1/24\\) meses (\\(=0.1667\\,{ cpm}\\)). Los argumentos de las funciones de Fourier son \\(\\phi_1(n)=\\omega_1 t_n=2\\pi(1/12)*n*\\Delta t=(\\pi/6)*n*1=n\\pi/6\\) y \\(\\phi_2(n)=\\omega_2 t_n=2\\pi(1/6)*n*\\Delta t=(\\pi/3)*n*1=n\\pi/3\\) Para este problema las matrices son \\\n\\[{\\textbf D}=\\left( \\begin{array}{ccccc}\n  N & c_1 & c_2 & s_1 & s_2 \\\\\n  c_1 & cc_{11} & cc_{12} & cs_{11} & cs_{12}\\\\\n  c_2 & cc_{21} & cc_{22} & cs_{21} & cs_{22} \\\\\n  s_1 & sc_{11} & sc_{12} & ss_{11} & ss_{12} \\\\\n  s_2 & sc_{21} & sc_{22} & ss_{21} & ss_{22} \\\\\n      \\end{array} \\right)=\\] \\[=\\scriptsize\n\\left( \\begin{array}{ccccc}\nN & c_1 & c_2 & s_1 & s_2 \\\\\n  c_1 & \\sum\\limits^N_{n=1}[cos(\\phi_1(n))cos(\\phi_1(n))] & cc_{12} & cs_{11} & cs_{12}\\\\\n  c_2 & cc_{21} & \\sum\\limits^N_{n=1}[cos(\\phi_1(n))cos(\\phi_2(n))] & cs_{21} & cs_{22} \\\\\n  s_1 & sc_{11} & sc_{12} & \\sum\\limits^N_{n=1}[sen(\\phi_1(n))sen(\\phi_1(n))] & ss_{12} \\\\\n  s_2 & sc_{21} & sc_{22} & ss_{21} & \\sum\\limits^N_{n=1}[sen(\\phi_2(n))sen(\\phi_2(n))] \\\\\n      \\end{array} \\right) =\n      \\] \\[\n=\n\\left( \\begin{array}{ccccc}\n  24 & 0 & 0 & 0 & 0 \\\\\n  0 & 12 & 0 & 0 & 0\\\\\n  0 & 0 & 12 & 0 & 0 \\\\\n  0 & 0 & 0 & 12 & 0 \\\\\n  0 & 0 & 0 & 0 & 12 \\\\\n      \\end{array} \\right)\n\\]\nLa matriz \\[{\\textbf y}=\\left( \\begin{array}{ccc}\n\\sum\\limits^N_{n=1}y(t_n)cos(\\omega_0 t_n) \\\\\n\\sum\\limits^N_{n=1}y(t_n)cos(\\omega_1 t_n) \\\\\n\\sum\\limits^N_{n=1}y(t_n)cos(\\omega_2 t_n) \\\\\n\\sum\\limits^N_{n=1}y(t_n)sen(\\omega_1 t_n) \\\\\n\\sum\\limits^N_{n=1}y(t_n)sen(\\omega_2 t_n) \\\\\n\\end{array} \\right)=\n\\left( \\begin{array}{ccc}\n262.5 \\\\\n-21.45 \\\\\n-5.4 \\\\\n-23.76 \\\\\n-0.51 \\\\\n\\end{array} \\right)\\,\\,^\\circ{C}\\]\nFinalmente encontramos las amplitudes de los armónicos resolviendo el sistema \\[{\\textbf z}=({\\textbf D}^T{\\textbf D})^{-1}{\\textbf D}^{T}{\\textbf y}={\\textbf D}^{-1}{\\textbf y}=\\left( \\begin{array}{ccc}\n10.93\\\\\n-1.78\\\\\n-0.45\\\\\n-1.98\\\\\n-0.04\n\\end{array} \\right)\\]\nEl coeficiente de correlación entre la señal original y la serie de Fourier con 2 armónicos es \\(r^2=0.92\\), es decir, solamente con 2 armónicos podemos explicar el 92% de la varianza total.\n\n\nDemodulación compleja\nEste método es utilizado para conocer el comportamiento de una componente o armónico con frecuencia particular \\(\\omega\\), tal como la marea diurna, o semidiurna, o las ondas inerciales. Aqui vamos a mostrar la forma clásica de demodular que consiste en ajustar por fragmentos de la serie un armónico teórico utilizando mínimos cuadrados. Cada fragmento de la serie debe, como mínimo, contener un ciclo del armónico a demodular. Para cada segmento, la anomalía de la componente de velocidad a la frecuencia de interés \\(\\omega\\) es \\[{\\textbf u} - \\overline{\\textbf u}=[u(t)-\\overline{u(t)} +iv(t)-\\overline{v(t)}]=\\] \\[R^+ e^{i(\\omega t + \\epsilon^+)} + R^- e^{-i(\\omega t + \\epsilon^-)}\\,,\\] donde \\(\\overline{u(t)}\\), \\(\\overline{v(t)}\\) son las componentes de la velocidad promedio, \\(R^+,\\,\\,R^-\\) y \\(\\epsilon^+\\,\\,\\epsilon^-\\) son las amplitudes y fases de las componentes rotatorias que giran en el sentido de las agujas del reloj (+) y en el sentido contrario (-). La serie temporal esta definida para cada \\(t_k\\,(k=1,2,....,N)\\) y las soluciones son encontradas resolviendo el sistema de ecuaciones \\[{\\textbf z}={\\textbf D}^{-1}{\\textbf y}\\,,\\] donde \\[{\\textbf y}=\\left( \\begin{array}{c}\nu(t_1) \\\\\nu(t_2) \\\\\n... \\\\\nu(t_n) \\\\\nv(t_1) \\\\\nv(t_2) \\\\\n... \\\\\nv(t_n) \\\\\n\\end{array} \\right)\\,;\\,\\,\\,\n{\\textbf z}=\\left( \\begin{array}{c}\nR^+cos(\\epsilon^+) \\\\\nR^+sen(\\epsilon^+)\\\\\nR^-cos(\\epsilon^-) \\\\\nR^-sen(\\epsilon^-)\\\\\n\\end{array} \\right)=\n\\left( \\begin{array}{c}\nACP \\\\\nASP\\\\\nACM \\\\\nASM\\\\\n\\end{array} \\right)\\,,\\] y la matriz \\({\\textbf D}\\) es \\[{\\textbf D}=\\left( \\begin{array}{cccc}\ncos(\\omega t_1) & -sen(\\omega t_1) & cos(\\omega t_1) & sen(\\omega t_1) \\\\\ncos(\\omega t_2) & -sen(\\omega t_2) & cos(\\omega t_2) & sen(\\omega t_2) \\\\\n... \\\\\ncos(\\omega t_n) & -sen(\\omega t_n) & cos(\\omega t_n) & sen(\\omega t_n) \\\\\nsen(\\omega t_1) & cos(\\omega t_1) & -sen(\\omega t_1) & cos(\\omega t_1) \\\\\nsen(\\omega t_2) & cos(\\omega t_2) & -sen(\\omega t_2) & cos(\\omega t_2) \\\\\n... \\\\\nsen(\\omega t_n) & cos(\\omega t_n) & -sen(\\omega t_n) & cos(\\omega t_n) \\\\\n\\end{array} \\right)\\,.\\]\nUna vez los valores de \\({\\textbf z}\\) son encontrados a partir de la solución de mínimos cuadrados de arriba, podemos encontrar los parámetros de la elipse como: \\[R^+=\\sqrt{\\left( ASP^2 + ACP^2\\right)}\\,;\\,\\,\\,\\,\\,R^-=\\sqrt{\\left( ASM^2 + ACM^2\\right)}\\] \\[\\epsilon^+=tan^{-1}\\left(\\frac{ASP}{ACP}\\right)\\,;\\,\\,\\,\\,\\,\\epsilon^-=tan^{-1}\\left(\\frac{ASM}{ACM}\\right)\\]\nPor ejemplo, si queremos demodular la amplitud y fase de las ondas inerciales observadas en un anclaje situado en latitudes medias, debemos de usar una frecuencia \\(\\omega=2\\Omega sen\\phi\\) y ajustar por mínimos cuadrados segmentos de \\(24\\,{ h}\\) sin superposición. La serie temporal medida por el anclaje debería de ser horaria para que existan mas datos por segmento que parámetros a ajustar.\nOtra forma, tal vez mas sencilla, es la siguiente. Imaginemos que la serie original es \\(X(t)\\) y se asume como una señal periódica con frecuencia igual a la de interés mas otras cosas que llamamos \\(Z(t)\\) \\[X(t)=A(t)cos\\left(\\omega t + \\varphi(t)\\right) + Z(t)= \\frac{1}{2}A(t)\n       \\left[e^{i(\\omega t + \\varphi(t))} + e^{-i(\\omega t + \\varphi(t))} \\right] + Z(t)\\,,\\] donde la amplitud \\(A(t)\\) y la fase \\(\\varphi(t)\\) de la señal periódica se asumen que dependen del tiempo pero que varían “lentamente” en comparación a la frecuencia \\(\\omega\\).\nPara demodular tenemos que: \\\n\nMultiplicar \\(X(t)\\) por \\(e^{-i\\omega t}\\): \\[Y(t)=X(t)e^{-i\\omega t}=\\frac{1}{2}A(t)\n   \\left[e^{i(\\omega t + \\varphi(t))} + e^{-i(\\omega t + \\varphi(t))} \\right]e^{-i\\omega t} + Z(t)e^{-i\\omega t}=\\] \\[=\\frac{1}{2}A(t)e^{i(\\omega t + \\varphi(t))}e^{-i\\omega t} + \\frac{1}{2}A(t)e^{-i(\\omega t + \\varphi(t))}e^{-i\\omega t} +\nZ(t)e^{-i\\omega t}=\\] \\[=\\underbrace{\\frac{1}{2}A(t)e^{i\\varphi(t)}}_{(a)} + \\underbrace{\\frac{1}{2}A(t)e^{-i(2\\omega t + \\varphi(t))}}_{(b)} +\n\\underbrace{Z(t)e^{-i\\omega t}}_{(c)}\\,.\\]\n\nEl término (a) varía lentamente ya que \\(\\varphi(t)\\) también lo hace y no tiene energía (potencia espectral) a la frecuencia de demodulación \\(\\omega\\) o arriba de ella. El término (b) oscila a dos veces la frecuencia de demodulación, i.e., \\(2\\omega\\). El término (c) varía a la frecuencia \\(\\omega\\). Debido a que \\(Z(t)\\) no tiene energía a la frecuencia \\(\\omega\\), entonces el término (c) no tendrá tampoco energía en la frecuencia cero, i.e., \\(\\omega=0\\).\\\n\nFiltro pasa-bajas de la serie \\(Y(t)\\) para eliminar las ondas con frecuencia \\(\\omega\\) o por encima de \\(\\omega\\). Esto eliminará prácticamente los términos (b) y (c), y suavizará (a). El resultado es \\[Y_s(t)=\\frac{1}{2}A_s(t)e^{i\\varphi_s(t)}\\,,\\] donde el subíndice \\(_s\\) significa suavizado o filtro pasa-bajas. \\\nExtraer \\(A_s(t)\\) y \\(\\varphi_s(t)\\): \\[\\frac{1}{2}A_s(t)=|Y_s(t)|=2\\left( Re[Y_s]^2 + Im[Y_s]^2\\right)^{1/2}\\] \\[e^{i\\varphi(t)}=2\\frac{Y_s(t)}{A_s(t)}\\,;\\,\\,\\,\\,\\,\\varphi_s(t)=tan^{-1}\\left(\\frac{Im[Y_s]}{Re[Y_s]}\\right)\\]\n\nAl suavizar hacemos dos cosas. Primero, eliminamos los términos no deseados (a) y (b). El tipo de filtrado o suavizado determina la anchura de la banda de frecuencias de las oscilaciones retenidas. Por ejemplo, si usamos un triángulo (ventana triangular) de longitud \\(2T-1\\) donde \\(T=2\\pi/\\omega\\) es el periodo de demodulación, entonces para la banda para la potencia-media (\\(3\\,{ dB}\\) desde el pico) será \\(\\omega \\in [T/(1+0.44295),T/(1-0.44295)]\\). Potencia media se refiere a la frecuencia a la cual la potencia se ha reducido a la mitad de su valor medio de la banda. Segundo, el filtrado suaviza las series de amplitud y la fase. \\\n\nLa elección de la frecuencia de demodulación \\(\\omega\\) se puede validar ajustando localmente una línea a la fase,\\(\\varphi\\simeq a + bt\\). Típicamente esto lo haremos en fragmentos de longiutd \\(T\\). De esta forma si seleccionamos el origen en el tiempo central de cada fragmento (tal que \\(a\\simeq 0\\)) obtenemos que \\(cos(\\omega t + \\varphi)\\simeq cos(\\omega t + bt)=cos(\\hat{\\omega} t)\\).\n\nLa frecuencia ajustada \\(\\hat{\\omega}=\\omega + a\\) es una validación de la elección inicial de nuestra frecuencia de demodulación \\(\\omega\\)."
  },
  {
    "objectID": "article2_new.html#acknowledgments",
    "href": "article2_new.html#acknowledgments",
    "title": "Julio Sheinbaum",
    "section": "Acknowledgments",
    "text": "Acknowledgments\nAyuda de Muchos"
  },
  {
    "objectID": "article2_new.html#open-research",
    "href": "article2_new.html#open-research",
    "title": "Julio Sheinbaum",
    "section": "Open research",
    "text": "Open research\nDisponible para todos"
  },
  {
    "objectID": "article2_new.html#references",
    "href": "article2_new.html#references",
    "title": "Julio Sheinbaum",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "chap2.html#empieza",
    "href": "chap2.html#empieza",
    "title": "Test Reveal Presentations",
    "section": "Empieza",
    "text": "Empieza\n\nTurn on Math\n\\(Ax=b\\)\nCauchy-Shwarz\n\n\\[\\left( \\sum_{k=1}^n a_k b_k \\right)^2 \\leq \\left( \\sum_{k=1}^n a_k^2 \\right) \\left( \\sum_{k=1}^n b_k^2 \\right)\\]"
  },
  {
    "objectID": "chap2.html#more-math",
    "href": "chap2.html#more-math",
    "title": "Test Reveal Presentations",
    "section": "More Math",
    "text": "More Math\n\n\n\nA Cross Product Formula\n\n\\[\\mathbf{V}_1 \\times \\mathbf{V}_2 =  \\begin{vmatrix}\n  \\mathbf{\\hat i} &  \\mathbf{\\hat j} & \\mathbf{\\hat k} \\\\\n  \\frac{\\partial X}{\\partial u} &  \\frac{\\partial Y}{\\partial u} & 0 \\\\\n  \\frac{\\partial X}{\\partial v} &  \\frac{\\partial Y}{\\partial v} & 0\n  \\end{vmatrix}  \\]\n\n\nThe Lorenz Equations \\[\n\\begin{aligned}\n\\dot{x} & = \\sigma(y-x) \\\\\n\\dot{y} & = \\rho x - y - xz \\\\\n\\dot{z} & = -\\beta z + xy\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "chap2.html#julia",
    "href": "chap2.html#julia",
    "title": "Test Reveal Presentations",
    "section": "Julia",
    "text": "Julia\nExample taken from https://docs.makie.org/stable/\n#| echo: true\nusing GLMakie\n\nBase.@kwdef mutable struct Lorenz\n    dt::Float64 = 0.01\n    σ::Float64 = 10\n    ρ::Float64 = 28\n    β::Float64 = 8/3\n    x::Float64 = 1\n    y::Float64 = 1\n    z::Float64 = 1\nend\n\nfunction step!(l::Lorenz)\n    dx = l.σ * (l.y - l.x)\n    dy = l.x * (l.ρ - l.z) - l.y\n    dz = l.x * l.y - l.β * l.z\n    l.x += l.dt * dx\n    l.y += l.dt * dy\n    l.z += l.dt * dz\n    Point3f(l.x, l.y, l.z)\nend\n\nattractor = Lorenz()\n\npoints = Observable(Point3f[])\ncolors = Observable(Int[])\n\nset_theme!(theme_black())\n\nfig, ax, l = lines(points, color = colors,\n    colormap = :inferno, transparency = true,\n    axis = (; type = Axis3, protrusions = (0, 0, 0, 0),\n        viewmode = :fit, limits = (-30, 30, -30, 30, 0, 50)))\n\nrecord(fig, \"lorenz.mp4\", 1:120) do frame\n    for i in 1:50\n        push!(points[], step!(attractor))\n        push!(colors[], frame)\n    end\n    ax.azimuth[] = 1.7pi + 0.3 * sin(2pi * frame / 120)\n    notify.((points, colors))\n    l.colorrange = (0, frame)\nend"
  },
  {
    "objectID": "chap2.html#lorenz-atractor",
    "href": "chap2.html#lorenz-atractor",
    "title": "Test Reveal Presentations",
    "section": "Lorenz Atractor",
    "text": "Lorenz Atractor"
  },
  {
    "objectID": "chap2.html#polar-axis",
    "href": "chap2.html#polar-axis",
    "title": "Test Reveal Presentations",
    "section": "Polar Axis",
    "text": "Polar Axis\nFor a demonstration of a line plot on a polar axis, see Figure 1.\n\n\n\n\n\n\n\n\nFigure 1: A line plot on a polar axis"
  },
  {
    "objectID": "chap2.html#python",
    "href": "chap2.html#python",
    "title": "Test Reveal Presentations",
    "section": "Python",
    "text": "Python\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nr = np.arange(0, 2, 0.01)\ntheta = 2 * np.pi * r\nfig, ax = plt.subplots(subplot_kw={'projection': 'polar'})\nax.plot(theta, r)\nax.set_rticks([0.5, 1, 1.5, 2])\nax.grid(True)\nplt.show()"
  },
  {
    "objectID": "chap2.html#gfs-download-and-plot",
    "href": "chap2.html#gfs-download-and-plot",
    "title": "Test Reveal Presentations",
    "section": "GFS Download and Plot",
    "text": "GFS Download and Plot\n:::: {.columns}\n::: {.column width=“60%”}\n\n#| label: xarray plot\n#| fig-cap: \"xarray demo\"\nimport xarray as xr\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport cartopy.crs as ccrs\nfrom matplotlib.animation import FuncAnimation\nvariables=['u-component_of_wind_height_above_ground','v-component_of_wind_height_above_ground']\ndsw=xr.open_dataset('https://thredds.ucar.edu/thredds/dodsC/grib/NCEP/GFS/Global_0p25deg/Best')[variables]\nfrom datetime import datetime, timedelta\nstarttime=datetime.utcnow()\nstarttime\ninittime = datetime.utcnow().date().isoformat()   ### Simulation startime..\nendtime = starttime + timedelta(days=10)\nfinaltime=endtime.date().isoformat()\nprint(inittime)\nprint(finaltime)\nlat_toplot = np.arange(5, 35.25, 0.25) # last number is exclusive\nlon_toplot = np.arange(260, 310.25, 0.25) # last number is exclusive\ndataw= dsw.sel(time1=slice(inittime,finaltime),height_above_ground2=10, lon=lon_toplot, lat=lat_toplot)\nu10=dataw['u-component_of_wind_height_above_ground'].values\nv10=dataw['v-component_of_wind_height_above_ground'].values\nlon=dataw.lon.values\nlat=dataw.lat.values\nl=10\nU10=u10[l,:,:].squeeze()\nV10=v10[l,:,:].squeeze()\nvec_crs = ccrs.RotatedPole(pole_longitude=180.0, pole_latitude=90.0)\n#central_rotated_longitude=0.0)\ndata_crs=ccrs.PlateCarree()\n#print(dataw.time[l])\nfig = plt.figure(figsize=(20, 5))\nax1 = fig.add_subplot(1, 1, 1, projection=ccrs.PlateCarree())\nax1.set_extent([260, 311, 4, 40], crs=ccrs.PlateCarree())\nax1.coastlines()\nmagnitude = (U10 ** 2 + V10 ** 2) ** 0.5\n#magnitude.shape\nax1.streamplot(lon, lat, U10, V10, transform=vec_crs,\n                  linewidth=2, density=2, color=magnitude)\nax1.quiver(lon[::5],lat[::5],U10[::5,::5],V10[::5,::5],scale=200.0,color='b',transform=data_crs)\nplt.savefig('foo.png', bbox_inches='tight')\nplt.show()"
  },
  {
    "objectID": "chap2.html#gfs-download-and-plot-output",
    "href": "chap2.html#gfs-download-and-plot-output",
    "title": "Test Reveal Presentations",
    "section": "GFS Download and Plot",
    "text": "GFS Download and Plot\n\n2024-02-08\n2024-02-18"
  },
  {
    "objectID": "chap2.html#plotly",
    "href": "chap2.html#plotly",
    "title": "Test Reveal Presentations",
    "section": "Plotly",
    "text": "Plotly\n\n\n\nimport plotly.express as px\nimport plotly.io as pio\ndf = px.data.iris()\nfig = px.scatter(df, x=\"sepal_width\", y=\"sepal_length\", \n                 color=\"species\", \n                 marginal_y=\"violin\", marginal_x=\"box\", \n                 trendline=\"ols\", template=\"simple_white\")\nfig.show()\n\n\n\n\n\n\n\nhttps://quarto.org"
  },
  {
    "objectID": "chap2.html#plotly-output",
    "href": "chap2.html#plotly-output",
    "title": "Test Reveal Presentations",
    "section": "Plotly",
    "text": "Plotly"
  },
  {
    "objectID": "chap4.html",
    "href": "chap4.html",
    "title": "Notas del Curso Análisis de Datos",
    "section": "",
    "text": "m– title: “Notas Curso Análisis de datos” format: pdf: agu-pdf: keep-tex: true agu-html: default author: - name: Enric Pallas affiliations: - name: Centro de Investigación Científica y de Educación Superior de Ensenada, CICESE department: Physical Oceanography, address: Carretera Ensenada-Tijuana 3018 city: Ensenada region: Baja California country: MEXICO postal-code: 22860 orcid: 0000-0001-000-000 email: epallas@cicese.mx url: https://cicese.edu.mx/~epallas - name: Julio Sheinbaum affiliations: - name: Centro de Investigación Científica y de Educación Superior de Ensenada, CICESE department: Physical Oceanography, address: Carretera Ensenada-Tijuana 3018 city: Ensenada region: Baja California country: MEXICO postal-code: 22860 orcid: 0000-0001-7031-5225 email: julios@cicese.mx url: https://jsheinbaum.github.io acknowledgements: Translated template to Quarto.\nabstract: | En el océano conviven una gran cantidad de corrientes de diferentes escalas espaciales y temporales. Las escalas espaciales típicas de la circulación oceánica son la larga escala, la mesoescala, submesoescala, y microescala. La larga escala es del \\({\\cal O}(1000\\,km)\\) y esta determinada por la circulación general en el océano como la termohalina y los grandes giros anticiclónicos de las grandes cuencas oceánicas; las escalas temporales de la larga escala varia entre meses y años. La mesoescala esta definida por corrientes del \\({\\cal O}(100\\,km)\\) como remolinos, corrientes costeras, filamentos, frentes, etc. Son corrientes mas regionales pero pueden tener gran influencia sobre la circulación general o de larga escala. Sus escalas temporales son de semanas a meses. La submesoescala corresponde a corrientes del \\({\\cal O}(10\\,km)\\) de caracter local remolinos, filamentos, frentes, corrientes en playas, puertos, y estuarios. La submesoscala varía temporalmente con rapidez en tiempos que varían de horas a días. Finalmente podemos hablar de la microescala que son remolinos del orden de centímetros a metros y generalmente es la escala característica de la turbulencia que transfiere energía desde la submesoescala hacia la disipación molecular. Aquí podemos hablar de fenómenos del orden de segundos y minutos.\nkeywords: [] key-points: - Probar interactividad y teoría - Jupyter,Julia,Pluto. - Vamos viendo. bibliography: bibliography.bib\ncitation: container-title: Geophysical Research Letters keep-tex: true date: last-modified"
  },
  {
    "objectID": "chap4.html#estadística-y-conceptos-de-probabilidad",
    "href": "chap4.html#estadística-y-conceptos-de-probabilidad",
    "title": "Notas del Curso Análisis de Datos",
    "section": "Estadística y conceptos de probabilidad",
    "text": "Estadística y conceptos de probabilidad\nA pesar de nuestra formación determinista a la hora de resolver problemas matemáticos y aunque consideremos que las ecuaciones de Navier-Stokes que describen el movimiento del océano son deterministas, la estadística es ampliamente utilizada en oceanografía debido a diferentes razones:\n\nPara una descripción completa del océano es necesario especificar una gran cantidad de variables, muchas de las cuales son desconocidas. Un ejemplo de ello son las parametrizaciones que se hacen en oceanografía para describir variables que no pueden medirse directamente. Una parametrización no es nadammas que un modelo estadístico que explica la evolución de una variable dependiente de otras variables independientes. Por ejemplo, parametrización del esfuerzo del viento en función del corte vertical o parametrización del coeficiente de arrastre en función de la velocidad del viento a 10m de la superfície del océano.\nEl océano es altamente no lineal. La evolución de una cierta variable no se puede estudiar de forma aislada.\n\nEjemplo:\nSupongamos el término de aceleración horizontal en las ecuaciones de Navier Stokes para fluidos incompresibles,\n\\[\\begin{equation}\n\\frac{\\partial{\\mathbf {u}_h}}{\\partial{t}} + \\mathbf {u}_h \\cdot {\\nabla}_h \\mathbf {{u}_h}\n\\end{equation}\\]\nComo ya sabemos por el curso de Mecánica de Fluidos, la aceleración de un fluido es una derivada material y consta de un término local (aceleración local) y de un término advectivo o aceleración advectiva. En general, esta ecuación no se aplica a partículas de agua individuales.\nEn oceanografía hablamos de continuo. No estamos interesados en las características cinemáticas de las partículas individuales sino en la manifestación promedia del movimiento molecular, es decir, del fluido como un conjunto o contínuo. Es decir, asumimos que el fluido es uniforme en el espacio que ocupa sin considerar la estructura molecular.\nPor ello debemos de promediar de alguna forma para explicar el comportamiento conjunto del fluído y no de una partícula de agua específica? Y como se realiza tal promedio? En general, el promediado se realiza de tal forma que nos permite separar la larga escala que trataremos como determinística, de la pequeña escala que consideramos un proceso aleatorio (turbulento). Supongamos entonces la separación de la velocidad horizontal en una velocidad promedio y una velocidad fluctuante alrededor de la media\n\\[{\\mathbf u_h} = &lt;{\\mathbf u_h}&gt; + {\\mathbf u'_h} \\]\ndonde $ &lt; &gt; $ denotan promedio. Si aplicamos esta descomposición a la componente \\(x\\) de la aceleración obtenemos:\n\\[\\begin{equation}\n\\frac{\\partial &lt;\\mathbf {u}&gt;}{\\partial t} +  &lt;{\\mathbf u}&gt; \\cdot  {\\nabla} &lt;{\\mathbf u} &gt;  + &lt; {\\mathbf u'} \\cdot {\\nabla} \\mathbf {u}'&gt;\n\\end{equation}\\]\n\\[\\begin{equation}\n\\frac{\\partial \\mathbf {&lt; u &gt;}}{\\partial t} + \\nabla \\cdot (&lt;{\\mathbf u}&gt; \\textbf {&lt; u &gt;}) + \\nabla \\cdot&lt; \\mathbf {u'} \\mathbf {u}' &gt;\n\\end{equation}\\]\nDonde usamos la ecuación de continuidad\n\\[ \\nabla \\cdot (&lt; \\mathbf {u} &gt; + \\mathbf {u}') =0 \\]\npara pasar de la primera a la segunda expresión.\n\n\nInevitablemente, las pequeñas escalas o fluctuaciones respecto a la larga escala aparecen en la expresión de la aceleración de larga escala. De forma que la separación que deseamos no es tan simple ya que debemos de conocer la estadística de la pequeña escala para poder describir la circulación media.\nEl término \\(&lt;\\textbf {u}'_h u'&gt;\\) se denomina esfuerzo de Reynolds y nos informa de la correlación entre las componentes fluctuantes (alta frecuencia) de la velocidad. Por ejemplo, \\(&lt;u'v'&gt; = 0\\) significa que no existe i correlación y hablamos de isotropía. Si \\(&lt;u'v'&gt; &lt; 0\\), significa que las fluctuaciones están inversamente correlacionadas, i.e., anisotropía.\nEste es un gran problema no resuelto en la oceanografía física. El esfuerzo de Reynolds aparece porque la advección es no-lineal de tal forma que no podemos estudiar la larga escala sin conocer información de la pequeña escala que es un proceso aleatorio. Por similitud con el flujo laminar, los términos de esfuerzo de Reynolds se parametrizan estadísticamente como proporcionales a los gradientes de velocidad. El factor de proporcionalidad es el coeficiente de viscosidad, en este caso, turbulento. Es aqui donde utilizar herramientas estadísticas tiene sentido.\n\nNo podemos controlar las variables oceanográficas; estan en constante cambio a medida que el sistema observado evoluciona.\n\nEjemplo:\nEn el océano coexisten mareas, ondas internas, remolinos, turbulencia de pequeña escala,…las cuales enmascaran el fenómeno oceanográfico que estamos interesados en estudiar. Estos procesos incontrolables por el oceanógrafo en ocasiones es útil considerarlos aleatorios y utilizar herramientas estadísticas para caracterizarlos.\nImaginemos que queremos conocer cual es la temperatura superficial promedio en la bahía de Todos Santos. Una forma de proceder sería promediar todos los datos de temperatura superficial que disponemos de los últimos 100 años y promediarlos? Pero, ¿es realmente lo que deseamos? ¿Deberíamos de considerar la estaciones del año y obtener un pormedio para cada estación? ¿Qué sucede en años Niño, el cual sabemos que afecta la temperatura del océano? En definitiva, debemos de definir sobre que conjunto de datos vamos a promediar, y dichos promedio va a reflejar efectivamente esa elección.\n\nEste ejemplo precisa de la distinción entre lo que consideramos nuestra señal (temperatura media) de los procesos que son ruido (Estaciones del año, los años Niño, ondas internas, etc.). De esta forma, al definir el promedio estamos haciendo explícita la separación entre señal y ruido. Finalmente, una vez definido sobre que promediar, existen en literatura una gran cantidad de herramientas estadísticas que podemos utilizar. Definir señal y ruido, y determinar sobre que conjunto de datos vamos a calcular el promedio, es una tarea difícil. Conocer como debemos muestrear el océano también debe hacerse cuidadosamente.\n\nEn oceanografía física se muestrea el océano de forma discontínua, es decir, se obtienen medidas puntuales en el espacio y en el tiempo. Como dijimos anteriormente, el océano contiene procesos de diferentes escalas espaciales y temporales, nolineales, y aleatorios. Es por ello que es sumamente importante saber escojer el intervalo de muestreo \\(\\Delta{t}\\) dependiendo del fenómeno que se quiere muestrear. Debemos de tener en mente que la frecuencia mas alta que podemos resolver es la frecuencia de Nyquist:\n\\[f_N=1/(2\\Delta{t})\\,.\\]\nPor ejemplo, si medimos a intervalos de \\(\\Delta{t}=5\\,{ h}\\) podremos como máximo resolver procesos que ocurren con frecuencia \\(f_N\\le1/10\\,{ cph}\\). La frecuencia mas baja que podemos resolver va a depender de la longitud del registro. A esa frecuencia le llamamos frecuencia fundamental\n\\[f_0=1/(\\Delta{t}N)\\,,\\]\n{}donde \\(T=\\Delta{t} N\\) es la duración del muestreo y N es el número de muestras o datos. En general, debemos de medir suficiente tiempo para registrar varios ciclos del fenómeno de estudio para tener significancia estadística. Por lo tanto, nuestra resolución frecuencial va a depender del intervalo y duración del muestreo. El cociente \\(f_N/f_0=(1/2\\Delta{t})/(1/N\\Delta{t})=N/2\\) indica el número máximo de componentes de Fourier que podemos estimar. Una señal periódica se puede descomponer en la suma de un conjunto (infinito) de funciones oscilatorias de senos y cosenos o componentes de Fourier. Esto lo veremos en el capítulo~7. A cada muestreo de un fenómeno le denominamos realización, y a un conjunto de realizaciones se les denomina ensamble.\n\nEstadística básica\nLa estadística trata de describir las características de una población continua a partir de muestras discretas de la misma. Hablamos de población y de muestra de una población. Si calculamos, por ejemplo, la media de una población, estamos calculando un {parámetro}. Cuando calculamos la media de una muestra le llamamos un estadístico de la población.\n\nLa estadística nos ayuda a organizar, analizar, presentar datos, y nos da información de cómo planear la recolección de los mismos, i.e. a muestrear.\n\nLa media:\n\n\nLa media de una muestra de N valores \\(x_i=x_1,x_2,...,x_N\\) es\n\\[\\begin{equation}\n\\bar{x}=\\frac{1}{N}\\sum^N_{i=1}x_i=&lt;x&gt;\\,.\n\\end{equation}\\]\nLa media debe de diferenciarse de la mediana. La media es el momento de orden cero. La mediana de una población es aquel valor numérico que separa el 50% de valores mas altos del 50% de valores mas bajos. Se puede calcular ordenando de menor a mayor el conjunto de valores y escojer el valor central si el conjunto de datos es impar o el promedio de los dos centrales si es par.\n\n\nLa varianza:\n\n\nLa varianza de un una muestra de N valores \\(x_i\\) es\n\\[\\begin{equation}\ns^2=\\frac{1}{N-1}\\sum^N_{i=1}(x_i-\\bar{x})^2=&lt;x'^2&gt;\\,,\n\\end{equation}\\]\ndonde las primas indican fluctuaciones alrededor de la media. La varianza es una medida de cuán lejos estan los diferentes puntos de la muestra de la media de la población. La varianza es el segundo momento alrededor de la media. Al dividir por \\(N\\) estamos subestimando la verdaderavarianza de la población. Al dividir por \\(N-1\\) obtenemos un estimador insesgado.\n\nNOTA: el sesgo de un estimador se refiere a la diferencia entre su esperanza matemática y el valor numérico (real) del parámetro que se estima. Un estimador que no tiene sesgo se dice insesgado. Por ejemplo, para la media:\n\\[E[x]-\\mu \\rightarrow {0}\\]\n\\[\\bar{x}-\\mu \\rightarrow {0}\\]\nEJERCICIO: Demostrar porqué hay que dividir por \\(N-1\\) en lugar de \\(N\\) para que la definición de varianza sea un estimador insesgado.\n\n\nLa desviación típica:\n\n\nEs la raíz cuadrada de la varianza. Se suele escribir como \\(\\sigma\\) para referirse a la población o como \\(s\\) en estadística\n\\[\\begin{equation}\ns=\\sqrt{s^2}\\,.\n\\end{equation}\\]\n\nMomentos de orden superior:\n\n\nPodemos definir un momento alrededor de la media como:\n\\[\\begin{equation}\nm_p=\\frac{1}{N}\\sum^N_{i=1}(x_i-\\bar{x})^p=&lt;x'^p&gt;\\,.\n\\end{equation}\\]\nDe esta forma \\(m_2\\) es la varianza, \\(m_3\\) es la asimetría, y \\(m_4\\) la curtosis. El momento \\(m_3\\) indica la asimetría de la muestra alrededor de la media (\\(m_3&gt;0\\) implica distribución con cola larga en la parte positiva y viceversa). \\(m_4\\) indica el grado de esparcimiento de las muestras alrededor de la media. Una mayor curtosis indica mayor concentración de puntos alrededor de la media. Los momentos de orden superior (\\(&gt;2\\)) se suelen adimensionalizar dividiendo por la desviación estándar:\n\\[\\begin{equation}\n    m_3=\\frac{1}{N}\\sum^N_{i=1}\\left[\\frac{x_i-\\bar{x}}{\\sigma}\\right]^3=&lt;(x/\\sigma)'^3&gt;\n\\end{equation}\\]\n\\[\\begin{equation}\n    m_4=\\frac{1}{N}\\sum^N_{i=1}\\left[\\frac{x_i-\\bar{x}}{\\sigma}\\right]^4-3=&lt;(x/\\sigma)'^4&gt;-3\n\\end{equation}\\]\ndonde el factor \\(-3\\) hace que la curtosis tome el valor cero para una distribución Normal.\n\nCovarianza y correlación:\n\n\nLa covarianza entre dos variables \\(x\\) e \\(y\\) puede definirse como un estadístico que relaciona \\(x\\) e \\(y\\) de la siguiente forma\n\\[C_{xy}=&lt;x'y'&gt;=&lt;(x-\\bar{x})(y-\\bar{y})&gt;=\\frac{1}{N-1}\\sum\\limits^N_{i=1} (x_i-\\bar{x})(y_i-\\bar{y})\\,.\\]\nLa correlación es la covarianza normalizada\n\\[\\rho_{x y}=\\frac{C_{x y}}{s_x s_y}=\\frac{&lt;x' y'&gt;}{\\sqrt{&lt;x'^2&gt;&lt;y'^2&gt;}}\\,. \\]\nConsideremos el modelo estadístico lineal de media cero (es una recta que pasa por \\((\\overline{x},\\overline{y})=(0,0)\\))\n\\[\\hat{y}=\\alpha x\\,,\\]\ndonde \\(\\alpha\\) es una constante. El error cometido por este estimador se define como el error cuadratico medio\n\\[\\epsilon=&lt;(\\hat{y}-y)^2&gt;=\\alpha^2&lt;x^2&gt;+&lt;y^2&gt;-2\\alpha&lt;xy&gt;\\]\ny si queremos minimizar dicho error entonces tenemos que encontrar que \\(\\alpha\\) es el que provoca que la derivada \\(\\partial{\\epsilon}/\\partial{\\alpha}\\rightarrow{0}\\). Es decir\n\\[\\partial{\\epsilon}/\\partial{\\alpha}=2\\alpha&lt;x^2&gt;-2&lt;xy&gt;=0\\,,\\]\ny el \\(\\alpha\\) es\n\\[\\alpha=\\frac{&lt;xy&gt;}{&lt;x^2&gt;}\\,.\\]\nEl error cuadrático mínimo se encuentra substituyendo el valor de \\(\\alpha\\) en la expresión del error \\(\\epsilon\\) de arriba\n\\[\\epsilon=\\frac{&lt;xy&gt;^2}{&lt;x^2&gt;} + &lt;y^2&gt; - 2\\frac{&lt;xy&gt;^2}{&lt;x^2&gt;}=\n           &lt;y^2&gt;\\left(\\frac{&lt;xy&gt;^2}{&lt;x^2&gt;&lt;y^2&gt;}+1-2\\frac{&lt;xy&gt;^2}{&lt;x^2&gt;&lt;y^2&gt;}\\right)=\\]\n\\[=&lt;y^2&gt;(1-\\rho^2_{xy})\\,.\\]\nSi \\(\\rho^2_{xy}=1\\) entonces el error es cero, es decir, mínimo error. Opuestamente, si \\(\\rho^2_{xy}=0\\) entonces el error es igual a la varianza, es decir, máximo error. Si \\(\\rho\\) toma valores intermedios, i.e., \\(\\rho^2_{xy}=0.5\\), entonces el error es \\(\\epsilon=0.5&lt;y^2&gt;\\), es decir, el error del modelo lineal es un \\(50\\%\\) de la varianza. Por lo tanto, la correlación al cuadrado puede definirse también ciomo la eficiencia relativa del estimador \\(\\hat{y}^2\\) o la fracción de varianza explicada por el modelo lineal\n\\[\\rho^2_{xy}=\\frac{&lt;\\hat{y}^2&gt;}{&lt;y^2&gt;}=\\frac{{varianza\\,\\,\\,explicada}}{{ varianza\\,\\,\\,total}}\\,.\\]\nA este parámetro se le puede encontrar en literatura inglesa como skill del modelo lineal."
  },
  {
    "objectID": "chap4.html#probabilidad",
    "href": "chap4.html#probabilidad",
    "title": "Notas del Curso Análisis de Datos",
    "section": "Probabilidad",
    "text": "Probabilidad\n\nDistribuciones de probabilidad:\nLa {función de distribución acumulativa} \\(D_x(r)\\) se define como la probabilidad que una variable aleatoria \\(x\\) sea menor o igual a \\(r\\), es decir, \\(P(x\\le r)\\). Matemáticamente:\n\\[D(x)=\\int^r_{-\\infty}F(x)dx\\,,\\]\ndonde\n\\[F(x)=\\frac{d}{dx} D(x)\\]\nes la función de densidad de probabilidad (PDF, por su siglas en inglés). La PDF nos informa de la probabilidad que \\(x\\) sea igual a un cierto valor \\(r\\), \\(P(x=r)\\).\n\n{} Algunas propiedades de \\(D(x)\\):\n\n* \\(D(r)\\le D(s)\\,\\,\\,{ if}\\,\\,\\,r\\le s\\)\n* \\(D(-\\infty)=0\\)\n* \\(D(\\infty)=1\\)\n\nAlgunas propiedades de \\(F(x)\\):\n\n\n\\(F(x)\\ge0\\)\n\n\\(\\int^{\\infty}_{-\\infty} F(x) dx=1\\)\n\n\nLa probabilidad que una variable aleatoria \\(x\\) este contenida en el intervalo \\([r,r+dr]\\) es la integral de la función de densidad de probabilidad \\[P(r\\le x\\le r+dr)=\\int^{r+dr}_r F(x)dx\\,.\\]\nAmbas definiciones son parecidas aunque no son lo mismo. Para ello veamos el ejemplo de la suma del lanzamiento de dos dados al aire.\n\nLa densidad de probabilidad de que la suma de los dos dados sea 7 es máxima y que sea 2 o 12 es mínima. Este ejemplo describe dos propiedades fundamentales de funciones de probabilidad discretas: (i) \\(P(X=x) \\ge 0\\) y (ii)\\(\\sum{P(x)}=1\\). La distribución de probabilidad acumulativa y la función de densidad de probabilidad tienen las siguientes distribuciones\n\n{} Momentos estadísticos de una función de densidad de probabilidad:\n\nLos momentos centrados (o alrededor de la media) de una distribución de probabilidad se definen como\n\\[m_r=E[(x-E[x])^r]=\\int^{\\infty}_{-\\infty} (x-\\mu)^rF(x)dx\\,.\\]\nComo caso particular, los momentos alrededor del origen (i.e., \\(\\mu=0\\)) son: \\[m^0_r=E[x^r]=\\int^{\\infty}_{-\\infty} x^rF(x)dx\\,.\\]\nEntonces, los primeros tres momentos centrados se definen como\n\\[m_0=E[(x-E[x])^0]=E[1]=\\int^{\\infty}_{-\\infty}F(x)dx=1\\,,\\] \\[m_1=E[(x-E[x])^1]=E[x]-\\mu=\\int^{\\infty}_{-\\infty} (x-\\mu)^1 F(x)dx=0\\,,\\] \\[m_2=E[(x-E[x])^2]=E[x^2 + E[x]^2 -2xE[x]]=\n      E[x^2]+E[x]^2-2E[x]E[x]=\\] \\[E[x^2]-E[x]^2=\\underbrace{E[x^2]}_{\\sigma^2}-\\mu^2=\\int^{\\infty}_{-\\infty} (x-\\mu)^2 F(x)dx=\\sigma^2-\\mu^2\\,.\\]\nLos momentos alrededor de cero (\\(\\mu=0\\)) tambien pueden ser estandarizados: \\[m^*_r=m_r/\\sigma^r=\\frac{E[(x-E[x])^r]}{(\\underbrace{E[(x-E[x])^2]}_{\\sigma^2})^{r/2}}\\,.\\]\nLos cuatro primeros momentos estadísticos alrededor de cero estandarizados son: \\[m^*_1=m_1/\\sigma^1=\\frac{E[(x-\\mu)^1]}{(E[(x-\\mu)^2])^{1/2}}=\\frac{\\mu-\\mu}{\\sqrt{E[(x-\\mu)^2]}}=0\\,,\\] \\[m^*_2=m_2/\\sigma^2=\\frac{E[(x-\\mu)^2]}{(E[(x-\\mu)^2])^{2/2}}=1\\,,\\] \\[m^*_3=m_3/\\sigma^3=\\frac{E[(x-\\mu)^3]}{(E[(x-\\mu)^2])^{3/2}}\\,,\\] \\[m^*_4=m_4/\\sigma^4=\\frac{E[(x-\\mu)^4]}{(E[(x-\\mu)^2])^{4/2}}\\,.\\]\n\nDistribución uniforme:\nLa distribución de probabilidad uniforme viene dada por \\[F(x)=\\frac{1}{b-a}\\,\\,\\,, a \\le x \\le b\\] \\[=0,\\,\\,\\,\\,fuera\\,\\,del\\,\\,intervalo\\]\n\nSe deduce de la expresión de área de un cuadrado:\n\\[Area=base*altura=(b-a)F(x)=1\\]\nLa función de distribución acumulativa es \\[D(x)=0,\\,\\,\\,x&lt;a\\] \\[D(x)=\\frac{x-a}{b-a},\\,\\,\\,a \\le x \\le b\\] \\[D(x)=1,\\,\\,\\,x \\ge b\\]\n\nLa media es \\(\\mu=(b+a)/2\\) y la varianza es \\(\\sigma^2=1/3(a^2 + b^2 +ab)\\). Demostración:\n\n{} Los momentos estadísticos alrededor del origen de la distribución uniforme son \\[m^0_r=E[x^r]=\\int^{\\infty}_{-\\infty} x^rF(x)dx=\\int^{b}_{a} \\frac{x^r}{b-a}dx=\\]\n\\[\\frac{1}{b-a}\\int^{b}_{a}x^r dx=\\frac{1}{b-a}\\left[\\frac{x^{r+1}}{r+1}\\right]^b_a=\n      \\frac{1}{b-a}\\left[\\frac{b^{r+1}}{r+1}-\\frac{a^{r+1}}{r+1}\\right]=\\frac{b^{r+1}-a^{r+1}}{(b-a)(r+1)}\\]\ny por lo tanto la media es\n\\[m^0_1=E(x)=\\frac{b^2-a^2}{2(b-a)}=\\frac{(b-a)(b+a)}{2(b-a)}=\\frac{b+a}{2}\\,,\\]\ny la varianza\n\\[m^0_2=E(x^2)=\\frac{b^3-a^3}{3(b-a)}=\\frac{(b-a)(a^2+b^2+ab)}{3(b-a)}=\\frac{1}{3}(a^2 + b^2 +ab)\\,.\\]\nEjemplo de distribución uniforme: La ruleta rusa. Supongamos que puede tomar 360 valores, es decir, \\(0 \\le x \\le 360\\). Entonces\n\\[F(x)=\\frac{1}{360},\\,\\,\\,0 \\le x \\le 360\\,,\\]\ny, por ejemplo, la probabilidad de que la bola caiga entre el 50 y el 360 es\n\\[P(50\\le x \\le 360)=\\int^{360}_{50}\\frac{1}{360}dx=\\frac{1}{360}\\left[x\\right]^{360}_{50}=\\frac{310}{360}=0.8611\\,(\\sim86\\%).\\]\nLa función de distribución acumulativa es\n\\[D(x)=0,\\,\\,\\,x&lt;0\\] \\[D(x)=\\frac{x}{360},\\,\\,\\,0 \\le x \\le 360\\] \\[D(x)=1,\\,\\,\\,x \\ge 360\\]\n\nDistribución normal o Gaussiana:\n\n\nLa distribución normal es una de las distribuciones mas recurrente en la naturaleza. En general cualquier variable aleatoria medida, especialmente aquellas que son suma de otras variables aleatorias, tiene una distribución normal alrededor de la media\n\\[F(x)=\\frac{1}{\\sigma\\sqrt{2\\pi}}{exp}\n   \\left\\{ -\\frac{(x-\\bar{x})^2}{2\\sigma^2} \\right\\}\\,.\\]\nLa distribución de función acumulativa normal se obtiene integrando la expresión de arriba. Para ello vamos a realizar el cambio de variable (que no es nada mas que estandarizar la variable aleatoria x)\n\\[z=\\frac{x-\\bar{x}}{\\sigma\\sqrt{2}}\\] y \\[dz=\\frac{dx}{\\sigma\\sqrt{2}}\\,,\\]\nde lo que se deduce\n\\[D(z)=\\frac{\\sigma\\sqrt{2}}{\\sigma\\sqrt{2\\pi}}\\int^z_{-\\infty} {exp}\n  \\left\\{ -z^2 \\right\\}dz =\n  \\frac{1}{\\sqrt{\\pi}}\\int^z_{-\\infty} {exp}\n  \\left\\{ -z^2 \\right\\} dz\\,,\\]\ndonde \\(\\frac{2}{\\sqrt{\\pi}}\\int^z_{0}{exp}\\left\\{ -t^2 \\right\\}dt={erf}(z)\\,.\\)\nLos momentos estadísticos alrededor del origen de la función de distribución Normal son\n\\[m^0_r=E[x^r]=\\int^{\\infty}_{-\\infty} x^rF(x)dx=\\frac{1}{\\sigma\\sqrt{2\\pi}}\\int^{\\infty}_{-\\infty}x^r{exp}\n   \\left\\{ -\\frac{(x-\\bar{x})^2}{2\\sigma^2} \\right\\} dx\\]\nHagamos el cambio de variable\n\\[u=\\frac{x-\\bar{x}}{\\sigma\\sqrt{2}}\\] \\[du=\\frac{dx}{\\sigma\\sqrt{2}}\\]\nSi substituimos en la expresión de \\(m_r\\) obtenemos\n\\[m^0_r=\\frac{\\sigma\\sqrt{2}}{\\sigma\\sqrt{2\\pi}}\\int^{\\infty}_{-\\infty}\n  \\left( \\sigma \\sqrt{2}u+\\bar{x}\\right)^r{e}^{-u^2}du=\n  \\frac{1}{\\sqrt{\\pi}}\\int^{\\infty}_{-\\infty}\\left( \\sigma \\sqrt{2}u+\\bar{x}\\right)^r{e}\n   ^{-u^2}du\\,.\\]\nEjercicio: Deducir los momentos estadísticos de orden 1 y 2 de la distribución Normal, es decir, la media y la varianza. Integrales útiles:\n\\[\\int e^{-ax^2}dx=\\frac{\\sqrt{\\pi}}{2\\sqrt{a}}{erf}(x\\sqrt{a})\\] \\[\\int xe^{-ax^2}dx=-\\frac{1}{2a}e^{-ax^2}\\,,\\]\ndonde la función de error se define cómo:\n\\[erf(z)=\\frac{2}{\\sqrt{\\pi}}\\int_0^z e^{-t^2} dt\\,.\\]\nLa función de error cumple las siguientes identidades: \\[{erf}(0)=\\frac{2}{\\sqrt{\\pi}}\\int_0^0 e^{-t^2} dt=0\\], \\[{erf}(\\infty)=\\frac{2}{\\sqrt{\\pi}}\\underbrace{\\int_0^\\infty e^{-t^2} dt}_{\\frac{\\sqrt{\\pi}}{2}}=1\\]. \\[{erf}(-\\infty)=\\frac{2}{\\sqrt{\\pi}}\\underbrace{\\int_0^{-\\infty} e^{-t^2} dt}_{-\\frac{\\sqrt{\\pi}}{2}}=-1\\].\nRespuesta: La media y la varianza son\n\\[m^0_1=E(x)=\\mu\\,\\,\\,\\,\\,\\,;\\,\\,\\,\\,\\,\\,m_1=0\\] \\[m^0_2=Var(x)=\\mu^2 + \\sigma^2\\,\\,\\,\\,\\,\\,;\\,\\,\\,\\,\\,\\,m_2=\\sigma^2\\]\nDeducción de la media: El momento de orden 1 centrado es:\n\\[m_1=\\frac{\\sigma\\sqrt{2}}{\\sqrt{\\pi}}\\int^{\\infty}_{-\\infty}u{e}\n   ^{-u^2}du=-\\frac{\\sigma\\sqrt{2}}{2\\sqrt{\\pi}}{e}\n   ^{-u^2}\\Big|^{\\infty}_{-\\infty}=0\\,.\\]\ny alrededor de cero (momentos crudos):\n\\[m^0_1=\\frac{1}{\\sqrt{\\pi}}\\int^{\\infty}_{-\\infty}(\\sigma \\sqrt{2} u + \\mu){e}\n   ^{-u^2}du=\\frac{\\sigma\\sqrt{2}}{\\sqrt{\\pi}}\\int^{\\infty}_{-\\infty}u{e}\n   ^{-u^2}du + \\frac{1}{\\sqrt{\\pi}}\\int^{\\infty}_{-\\infty}\\mu{ e}\n   ^{-u^2}du=\\]\n\\[=\\underbrace{\\frac{\\sigma\\sqrt{2}}{\\sqrt{\\pi}}\\left( -\\frac{1}{2}{ e}\n   ^{-u^2}\\right)}_{0} \\Big|^{\\infty}_{-\\infty}+\n   \\frac{\\mu}{\\sqrt{\\pi}}\\left( \\frac{\\sqrt{\\pi}}{2}{ erf}(u)\\right) \\Big|^{\\infty}_{-\\infty}=\n   \\frac{\\mu}{2}[1-(-1)]=\\mu\\,,\\]\nDeducción de la varianza:\nEl momento de orden 2 centrado es:\n\n\\[m_2=\\frac{(\\sigma\\sqrt{2})^2}{\\sqrt{\\pi}}\\int^{\\infty}_{-\\infty}u^2{ e}^{-u^2}du\\]\n\\[x=u \\rightarrow dx=du\\] \\[dy=u{ e}^{-u^2} \\rightarrow y=\\frac{1}{2}{ e}^{-u^2}\\]\n\\[m_2=\\frac{2\\sigma^2}{\\sqrt{\\pi}}\\left[ -\\frac{1}{2}{ e}^{-u^2}\n      \\Big|^{\\infty}_{-\\infty} +\\frac{\\sqrt{\\pi}}{4} { erf}(u)\\Big|^{\\infty}_{-\\infty}\\right]=\n  \\frac{2\\sigma^2}{\\sqrt{\\pi}}\\left[0 + \\frac{\\pi}{4}\\left({ erf}(\\infty) - { erf}(-\\infty) \\right)\n  \\right]=\\] \\[=\\frac{2\\sigma^2}{\\sqrt{\\pi}}\\left[ \\frac{\\pi}{4}+\\frac{\\pi}{4}\\right]=\\sigma^2\\]\n\ny alrededor de cero (momentos crudos):\n\\[m^0_2=\\frac{1}{\\sqrt{\\pi}}\\int^{\\infty}_{-\\infty}(\\sigma \\sqrt{2} u + \\mu)^2{ e}^{-u^2}du=\n   \\frac{1}{\\sqrt{\\pi}}\\int^{\\infty}_{-\\infty}2\\sigma^2 u^2{ e}^{-u^2}du +\\] \\[ +\\frac{\\mu^2}{\\sqrt{\\pi}}\\int^{\\infty}_{-\\infty}{ e}^{-u^2}du\n   +\\frac{2\\sigma\\sqrt{2}\\mu}{\\sqrt{\\pi}}\\int^{\\infty}_{-\\infty}u{ e}^{-u^2}du=\\]\n\\[\n   =\\underbrace{\\frac{2\\sigma^2}{\\sqrt{\\pi}}\\int^{\\infty}_{-\\infty}u^2{ e}^{-u^2}du}_{m_2=\\sigma^2}\n   +\\mu \\underbrace{\\frac{1}{\\sqrt{\\pi}}\\int^{\\infty}_{-\\infty}\\mu{ e}^{-u^2}du}_{m_1^0=\\mu}+\n   +2\\mu \\underbrace{\\frac{\\sigma\\sqrt{2}}{\\sqrt{\\pi}}\\int^{\\infty}_{-\\infty}u{ e}^{-u^2}du}_{m_1=0}= \\sigma^2+\\mu^2\\,,\\]\n\nLa probabilidad de que una variable normalmentedistribuida caiga en una desviación estándar de su valor medio viene dada por \\[P(-1\\le z \\le 1)=\\int^{+1}_{-1} F(z) dz=\\frac{1}{\\sqrt{2\\pi}}\\int^{+1}_{-1}e^{-\\frac{1}{2}z^2}dz=\\] \\[=\\frac{1}{\\sqrt{2\\pi}}\\frac{\\sqrt{\\pi}}{2\\sqrt{1/2}}\\left[{ erf}({z\\sqrt{1/2}})\\right]^1_{-1}=\n   \\frac{1}{2}\\left[{ erf}({1/\\sqrt{2}})-{ erf}({-1/\\sqrt{2}}) \\right]=\\] \\[=\\frac{1}{2}\\left[0.6827-(-0.6827)\\right]=0.6827\\,(~68.27\\%)\\,,\\]\n\ny similarmente para 2 y 3 desviaciones estándar\n\n\\[P(-2\\le z \\le 2)=\\int^{+2}_{-2} F_x(z) dz=95.45\\%\\] \\[P(-3\\le z \\le 3)=\\int^{+3}_{-3} F_x(z) dz=99.73\\%\\,.\\]\nEntonces solo hay un \\(4.55\\%\\) de probabilidad de que una variable normalmente distribuida caiga fuera de dos desviaciones estándar respecto de la media. Puesto que es una probabilidad con 2 colas, la probabilidad de que una variable normal exceda su media por mas de \\(2\\sigma\\) es la mitad de esto, es decir \\(2.275\\%\\), ya que la distribución normal es simétrica.\nEn la práctica una PDF se calcula como un histograma escalado. Es por ello que necesitamos escojer el tamaño y localización de los bins en el histograma. La demo muestra las consecuencias de esta elección (pdf_demo.m).\n\nDistribución de Poisson:\n\n\nLa distribución de Poisson expresa la probabilidad de que ocurra un determinado número de eventos durante un cierto intervalo en el tiempo o distancia en el espacio. Se usa generalmente para la ocurrencia de sucesos con muy poca probabilidad o muy ``raros’’. La expresión para la función acumulativa es:\n\\[D(x)=P(x \\le r)=e^{-\\lambda}\\sum^{|r|}_{k=0}\\frac{\\lambda^k}{k!}\\,,\\]\ndonde \\(\\lambda\\) es el valor promedio\ny la función de densidad de probabilidad\n\\[F(x)=P(x=k)=\\frac{{\\lambda}^k e^{-\\lambda}}{k!}\\]\nLos momentos estadísticos alrededor del origen de la función de distribución de Poisson se pueden calcular directamente con sumatorios y expansion de Taylor:\n\\[m^0_r=E[x^r]=\\sum k^r P(X=k)=\\sum_{k\\ge0} k^rF(x)= \\sum_{k\\ge0} k^r\\frac{{\\lambda}^k e^{-\\lambda}}{k!}=\\] \\[=\\lambda e^{-\\lambda}\\sum_{k \\ge 0}\\frac{k^r}{k!}\\lambda^{k-1}\\]\n{}Veamos el momento de orden 1 alrededor del orígen:\n\\[m^0_1=E[x]=\\lambda e^{-\\lambda}\\sum_{k \\ge 0}\\frac{k}{k!}\\lambda^{k-1}=\n\\lambda e^{-\\lambda}\\sum_{k \\ge 1}\\frac{k}{(k-1)!k}\\lambda^{k-1}=\n\\lambda e^{-\\lambda}\\sum_{k \\ge 1}\\frac{1}{(k-1)!}\\lambda^{k-1}\\] \\[=\\lambda e^{-\\lambda}\\sum_{j \\ge 0}\\frac{\\lambda^{j}}{j!}\\,,\\] para \\(j=k-1\\). Finalmente expandiendo en series de Taylor la función exponencial\n\\[e^{\\lambda}=\\sum_{j \\ge 0}\\frac{1}{j!}\\lambda^{j}\\], obtenemos:\n\\[m^0_1=\\lambda e^{-\\lambda}e^{\\lambda}=\\lambda\\]\nEjercicio: Demostrar que el momento estadístico alrededor del origen de orden 2 de la distribución de Poisson es igual a \\(m^0_2=\\lambda+\\lambda^2\\).\n\\[m^0_2=E[x^2]=\\lambda e^{-\\lambda}\\sum_{k \\ge 0}\\frac{k^2}{k!}\\lambda^{k-1}=\n               \\lambda e^{-\\lambda}\\left[ \\sum_{k \\ge 1}(k-1)\\frac{1}{(k-1)!}\\lambda^{k-1} +\n                                      \\sum_{k \\ge 1} \\frac{1} {(k-1)!} \\lambda^{k-1} \\right]=\\]\n\\[= \\lambda e^{-\\lambda}\\left[ \\lambda \\sum_{k \\ge 2}\\frac{1}{(k-2)!}\\lambda^{k-2} + \\sum_{k \\ge 1}\\frac{1}{(k-1)!}\\lambda^{k-1}\\right]=\\] \\[= \\lambda e^{-\\lambda}\\left[ \\lambda \\sum_{j \\ge 0}\\frac{1}{j!}\\lambda^{j} + \\sum_{i \\ge 0}\\frac{1}{(i)!}\\lambda^{i}\\right]=\n\\lambda e^{-\\lambda} \\left[\\lambda e^{\\lambda} + e^{\\lambda}\\right]=\\]\n\\[=\\lambda (\\lambda +1)=\\lambda^2 + \\lambda\\,.\\]\nEn el casso que fuera el momento de orden 2 centrado se escribiría:\n\\[E[(x-E[x])^2]=E[x^2]-(E[x])^2=\\lambda^2 + \\lambda - (\\lambda)^2=\\lambda\\,.\\]\nEjemplo: En los últimos 160 años, han sucedido 680 tormentas intensas en el Golfo de México, incluyendo depresiones, tormentas tropicales, y huracanes. Asumimos que la frecuencia de ocurrencia de una tormenta intensa en el Golfo de México sigue una distribución de Poisson (eventos “raros”, poco frecuentes). Calcula la probabilidad de que ocurran 2 huracanes en 1 año:\n\n\nEl número promedio de tormentas intensas por año es: \\(\\mu=680/160=4.25\\) huracanes/año.\nLa probabilidad de que ocurran 2 huracanes en 1 año es:\n\n\\[P(x=2)=\\frac{{\\lambda}^2 e^{-\\lambda}}{2!}=\\frac{{4.25}^2 e^{-4.25}}{2!}=0.1288\\,(\\sim12\\%)\\]\nLa probabilidad es muy baja debido a que exigimos que sean exactamente 2 huracanes en un año y no, por ejemplo, \\(&gt;2\\). En el segundo caso, la probabilidad aumentaría considerablemente\n\\[P(x&gt;2)=P(x=3)+P(x=4)+....=1-P(x\\le 2)=1-[P(x=0)+P(x=1)+P(x=2)]=\\] \\[=1-[0.0143+0.0606+0.1288]=1-0.2037=0.7963\\,(\\sim80\\%)\\]\n\nDistribución Binomial:\n\nSupongamos que tenemos un conjunto de \\(n\\) tiradas en los cuales pueden suceder únicamente dos cosas: acierto' ofallo’. La probabilidad de acertar en una tirada es p=P. Si \\(X\\) es el número total de aciertos en \\(n\\) tiradas, entonces la probabilidad de que el número de aciertos sea \\(k\\) es:\n\\[P(X=k)=\\left( \\begin{array}{c}\nn \\\\ k\n       \\end{array} \\right)\np^k (1-p)^{n-k}, \\, k=0,1,2,3,....,n\\,,\\]\n}donde la expresión\n\\[\\left( \\begin{array}{c}\nn \\\\ k\n       \\end{array} \\right)=C(n,k)\\equiv\\frac{n!}{(n-k)!k!}\\,,\\]\n\nes el número de diferentes combinaciones de grupos de k objetos que pueden ser elegidos de un conjunto total de n objetos. Estos números se denominan coeficientes binomiales. La probabilidad de que el número de aciertos caiga en un rango de valores es\n\\[P(a\\le X\\le b)=\\sum^b_a P(X)\\]\nEjemplo 1: ¿Cual es la probabilidad de obtener exactamente 6 caras de 10 lanzamientos de moneda? Respuesta:\n\\[P(x=6) = C(10,6)0.5^6(1-0.5)^{10-6} = \\frac{10!}{(10-6)!6!}0.5^6(1-0.5)^{10-6} \\simeq 0.205\\]\n{Ejemplo 2:} ¿Cual es la probabilidad de obtener mas de 15 caras de 20 lanzamientos de moneda? Respuesta:\n\\[\\sum^{20}_{k=16} \\left( \\begin{array}{c} 20 \\\\ k\n       \\end{array} \\right) 0.5^k(1-0.5)^{20-k}=0.006\\,.\\]\nSi realizas esta operación a mano se vuelve muy tediosa. Es por ello que se utiliza la aproximación Normal a la distribución Binomial (DeMoivre-Laplace).\n{} Teorema de DeMoivere-Laplace (aproximación de Binomial a Normal)\n\nLa distribución binomial de una variable X definida por n tiradas independientes cada una de las cuales tienen una probabilidad \\(p\\) de acertar, es aproximadamente una distribución Normal de media \\(np\\) y desviación típica \\(\\sqrt{np(1-p)}\\), cuando n es suficientemente grande. Entonces se deduce que para cualquier número a y b,\n\\[lim_{n\\rightarrow\\infty} P \\left( a&lt;\\frac{X-np}{\\sqrt{np(1-p)}}&lt;b\\right)= \\frac{1}{\\sqrt{2\\pi np(1-p)}}\\int^b_a exp-\\left[\\frac{(x-np)^2}{2np(1-p)}\\right]dx\\,.\\]\n\nEsto significa que el estadístico, \\(\\frac{X-np}{\\sqrt{np(1-p)}}\\) , tiene una distribución Normal. Este teorema es un caso particular del teorema del límite central y nos permite de simplificar la solución de un problema binomial.\n{} Ejemplo de la aproximación Normal a la distribución Binomial:\n\nEl 2% de los XBTs fabricados por una empresa presentan defectos. Si hemos adquirido 2000 XBTs, ¿Cual es la probabilidad de que haya menos de 50 defectuosos?\n\nRespuesta: Se trata de una distribución binomial ya que solo pueden ser defectuosos o no defectuosos. La probabilidad que sea defectuoso es \\(p=0.02\\) (2%) y \\(n=2000\\), lo que nos da una distribución Binomial \\(B(2000,0.02)\\). Puesto que la \\(n\\) es grande podemos hacer una aproximación a la distribución Normal. Calculamos la media y desviación estándar de la distribución Normal \\(\\mu=np=200*0.02=40\\) y \\(\\sigma=\\sqrt{np(1-p)}=\\sqrt{2000*0.02*(1-0.02)}=6.26\\) \\(x\\) es \\(B(2000,0.02)\\) y \\(x_N\\) es \\(N(40,6.26)\\).\nLa probabilidad que \\(x&lt;50\\) es \\[p(x&lt;50)=p(x_N\\le 49)\\,,\\] y si estandarizamos \\[p(x_N\\le 49)=p\\left(z\\le \\frac{49-40}{6.26} \\right)=p(z\\le 1.44)=0.9251\\,.\\]\n{} EJERCICIOS de estadística y probabilidad:\n\nEjercicio 1: Calcule E[x] si x tiene la función de densidad de probabilidad\n\\[ f(x)=\\Bigg(\\begin{array}{c}\n\\frac{1}{4}xe^{-\\frac{x}{2}}\\,\\,\\,\\,\\,,x&gt;0 \\\\ 0 \\,\\,\\,\\,\\,,otherwise\n\\end{array})\\,.\\]\nLa esperanza E[x] de la función \\(f(x)\\) es entonces\n\\[E[x]=\\int^\\infty_0 x\\left(\\frac{1}{4}xe^{-\\frac{x}{2}}\\right)dx\n      =\\frac{1}{4}\\int^\\infty_0 x^2e^{-\\frac{x}{2}}dx\\,.\\]\nDefinamos \\(y=x/2\\); entonces \\(x=2y\\) y \\(dx=2dy\\) y obtenemos\n\\[E[x]=\\frac{1}{4}\\int^\\infty_0 x^2e^{-\\frac{x}{2}}dx\n        =\\frac{1}{4}\\int^\\infty_0 (2y)^2e^{-y}2dy\n    =2\\int^\\infty_0y^2e^{-y}dy\\,.\\] Vamos ahora a resolver la integral por partes. Hacemos la siguiente sustitución: \\(u=y^2\\), \\(dv=e^{-y}\\) y por ende \\(du=2ydy\\) y \\(v=-e^{-y}\\). La integral se puede reescribir usando la expresión general de integración por partes \\(h(x)=uv-\\int vdv\\):\n\\[E[x]=2\\int^\\infty_0y^2e^{-y}dy=2\\left[ -y^2e^{-y}-\\int-e^{-y}(2y)dy\\right]\n      =2\\left[ -y^2e^{-y}+2\\int ye^{-y}dy\\right]\\,.\\]\nIntegramos de nuevo por partes. Usa \\(u=y\\), \\(dv=e^{-y}\\) y entonces \\(du=dy\\) y \\(v=-e^{-y}\\)\n\\[E[x]=2\\left[ -y^2 e^{-y} + 2 \\Big\\{ -y e^{-y} - \\int -e^{-y}dy \\Big\\} \\right]=\\] \\[=2\\left[ -y^2 e^{-y} + 2 \\Big\\{ -y e^{-y} - e^{-y}         \\Big\\} \\right]=\\]\n\\[=-2y^2e^{-y}-4ye^{-y}-4e^{-y}=\\] \\[=\\left[ -2e^{-y}(y^2+2y+2) \\right]^{\\infty}_0=\\] \\[=\\lim_{n \\to\\infty}\\left[ -2e^{-y}(y^2+2y+2) \\right]-\\left[ -2e^{-y}(y^2+2y+2) \\right]_{y=0}\\,.\\]\nEl límite es ahora del tipo \\(\\infty/\\infty\\) y entonces usamos la regla de l’Hopital \\[E[x]=-2\\lim_{y \\to\\infty}\\frac{2y+2}{e^y}+2\\left[e^{-y}(y^2+2y+2) \\right]_{y=0}\\] Usamos la regla de l’Hopital de nuevo\n\\[E[x]=-2\\lim_{y \\to\\infty}\\frac{2}{e^y}+2\\left[e^{-y}(y^2+2y+2) \\right]_{y=0}=0+2\\left[e^{-y}(y^2+2y+2) \\right]_{y=0}=4\\]\n{} Ejercicio 2: Calcule E[x] si x tiene la función de densidad de probabilidad\n\\[f(x)=\\Bigg\\{\\begin{array}{c}\nc(1-x^2)\\,\\,\\,\\,\\,,-1&lt;x&lt;1 \\\\ 0 \\,\\,\\,\\,\\,,otherwise\n       \\end{array} \\,.\\]\n\\[E[x]=\\int^1_{-1} x[c(1-x^2)]dx=c \\int^1_{-1} x[(1-x^2)]dx=\\] \\[=c \\int^1_{-1} x-x^3dx=c\\left[\\frac{x^2}{2}+\\frac{x^4}{4}\\right]^{1}_{-1}=0\\]\n{} Ejercicio 3: Calcule E[x] si x tiene la función de densidad de probabilidad\n\\[f(x)=\\Bigg\\{\\begin{array}{c}\n\\frac{5}{x^2}\\,\\,\\,\\,\\,,x&gt;5 \\\\ 0 \\,\\,\\,\\,\\,,x\\le5\n       \\end{array} \\,.\\]\n\\[E[x]=\\int^{\\infty}_{5}x\\frac{5}{x^2}dx=\\int^{\\infty}_{5}\\frac{5}{x}dx=5\\int^{\\infty}_{5}\\frac{1}{x}dx\\] \\[=5[lnx]^{\\infty}_5=5\\left[\\Big(\\lim_{x \\to\\infty} ln{x}\\Big)-ln{5}\\right]\\rightarrow \\infty\\]\n La variable aleatoria \\(x\\) tiene la siguiente función de densidad de probabilidad\n\\[f(x)=\\Bigg\\{\\begin{array}{c}\n          k(2x+3)\\,\\,\\,\\,\\,-1\\le x \\le 2 \\\\ 0 \\,\\,\\,\\,\\,otherwise\n         \\end{array} \\,.\\]\n Sea la función \\(g(x)\\) dada por\n\\[g(x)=\\Bigg\\{\\begin{array}{c}\nx+2\\alpha\\,\\,\\,\\,\\,,x\\le-\\alpha \\\\\nx \\,\\,\\,\\,\\,,-\\alpha \\le x \\le \\alpha \\\\\nx-2 \\alpha \\,\\,\\,\\,\\,,x&gt;\\alpha\n       \\end{array} \\,,\\]\ndonde asumimos que x esta normalmente distribuida. Calcula la media de \\(g(x)\\).\n\\[E[g(x)]=\\int^{\\infty}_{-\\infty} g(x) F(x) dx=\\int^{-\\alpha}_{-\\infty} (x+2\\alpha) F(x) dx +\n\\int^{\\alpha}_{-\\alpha} x F(x) dx + \\int^{\\infty}_{\\alpha} (x-2\\alpha) F(x) dx=\\] \\[=\\int^{-\\alpha}_{-\\infty} x F(x) dx + \\int^{-\\alpha}_{-\\infty} 2 \\alpha F(x) dx +\n   \\int^{\\alpha}_{-\\alpha} x F(x) dx + \\int^{\\infty}_{\\alpha} x F(x) - \\int^{\\infty}_{\\alpha} 2\\alpha F(x)dx=\\] \\[=\\int^{-\\infty}_{-\\infty} x F(x) dx +  2\\alpha \\left[ \\int^{-\\alpha}_{-\\infty} F(x) dx - \\int^{\\infty}_{\\alpha}  F(x)\\right]=\\] \\[=\\int^{-\\infty}_{-\\infty} x F(x) dx +  2\\alpha \\left[ D(x=-\\alpha) - \\Bigg(1-\\int^{-\\alpha}_{-\\infty} F(x) dx\\Bigg)\\right]=\\] \\[=\\int^{-\\infty}_{-\\infty} x F(x) dx +  2\\alpha \\left[ D(x=-\\alpha) - \\Bigg(1-D(x=\\alpha)\\Bigg)\\right]=\\] \\[=\\mu + 2 \\alpha \\left[ D(-\\alpha) - 1 + D(\\alpha) \\right]\\,.\\]\ndonde la media de la distribución Normal es\n\\[E[x]=\\int^{-\\infty}_{-\\infty} x F(x) dx=\\int^{-\\infty}_{-\\infty} x \\frac{1}{\\sigma \\sqrt{2\\pi}}e^{-\\frac{1}{2} \\big( \\frac{x-\\mu}{\\sigma} \\big)^2 } dx=\\mu\\,.\\]\n{} Teorema del límite central:\nDefinición 1: Sea \\(X_1,\\,X_2,\\,X_3,...,X_n\\) un conjunto de variables aleatorias, independientes e idénticamente distribuidas con media \\(\\mu\\) y varianza \\(\\sigma^2\\) distinta de cero. Sea \\[S_n=X_1+X_2+....+X_n\\,,\\] entonces \\[\\lim_{n \\to\\infty} Pr (Z_n\\le z)= \\Phi(z)\\,,\\] donde \\(\\Phi(z)\\) es una distribución Normal estándar y \\(Z_n=\\frac{S_n - n\\mu} {\\sigma\\sqrt{n}} = \\frac{\\bar{X} - \\mu} {\\sigma/\\sqrt{n}}\\) es una estandarización del sumatorio \\(S_n\\) de tal forma que la media de la nueva variable \\(Z_n\\) sea cero y su desviación estándard sea igual a 1. De esta forma, las variables \\(Z_n\\) convergerán a una distribución normal estándar \\(N(0,1)\\), cuando \\(n\\) tienda a infinito.\n\nDefinición 2: Sea \\(X_1,\\,X_2,\\,X_3,...,X_n\\) un conjunto de variables aleatorias, independientes e idénticamente distribuidas con media \\(\\mu\\) y varianza \\(\\sigma^2\\) distinta de cero. Entonces, si \\(n\\) es suficientemente grande, la variable aleatoria\n\\[\\bar{X}=\\frac{1}{n}\\sum^n_{i=1}{X_i}\\]\ntiene aproximadamente una distribución normal con media \\(\\mu(\\bar{X})=\\mu\\) y desviación típica \\(\\sigma(\\bar{X})=\\sigma/\\sqrt{n}\\).\n\nNOTA: Es importante remarcar que el teorema del límite central no dice nada acerca de la distribución de \\(X_i\\), solo de la distibución de su media muestral \\(\\bar{X}\\).\n\nAplicación 1: Calculo de probabilidades sobre la media muestral.\n\nEjemplo: La recolección de muestras de agua con una roseta es una variable aleatoria con media \\(\\mu=150\\,{ ml}\\) y varianza de \\(\\sigma^2=120\\,{ ml}^2\\). Si tomamos \\(n=40\\) muestras aleatorias de agua. (a) ?`Cual es la media y la desviación estándar de la media muestral?, (b) ¿Cual es la probabilidad de que la media muestral contenga entre \\(145\\) y \\(153\\,{ ml}\\) de agua?\n\n\n\\(\\mu(\\bar{X})=150\\,{ ml}\\) y \\(\\sigma(\\bar{X})=\\sigma/\\sqrt{n}=\\sqrt{120/40}= \\sqrt{3}\\,{ ml}\\)\n\nQueremos calcular \\(Pr(145 \\le \\bar{X} \\le 153)\\). Si escribimos la probabilidad en forma estandarizada, entonces:\n\n\\[Pr(145 \\le \\bar{X} \\le 153) =\n      Pr\\left( \\frac{145-150}{\\sqrt{3}} \\le Z \\le \\frac{153-150}{\\sqrt{3}}\\right)\n       \\simeq Pr(-2.89 \\le Z \\le 1.73)=\\]\n\\[=Pr(Z \\le 1.73)- Pr(Z \\le -2.89)=0.9582-(1-0.9981)=0.9582-0.0019=0.9563\\]\n{} Función de densidad de probabilidad conjunta\n\nLa probabilidad que dos variables aleatorias \\((x,y)\\) caigan en la región \\(R\\) (como por ejemplo un rectángulo) se obtiene integrando su función de probabilidad conjunta\n\\[P((x,y)\\in R)=\\int\\int_{R} F(x,y) dx dy\\,.\\]\nEn particular, si \\(R\\) es un rectángulo 2d \\({(x,y):r\\le x \\le r+dr, s \\le y \\le s+ds}\\), entonces\n\\[P((x,y)\\in R)=P(r\\le x \\le r+dr, s \\le y \\le s+ds)=\\int^{r+dr}_r\\int^{s+ds}_{s} F(x,y) dx dy\\,.\\]\nAlgunas propiedades:\n\n\n\\(F(x,y)\\ge0\\) para todo x,y.\n\n\\(\\int^{\\infty}_{-\\infty}\\int^{\\infty}_{-\\infty} F(x,y) dx dy=1\\)\n\n\nDefinición: La función de densidad de probabilidad marginal de variables aleatorias \\(x\\) e \\(y\\) son:\n\\(Fx(x)=\\int^{\\infty}_{-\\infty} F(x,y)dy\\) y \\(Fy(y)=\\int^{\\infty}_{-\\infty} F(x,y)dx\\,.\\)\n{} Ejemplo del uso de la función de densidad de probabilidad conjunta\n\nImaginemos que una empresa de instrumentación oceanográfica fabrica boyas Lagrangianas de grosor \\(x\\) y diámetro \\(y\\), los cuales varian de una boya a la otra. Imaginemos que la función de densidad de probabilidad conjunta de la variable aleatoria “dimensión del instrumento oceanográfico” es:\n\\[F(x,y)=\\frac{1}{6}(r+s)\\,\\,\\,si\\,\\,\\,(x,y)\\in R=\\{1\\le x \\le 2 ; 4 \\le y \\le 5\\}\\]\n\\[F(x,y)=0\\,\\,\\,si\\,\\,\\,(x,y)\\,fuera\\,de\\,R\\]\nAhora queremos saber que probabilidad hay de que una boya tenga un grosor \\(1 \\le x \\le 1.5m\\) y un diámetro \\(4.5 \\le y \\le 5m\\), es decir \\[P(1 \\le x \\le 1.5, 4.5 \\le y \\le 5)=\\int^{1.5}_{1} \\int^{5}_{4.5} \\frac{1}{6}(r+s) ds dr = 0.253= 25\\%\\]\n{} Significancia estadística utilizando la distribución Normal\n\nComo vimos en el teorema central del límite, para una población infinita (\\(N\\rightarrow\\infty\\)) la desviación estándar de la distribución de las medias muestrales es:\n\\[\\sigma(\\bar{x})=\\frac{\\sigma}{\\sqrt{N}}=error\\,estándar\\,del\\,estimado\\,de\\,la\\,media\\,.\\]\nAquí, \\(\\sigma\\) es la desviación estándar de la población y \\(N\\) es el número de datos (independoentes) utilizado para calcular la media muestral. Entonces, si promediamos observaciones de una población de desviación estándar \\(\\sigma\\), la desviación estándar de esos promedios disminuye como el inverso de la raíz cuadrada del tamaño muestral \\(N\\).\n\nSi \\(N\\) es suficientemente grande podemos usar las estimaciones de \\(\\sigma\\) y \\(\\bar{x}\\) para calcular el denominado estadístico \\(z\\) que corresponde a una distribución normal estandarizada de media \\(\\mu=0\\) y \\(\\sigma=1\\)\n\\[z=\\frac{\\bar{x}-\\mu}{\\sigma(\\bar{x})}=\\frac{\\bar{x}-\\mu}{\\frac{\\sigma}{\\sqrt{N}}}\\]\n\nLa fórmula de arriba puede modificarse convenientemente para darnos un test de significancia estadística para la diferencia entre medias muestrales con tamaños muestrales y desviación estándar diferentes:\n\\[z=\\frac{\\bar{x}_1-\\bar{x}_2-\\Delta_{1,2}}\n  {\\frac{\\sigma_1^2}{\\sqrt{N_1}} + \\frac{\\sigma_2^2}{\\sqrt{N_2}}}\\,,\\]\ndonde \\(\\Delta_{1,2}\\) es la diferencia esperada entre las dos medias, lo que se suele asumir cero en la práctica.\n\nSi el tamaño muestral \\(N\\) es menor de \\(30\\) entonces no podemos usar el estadístico \\(z\\), pero podemos utilizar la distribución t-student; o cuando queremos comparar varianzas, podemos usar la distribución chi-cuadrada. La t-student converge a una distribución normal para largos tamaños muestrales y se define como\n\\[t=\\frac{\\bar{x}-\\mu}{\\frac{s}{\\sqrt{N-1}}}=\\frac{\\bar{x}-\\mu}{\\frac{\\hat{s}}{\\sqrt{N}}};\n\\hat{s}=\\sqrt{\\frac{N}{N-1}s}\\,.\\]\nSi consideramos una población normalmente distribuida de media \\(\\mu\\) la función de densidad de probabilidad de la t-student es\n\\[F_{x}(t)=\\frac{f_0(\\nu)}{\\left(1+\\frac{t^2}{\\nu} \\right)^{\\frac{\\nu+1}{2}}}\\,,\\]\ndonde \\(\\nu=N-1\\) es el número de grados de libertad y \\(f_0(\\nu)\\) es una constante que depende en \\(\\nu\\) y permite que el área bajo la curva \\(F_x(t)\\) sea igual a la unidad. Los grados de libertad se definen como el número de muestras independientes \\(N\\) menos el número de parámetros del estadístico que queremos estimar.\n\nA diferencia del estadístico \\(z\\), la t-student depende del número de grados de libertad; la cola de la distribución es larga para números de grados de libertad bajos (o \\(N\\) pequeña). Para números altos de grados de libertad (o \\(N\\) grande), la distribución t-student se acerca al estadístico \\(z\\) o distribución Normal.\nIntervalos de confianza\n\nPara calcular valores de los estadísticos \\(z\\) y t-student debemos de fijar el nivel de confianza definido como \\(1-\\alpha\\); porcentaje del nivel de confianza \\(100(1-\\alpha)\\%\\). Esto se puede escribir simbolicamente cómo\n\\[P(-z_{\\alpha/2}&lt;z&lt;z_{\\alpha/2})=1-\\alpha\\]\n\\[P(-t_{\\alpha/2}&lt;t&lt;t_{\\alpha/2})=1-\\alpha\\,.\\]\nUna vez definido el nivel de confianza y los grados de libertad \\(\\nu\\) (para la t-student) podemos leer el valor de dichos estadísticos en tablas. En esas tablas \\(z_{\\alpha/2}\\) es el valor de \\(z\\) para el cual solo el \\(100*{\\alpha/2}\\%\\) de los valores de \\(z\\) es esperado ser mas grande (cola de la derecha de la distribución). Igualmente, \\(z_{-\\alpha/2}=-z_{\\alpha/2}\\) es el valor de \\(z\\) para el cual solo el \\(100*{\\alpha/2}\\%\\) de los valores de \\(z\\) es esperado ser mas pequeño (cola de la izquierda de la distribución). O dicho de otra forma,\\(z_{\\alpha/2}\\) es el valor por encima del cual existe un área bajo la curva de \\(\\alpha/2\\). Los valores de \\(z\\) y \\(t\\) son las integrales bajo las correspondientes funciones de densidad de probabilidad.\n\nIntervalo de confianza para \\(\\mu\\) (\\(N&gt;30\\), \\(\\sigma\\) conocida)\n\n\nCuando \\(N&gt;30\\) y \\(\\sigma\\) es conocida, podemos usar el estadístico \\(z\\) para encontrar el intervalo de confianza para \\(\\mu\\). Hay un \\(100*(1-\\alpha)\\%\\) que cualquier estadístico \\(z\\) caiga en el intervalo\n\\[z_{-\\alpha/2}&lt;\\frac{\\bar{x}-\\mu}{\\sigma}\\sqrt{N}&lt;z_{\\alpha/2}\\] \\[\\frac{\\sigma}{\\sqrt{N}}z_{-\\alpha/2}&lt;{\\bar{x}-\\mu}&lt;\\frac{\\sigma}{\\sqrt{N}}z_{\\alpha/2}\\] \\[-1\\frac{\\sigma}{\\sqrt{N}}z_{-\\alpha/2}&gt;{\\mu-\\bar{x}}&gt;-1\\frac{\\sigma}{\\sqrt{N}}z_{\\alpha/2}\\] \\[\\bar{x}-1\\frac{\\sigma}{\\sqrt{N}}z_{-\\alpha/2}&gt;\\mu&gt;\\bar{x}-1\\frac{\\sigma}{\\sqrt{N}}z_{\\alpha/2}\\]\ny sabiendo que es simétrica \\(-z_{\\alpha/2}=z_{-\\alpha/2}\\):\n\\[\\bar{x}-z_{\\alpha/2}\\frac{\\sigma}{\\sqrt{N}} &lt; \\mu &lt;\n   \\bar{x}+z_{\\alpha/2}\\frac{\\sigma}{\\sqrt{N}}\\,.\\]\nSupongamos que queremos encontrar el intervalo de confianza de \\(\\mu\\) al \\(95\\%\\) de confianza, es decir, entonces \\(\\alpha=0.05\\). Entonces \\(z_{\\alpha/2}=1.96\\) (de tablas estadísticas).\n\nEjemplo: \\(N=40\\), \\(\\sigma=0.5^\\circ{C}\\), y \\(\\bar{x}=12.7^\\circ{C}\\):\n\\[\\bar{x}-z_{0.025}\\frac{\\sigma}{\\sqrt{N}}&lt;\\mu&lt;\\bar{x}+z_{0.025}\\frac{\\sigma}{\\sqrt{N}}\\,,\\]\n\\[\\left[12.7-(1.96)0.5/\\sqrt{40}\\right]\\,^\\circ{C}&lt;\\mu&lt;\\left[12.7+(1.96)0.5/\\sqrt{40}\\right]\\,^\\circ{C}\\] \\[12.54^\\circ{C} &lt; \\mu &lt; 12.85^\\circ{C}\\]\n\nIntervalo de confianza para \\(\\mu\\) (\\(N&lt;30\\), \\(\\sigma\\) desconocida)\nCuando \\(N&lt;30\\) y \\(\\sigma\\) es desconocida, podemos usar el estadístico \\(t\\)-student para encontrar el intervalo de confianza para \\(\\mu\\). Hay un \\(100*(1-\\alpha)\\%\\) que cualquier estadístico \\(t\\) caiga en el intervalo\n\n\\[t_{-\\alpha/2}&lt;\\frac{\\bar{x}-\\mu}{s}\\sqrt{N-1}&lt;t_{\\alpha/2}\\,,\\]\n\\[\\bar{x}-t_{\\alpha/2}\\frac{s}{\\sqrt{N-1}} &lt; \\mu &lt;\n   \\bar{x}+t_{\\alpha/2}\\frac{s}{\\sqrt{N-1}}\\,.\\]\nSi \\(\\alpha=0.05\\), hay un 95% de probabilidad que cualquier estadístico \\(t\\) caiga en el intervalo \\[t_{-0.025}&lt;\\frac{\\bar{x}-\\mu}{s}\\sqrt{N-1}&lt;t_{0.025}\\,,\\] de lo cual podemos deducir que la verdadera media \\(\\mu\\) es de esperar con un 95% de confianza que caiga en el intervalo: \\[\\bar{x}-t_{0.025}\\frac{s}{\\sqrt{N-1}}&lt;\\mu&lt;\\bar{x}+t_{0.025}\\frac{s}{\\sqrt{N-1}}\\,.\\]\nDe forma general, podemos definir el intervalo de confianza como: \\[\\mu=\\bar{x}\\pm t_c\\frac{\\hat{s}}{\\sqrt{N}}\\,,\\] donde \\(t_c\\) es el valor crítico del estadístico \\(t\\) (límites del intervalo), el cual depende del número de grados de libertad y del nivel de confiabilidad deseado. El intervalo de confianza con el estadístico \\(z\\), el cual solo es apropiado para tamaños muestrales grandes (\\(N&gt;30\\)) donde la desviación estándar es conocida: \\[\\mu=\\bar{x}\\pm z_c\\frac{\\sigma}{\\sqrt{N}}\\,.\\] Observamos que la teoría para tamaños muestrales pequeños reemplaza el estadístico \\(z\\) por el \\(t\\) y utiliza una desviación estándar muestral modificada \\[\\hat{s}=\\sqrt{\\frac{N}{N-1}s}\\,.\\] % % {iferencias entre medias}\\ %Supongamos dos muestras de tamaño \\(N_1\\) y \\(N_2\\) extraidas de una población %con distribución normal con desviaciones estándar siguales. Supongamos %que las medias muestrales son \\(\\bar{x_1}\\) y \\(\\bar{x_2}\\) y las %desviaciones estándar muestrales son \\(s_1\\) y \\(s_2\\). Para comprobar %la hipótesis nula (\\(H_0\\)) que ambas muestras provienen de la misma población, %es decir \\(\\mu_1=\\mu_2\\) y \\(\\sigma_1=\\sigma_2\\) podemos usar la siguiente expresión\n\\[t=\\frac{(\\bar{x_1}-\\bar{x_2})-(\\mu_1-\\mu_2)}{\\sigma\\sqrt{\\frac{1}{N_1} + \\frac{1}{N_2}}};\\]\ndonde \\(\\nu=N_1+N_2-2\\).\n\n\nIntervalo de confianza para la diferencia de medias \\(\\mu_1 - \\mu_2\\)}\n\n\nEl teorema central del límite (TCL) para la diferencia de medias muestrales de dos poblaciones viene dado por\n\\[\\bar{x}_1-\\bar{x}_2\\,\\sim\\,{ N}(\\mu_{\\bar{x}_1-\\bar{x}_2},\\sigma_{\\bar{x}_1-\\bar{x}_2})\\,,\\]\n\ndonde \\[{ Media:}\\,\\,\\mu_{\\bar{x}_1-\\bar{x}_2}=\\mu_{\\bar{x}_1}-\\mu{\\bar{x}_2}=\\mu_1-\\mu_2\\]\n\\[\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,{ Varianza:}\\,\\,\\sigma^2_{\\bar{x}_1-\\bar{x}_2}=\n   \\sigma^2{\\bar{x}_1}+\\sigma^2{\\bar{x}_2}=\n   \\sqrt{  \\frac{{\\sigma_1}^2}{N_1} + \\frac{{\\sigma_2}^2}{N_2} }\\]\n\n Desviaciones estándar poblacionales (\\(\\sigma_1\\) y \\(\\sigma_2\\)) conocidas; \\(\\mu_1\\) y \\(\\mu_2\\) desconocidas} (estadístico z):\n\\[z=\\frac{\\bar{x}_1-\\bar{x}_2-(\\mu_1-\\mu_2)}\n         {\\sqrt{\\frac{{\\sigma_1}^2}{N_1}+\\frac{{\\sigma_2}^2}{N_2}}}\\]\n\n3.2 Desviaciones estándar poblacionales (\\(\\sigma_1\\) y \\(\\sigma_2\\)) y medias poblacionales (\\(\\mu_1\\) y \\(\\mu_2\\)) desconocidas (estadístico t):\n\\[t=\\frac{\\bar{x}_1-\\bar{x}_2-(\\mu_1-\\mu_2)}\n              {\\sqrt{\\frac{{s_1}^2}{N_1}+\\frac{{s_2}^2}{N_2}}}\\]\n\n3.3 Desviaciones estándar poblacionales (\\(\\sigma_1\\) y \\(\\sigma_2\\)) desconocidas pero iguales (estadístico t):\n\nSupongamos dos muestras de tamaño \\(N_1\\) y \\(N_2\\) extraídas de dos poblaciones Normales con desviaciones estándar iguales (\\(\\sigma_1=\\sigma_2\\)). Supongamos que conocemos las medias y desviaciones estándar muestrales \\(\\bar{x}_1\\) y \\(\\bar{x}_2\\) y \\(s_1\\) y \\(s_2\\). Para comprobar la hipótesis nula \\(H_o\\) que las muestran proceden de la misma población (\\(\\mu_1=\\mu_2\\) y \\(\\sigma_1=\\sigma_2\\)) usamos el estadístico t-score (o pooled t):\n\\[t=\\frac{\\bar{x}_1-\\bar{x}_2-(\\mu_1-\\mu_2)}\n         {\\hat{s}_d\\sqrt{\\frac{1}{N_1}+\\frac{1}{N_2}}}\\] \\ \\[\\hat{s}_d=\\sqrt{\\frac{(N_1-1)s^2_1 + (N_2-1)s^2_2}{N_1+N_2-2}}\\,,\\]\n\ndonde \\(\\nu=N_1+N_2-2\\) es el número de grados de libertad.\n\nEjemplo: Un ingeniero que diseña instrumentos oceanográficos está ineteresado en aumentar el tiempo durante el cuál la pintura “antifouling” evita que los microorganismos se peguen y crezcan sobre el instrumento oceanográfico. Se prueban dos fórmulas de pintura: fórmula 1 estándar y fórmula 2 con un nuevo ingrediente que aumenta el tiempo de acción.\nDe la experiencia se sabe que la desviación estándar del tiempo de acción de la pintura es de 8 días y ésta variabilidad no se vé afectada por el nuevo ingrediente. Se pintan 35 instrumentos con la fórmula 1 y otros 35 con la fórmula 2. Los tiempos promedios de acción del “antifouling” son de 116 días para la fórmula 1 y 112 días para la fórmula 2. ¿A qué conclusión puede llegar el ingeniero diseñador del instrumento sobre la eficacia del nuevo ingrediente, con un nivel de significancia de 0.01?\\\n\\(x_1\\equiv\\) Tiempo de acción “antifouling” fórmula 1\n\n\\(x_2\\equiv\\) Tiempo de acción “antifouling” fórmula 2\n\\(x_1\\sim\\) Desconocida (\\(\\mu_1,\\sigma_1=\\) 8 días)\n\\(x_2\\sim\\) Desconocida (\\(\\mu_2,\\sigma_2=\\) 8 días)\n\\(\\bar{x}_1-\\bar{x}_2\\,\\sim\\,{ N}(\\mu_1-\\mu_2,\\sigma_1/\\sqrt{N_1} + \\sigma_2/\\sqrt{N_2})\\) (TCL)\n\\(\\bar{x}_1=116\\) días\\ \\(\\bar{x}_2=112\\) días\n\\(N_1=N_2=35\\)  \\(\\alpha=0.01\\)\n\\(H_0:\\,\\mu_1 - \\mu_2 =0\\)\n\\(H_1:\\,\\mu_1 - \\mu_2 \\neq0\\)\n\nEl intervalo de confianza de la diferencia de medias \\(\\mu_1-\\mu_2\\)\n\\[\\bar{x}_1-\\bar{x}_2-z_{\\alpha/2} \\left(\n   \\frac{\\sigma_1}{\\sqrt{N_1}} + \\frac{\\sigma_2}{\\sqrt{N_2}}\\right)&lt;\n   \\mu_1-\\mu_2 &lt;\n   \\bar{x}_1-\\bar{x}_2+z_{\\alpha/2} \\left(\n   \\frac{\\sigma_1}{\\sqrt{N_1}} + \\frac{\\sigma_2}{\\sqrt{N_2}}\\right)\\]\n\\[\\bar{x}_1-\\bar{x}_2-2.33 (1.9124)&lt;\n   \\mu_1-\\mu_2 &lt;\n   \\bar{x}_1-\\bar{x}_2+2.33 (1.9124)\\]\n\\[\\bar{x}_1-\\bar{x}_2-4.4559&lt;\n   \\mu_1-\\mu_2 &lt;\n   \\bar{x}_1-\\bar{x}_2+4.4559\\]\n\\[4-4.4559&lt;\n   \\mu_1-\\mu_2 &lt;\n   4+4.4559\\,,\\]\n\ny el intervalo de confianza es:\n\n\\[-0.4559&lt;\\mu_1-\\mu_2&lt;8.4559\\,\\,{ al}\\,\\,99\\%\\,(\\alpha=0.01)\\]\n\\[H_0:\\,\\mu_1 - \\mu_2 = 0\\,\\,{ al}\\,\\,99\\%\\,,\\]\ny aceptamos hipótesis nula \\(H_0\\).\n\n\nIntervalo de confianza para la varianza:\n\nDistribución chi-cuadrada\n\nEn ocasiones queremos definir un intervalo de confianza para la varianza muestral. Para ello podemos usar el estadístico chi-cuadrado. Definamos\n\\[\\chi^2=(N-1)\\frac{s^2}{\\sigma^2}\\,.\\]\n\nPropiedades:\nPara definir el intervalo de confianza sabemos que hay un \\(100*(1-\\alpha)\\%\\) que cualquier estadístico \\(\\chi^2\\) caiga en el intervalo\n\\[\\chi^2_{1-\\alpha/2}&lt;(N-1)\\frac{s^2}{\\sigma^2}&lt;\\chi^2_{\\alpha/2}\\,,\\] \\[\\frac{1}{\\chi^2_{1-\\alpha/2}}&gt;\\frac{\\sigma^2}{(N-1)s^2}&gt;\\frac{1}{\\chi^2_{\\alpha/2}}\\,,\\]\ny entonces:\n\\[\\frac{(N-1)s^2}{\\chi^2_{\\alpha/2}}&lt;\\sigma^2&lt;\\frac{(N-1)s^2}{\\chi^2_{1-\\alpha/2}}\\,.\\]\nUsamos \\(1-{\\alpha/2}\\) porque la \\(\\chi^2\\) es positiva. El valor \\(\\chi^2_{\\alpha/2}\\) es mayor que el valor \\(\\chi^2_{1-\\alpha/2}\\). Las tablas dan la probabilidad a la derecha del valor.\nPara una población normalmente distribuida con desviación estándar \\(\\sigma\\), la función de densidad de probabilidad de la chi-cuadrada es: \\[F_x(\\chi)=f_0\\chi^{\\nu-2}e^{-\\frac{1}{2}\\chi^2};\\,\\,\\nu=N-1\\,.\\]\nPuesto que la distribución chi es asimétrica y positiva, si \\(\\alpha=0.05\\) (95% confianza), el intervalo de confianza para la varianza \\(\\sigma^2\\) como \\[\\frac{(N-1)s^2}{\\chi^2_{0.025}}&lt;\\sigma^2&lt;\\frac{(N-1)s^2}{\\chi^2_{0.975}}\\,,\\]\ny si leemos en las tablas para \\(\\nu=9\\) grados de libertad:\n\\[\\frac{(N-1)s^2}{19.023}&lt;\\sigma^2&lt;\\frac{(N-1)s^2}{2.700}\\,,\\]\nEjemplo: Supongamos que tenemos \\(\\nu=9\\) grados de libertad de nuestra estimación espectral de la componente meridional de la velocidad de la corriente. Sabemos que la varianza muestral de un pico espectral es \\(s^2=10\\,{ cm}\\,{ s}^2\\) ?`Cuál es el intervalo de confianza al 95% para la varianza?\n\nDe las tablas estadísticas vemos que para \\(\\nu=N-1=9\\) grados de libertad, \\(\\chi^2_{1-\\alpha/2}=\\chi^2_{0.095}=19.02\\) y \\(\\chi^2_{\\alpha/2}=\\chi^2_{0.025}=2.70\\). Entonces, el intervalo es:\n\\[\\frac{(9)10}{19.023}&lt;\\sigma^2&lt;\\frac{(9)10}{2.700}\\] \\[4.7\\,{ cm}^2\\,{ s}^{-2}&lt;\\sigma^2&lt;33.3\\,{ cm}^2\\,{ s}^{-2}\\]\n{rados de libertad}\\ El número de grados de libertad es el número de muestras independientes N menos el número de parámetros del estadístico que queremos estimar. Por ejemplo en el estadístico \\(t\\) \\[t=\\frac{\\bar{x}-\\mu}{\\frac{s}{\\sqrt{N-1}}}=\\frac{\\bar{x}-\\mu}{\\frac{\\hat{s}}{\\sqrt{N}}};\n\\hat{s}=\\sqrt{\\frac{N}{N-1}s}\\,,\\] calculamos la media muestral y la desviación estándar \\(s\\) a partir de los datos, pero la verdadera media \\(\\mu\\) debe ser estimada, por lo que \\(\\nu=N-1\\). Similarmente en el estadístico chi-cuadrada \\[\\chi^2=(N-1)\\frac{s^2}{\\sigma^2}\\,,\\] conocemos la varianza muestral \\(s^2\\) y el tamaño muestral \\(N\\), pero debemos estimar la verdadera varianza, y entonces \\(\\nu=N-1\\).\n{stadístico \\(F\\)}\\ Otro estadístico útil para tests espectrales es el estadístico \\(F\\). Si \\(s^2_1\\) y \\(s^2_2\\) son las varianzas de muestras aleatorias independientes de tamaño \\(N_1\\) y \\(N_2\\), tomadas de dos poblaciones Normales con la misma varianza \\(\\sigma^2\\), entonces \\[F=\\frac{s^2_1}{s^2_2}\\,,\\] es el valor de una variable aleatria cuya distribución es \\(F\\) con los parámetros \\(\\nu_1=N_1-1\\) y \\(\\nu_2=N_2-1\\). Este estadístico es muy útil en tests de significancia para los picos de los espectros frecuenciales de potencia. Los dos parámetros son los grados de libertad para las varianzas del cociente; \\(\\nu_1\\) para \\(s^2_1\\) y \\(\\nu_2\\) para \\(s^2_2\\).\n{ests para hipótesis}\\ Para usar los test de significancia estadística debemos de seguir 5 pasos:\n(1) Definir el nivel de confianza\n(2) Definir la hipótesis nulla \\(H_0\\) y su alternativa \\(H_1\\)\n(3) Definir el estadístico que usaremos\n(4) Definir la región crítica\n(5) Evaluar el estadístico y concluir\n\nEs muy importante definir correctamente la hipótesis nula. Es decir, estar seguro que rechazar la hipótesis nula \\(H_0\\) implica únicamente la existencia de su alternativa \\(H_1\\). Normalmente la hipótesis nula y su alternativa son mutualmente excluyentes. Ejemplos:\n\n\\(H_0\\): Las medias de dos muestras son iguales \\(H_1\\): Las medias de dos muestras no son iguales\n\n\\(H_0\\): El coeficiente de correlación es cero \\(H_1\\): El coeficiente de correlación no es cero\n\n\\ En una muestra de 41 inviernos la temperatura media de Enero es \\(5.55^\\circ{C}\\) y la desviación es de \\(0.65^\\circ{C}\\) ?`Cual es el intervalo de confianza al 95% de que la verdadera temperatura media sea esa? \\ (1) Nivel de confianza del 95% \\ (2) \\(H_0\\) es que la media verdadera se encuentra en el intervalo \\(5.55\\pm \\Delta{T}\\) y su alternativa \\(H_1\\) es que se encuentra fuera de este intervalo.\n(3) Usamos el estadístico \\(t\\).\n(4) La región crítica es \\(|t|&lt;t_{0.025}\\), lo cual para \\(\\nu=N-1=40\\) es \\(|t|&lt;2.26\\) (leido de tablas estadísticas). Escrito en términos de intervalo de confianza para la media poblacional: \\[\\bar{x}-2.0211\\frac{s}{\\sqrt{N-1}}&lt;\\mu&lt;\\bar{x}+2.0211\\frac{s}{\\sqrt{N-1}}  \\]\n(5) Si ponemos en números el intervalo obtenemos \\(5.06&lt;\\mu&lt;6.03\\). Tenemos un 95% de confianza que la verdadera temperatura media se encuentra en ese intervalo.\n{eorema de Bayes:}\\ Sea \\(E_i\\,,i=1,2,3,...,n\\) un conjunto de \\(n\\) eventos que constituyen una partición del espacio muestral \\(S\\)\n\\[\\bigcup^n_{i=1}E_i\\in S,\\]\n{}cada uno de los cuales tiene probabilidad positiva de ocurrir \\(P(Ei)&gt;0\\) para \\(i=1,2,....,n\\) y son exclusivos entre si \\[E_i\\cap E_j=\\o \\,\\,\\,\\,i\\ne j\\,.\\] Entonces dada la ocurrencia previa de un evento cualquiera \\(B\\), la probabilidad de que suceda el evento \\(E_j\\) es\n\\[\\begin{equation}\nP(E_j|B)=\\frac{P(B|E_j)P(E_j)}{\\sum^n_{i=1}P(B|E_i)P(E_i)}\\,,\n\\end{equation}\\]\n{}donde\n\\[P(E_j|E_i)=\\frac{P(E_i\\cap E_j)}{P(E_i)}\\,,\\]\nes la probabilidad condicional, es deicr, la probabilidad que ocurra el evento \\(E_j\\) si previamente ha ocurrido el evento \\(E_i\\) y \\(P(E_i \\cap E_j)\\) es la probabilidad que ambos eventos ocurran, i.e. la intersección de dos eventos. La intersección es puede escribir\n\\[P(E_i\\cap E_j)=P(E_j|E_i)*P(E_i)=P(E_i|E_j)*P(E_j)\\,.\\]\nSi ambos eventos son independientes (no interseccionan) tal que \\(P(E_i|E_j)=P(E_i)\\) obtenemos\n\\[P(E_i\\cap E_j)=P(E_i)*P(E_j)\\,,\\]\nes decir, la definición de independencia estadística entre eventos.\n\n{jemplo teorema de Bayes:} Imaginemos que queremos saber si una muestra de agua contiene diatomeas o no. La probabilidad de que una muestra de agua tomada al azar en la bahía de Todos Santos tenga diatomeas es de \\(1/100\\) (\\(P(D)=0.01\\)). La probabilidad de que si hay diatomeas el test de negativo es cero (\\(P(+|D)=1\\)) y la probabilidad de que el test de un falso positivo es del \\(5\\%\\) (\\(P(+|noD)=0.05\\)). Si agarramos una muestra de agua y da positivo, ?`Cual es la probabilidad de que hayan diatomeas? Intuitivamente, sabemos que existe un \\(5\\%\\) de probabilidad de que el test me de un falso positivo y por lo tanto, un \\(95\\%\\) de que si da positivo tenga diatomeas en la muestra de agua. Veamos que nos dice el teorema de Bayes. Sabemos que \\(P(D)=0.01\\), \\(P(+|D)=1\\), y \\(P(+|noD)=0.05\\). Si usamos el teorema de Bayes:\n\\[ P(D|+)=\\frac{P(+|D)P(D)}{P(+|D)P(D) + P(+|noD)P(noD)}=\n          \\frac{1*0.01}{1*0.01+0.05*0.99}\\sim\\frac{1}{6}\\,.\\]\nEn verdad solamente existe \\(1\\) probabilidad de \\(6\\) (\\(\\sim16\\%\\)) que si el test es positivo existan diatomeas. De cada \\(100\\) muestras solamente \\(1\\) tiene diatomeas y \\(5\\%*100=0.05*100=5\\) muestras de las \\(100\\) dan positivo y no tienen diatomeas. Por ello, sabiendo que la muestra que tiene diatomeas dio positivo, de \\(6\\) positivos, solamente \\(1\\) es cierto."
  },
  {
    "objectID": "chap4.html#repaso-de-álgebra-lineal",
    "href": "chap4.html#repaso-de-álgebra-lineal",
    "title": "Notas del Curso Análisis de Datos",
    "section": "Repaso de álgebra Lineal",
    "text": "Repaso de álgebra Lineal\nUna matriz es un elemento matemático compuesto por filas y columnas. Una matriz rectangular de dimensiones \\(m\\times n\\) se define como \\[{\\textbf A}=\\left(\\begin{array}{cccc}\n  a_{11} & a_{12} & .... & a_{1n}\\\\\n  a_{21} & a_{22} & .... & a_{2n}\\\\\n    .    &   .    &      &   . \\\\\n    .    &   .    &      &   . \\\\\n    .    &   .    &      &   . \\\\\n  a_{m1} & a_{m2} & .... & a_{mn}\\\\\n\\end{array}\n  \\right)\n\\,.\\]\nUn elemento de la matriz \\({\\textbf A}\\) queda definida como \\({\\textbf A}=[a_{ij}],\\,\\,\\,i=1,2,...,m; j=1,2,...,n\\,.\\) El primer índice \\(i\\) denota filas y el segundo \\(j\\) columnas.\n{efinición} Un vector es una matriz con solo una columna (\\(m\\times1\\)) \\[{\\textbf v}=\\left( \\begin{array}{c}\nv_1 \\\\ v_2 \\\\ . \\\\ . \\\\ . \\\\ v_m\n       \\end{array} \\right)\n       \\,.\\]\n{ectores ortogonales}\nSean dos vectores \\({\\textbf u}=[u_1,u_2,...,u_N]\\) y \\({\\textbf v}=[v_1,v_2,...,v_N]\\) de longitud \\(N\\), decimos que son ortogonales si \\[{\\textbf u}\\cdot{\\textbf v}=({\\textbf u},{\\textbf v})=\\sum\\limits^N_{i=1} u_i v_i={\\textbf u}^T{\\textbf v}=0\\,.\\]\n{orma, módulo, longitud de un vector}\nSea el vector \\({\\textbf u}=[u_1,u_2,...,u_N]\\), entonces la norma de dicho vector se define como \\[||{\\textbf u}||=|{\\textbf u}|=\\sqrt{u^2_1 + u^2_2 + ... + u^2_N}=({\\textbf u},{\\textbf u})^{1/2}=({\\textbf u}^T{\\textbf u})^{1/2}\\,.\\]\nEn general se puede obtener un {ector unitario} (de módulo \\(=1\\)) dividiendo el vector por su norma: \\[{\\textbf u}_I=\\frac{{\\textbf u}}{||{\\textbf u}||}\\,.\\]\n{} Vectores ortonormales\nDos vectores ortogonales \\({\\textbf u}\\) y \\({\\textbf v}\\) si tienen módulo la unidad, entonces se denominan vectores ortonormales.\n{uma de vectores}\nLa suma de 2 vectores \\({\\textbf u}=[u_1,u_2,...,u_N]\\) y \\({\\textbf v}=[v_1,v_2,...,v_N]\\) de longitud \\(N\\) se define como \\[{\\textbf u} + {\\textbf v}= [u_1 + v_1, u_2 + v_2,..., u_N + v_N]=\\sum\\limits^N_{i=1} u_i + v_i\\,.\\]\n{ombinación lineal de vectores}\nUn vector \\({\\textbf y}\\) se dice que es combinación lineal de un conjunto de vectores \\({\\textbf x}_1, {\\textbf x}_2, ...,{\\textbf x}_N\\) si se puede expresar como la suma de los \\(N\\) vectores multiplicados por \\(N\\) coeficientes escalares \\(a_1,a_2,...,a_N\\):\n\\[{\\textbf y}=a_1 {\\textbf x}_1 + a_2 {\\textbf x}_2 +...+a_N {\\textbf x}_N=\\sum\\limits^N_{i=1}a_i {\\textbf x}_i\\,.\\]\n{ndependencia lineal}\nUn conjunto de vectores \\({\\textbf y}_1, {\\textbf y}_2, ...,{\\textbf y}_N\\) se dice que es linealmente independiente si existe una combinación lineal finita de los vectores del conjunto tal que:\n\\[\\sum\\limits^N_{i=1} a_i {\\textbf y}_i=a_1{\\textbf y}_1 + a_2{\\textbf y}_2+...+a_N{\\textbf y}_N=0\\,,\\]\nque se satisface cuando no todos los coeficientes son cero. En caso contarrio, se dice que son linealmente dependientes.\n{} Ortonormalización Gram-Schmidt\nEs un método para convertir un conjunto de vectores \\({\\textbf v}\\) en vectores ortonormales. De forma general el proceso definido por Gram-Schmidt para ortonormalizar el vector ortogonal \\({\\textbf w}_k\\) a partir de un conjunto de vectores ortonormales \\({\\textbf u}_i=[{\\textbf u}_1, {\\textbf u}_2,...,{\\textbf u}_{k-1}]\\) se define como:\n\\[{\\textbf w}_k={\\textbf v}_k-\\sum\\limits^{k-1}_{i=1} {\\textbf u}_i\\cdot{\\textbf v}_k{\\textbf u}_i\\,.\\]\n\\[{\\textbf v}_k\\equiv\\,\\,{ vectores}\\,\\,{ originales}\\]\n\\[{\\textbf u}_i\\equiv\\,\\,{ vectores}\\,\\,{ ortonormales}\\]\n\\[{\\textbf w}_k\\equiv\\,\\,{ vectores}\\,\\,{ ortogonales}\\,\\,{ a}\\,\\,{\\textbf u}_j\\]\nEjemplo:\n\nConvertir el conjunto de vectores de la base \\(A\\) en una base ortonormal:\n\\[{\\textbf A}=\\left(\\begin{array}{cccc}\n  1 & 2 & 1\\\\\n  0 & 2 & 0\\\\\n  2 & 3 & 1\\\\\n  1 & 1 & 0\\\\\n\\end{array}\n  \\right)\n\\,.\\]\nPrimero normalizamos el primer vector columna \\({\\textbf v}_1\\): \\[{\\textbf u}_1=\\frac{{\\textbf v}_1}{||{\\textbf v}_1||}=\\left[\\frac{1}{\\sqrt{6}},0,\\frac{2}{\\sqrt{6}},\\frac{1}{\\sqrt{6}}\\right]\\,.\\]\n(1er vector ortonormal)\n\nAhora usamos la fórmula de arriba para encontrar el vector \\({\\textbf w}_2\\) ortogonal a \\({\\textbf u}_1\\)\n\\[{\\textbf w}_2={\\textbf v}_2- {\\textbf u}_1\\cdot{\\textbf v}_2{\\textbf u}_1=[2,2,3,1]-\n   \\left[\\frac{1}{\\sqrt{6}},0,\\frac{2}{\\sqrt{6}},\\frac{1}{\\sqrt{6}}\\right]\\cdot\n   [2,2,3,1]\\left[\\frac{1}{\\sqrt{6}},0,\\frac{2}{\\sqrt{6}},\\frac{1}{\\sqrt{6}}\\right]=\\]\n\\[=[2,2,3,1]-\\left(\\frac{9}{\\sqrt{6}}\\right)\\left[\\frac{1}{\\sqrt{6}},0,\\frac{2}{\\sqrt{6}},\\frac{1}{\\sqrt{6}}\\right]=\n   [2,2,3,1]-\\left[\\frac{3}{2},0,3,\\frac{3}{2}\\right]=\\left[ \\frac{1}{2},2,0,\\frac{-1}{2}\\right]\\,.\\]\n(vector ortogonal a \\(u_1\\))\nNormalizamos \\({\\textbf w}_2\\) para obtener el primer vector ortonormal a \\({\\textbf u}_1\\) \\[{\\textbf u}_2=\\frac{{\\textbf w}_2}{||{\\textbf w}_2||}=\\left[ \\frac{\\sqrt{2}}{6},\\frac{2\\sqrt{2}}{3},0,\\frac{-\\sqrt{2}}{6}\\right]\\]\n\n(2o vector ortonormal)\n\nAhora calculamos \\({\\textbf w}_3\\) en términos de \\({\\textbf u}_1\\) y \\({\\textbf u}_2\\) \\[{\\textbf w}_3=  {\\textbf v}_3- {\\textbf u}_1\\cdot{\\textbf v}_3{\\textbf u}_1 - {\\textbf u}_2\\cdot{\\textbf v}_3{\\textbf u}_2=\n  \\left[ \\frac{4}{9},\\frac{-2}{9},0,\\frac{-4}{9}\\right]\\,,\\] (vector ortogonal a \\(u_1\\) y \\(u_2\\)) \\ \\ y si normalizamos\n\n\\[{\\textbf u}_3=\\left[ \\frac{2}{3},\\frac{-1}{3},0,\\frac{-2}{3}\\right]\\,.\\]   (3er vector ortonormal)\n\nLa matriz o conjunto de vectores ortonormal es\n\\[{\\textbf A}=\\left(\\begin{array}{cccc}\n  \\frac{\\sqrt{6}}{6} & \\frac{\\sqrt{2}}{6} & \\frac{2}{3}\\\\\n  0 & \\frac{2\\sqrt{2}}{3} & \\frac{-1}{3}\\\\\n  \\frac{\\sqrt{6}}{3} & 0 & 0\\\\\n  \\frac{\\sqrt{6}}{6} & \\frac{-\\sqrt{2}}{6} & \\frac{-2}{3}\\\\\n\\end{array}\n  \\right)\n\\,\\] (base ortonormal)\n{} **Aplicación lineal*\nSean dos espacios vectoriales \\(V\\) y \\(W\\), decimos que una aplicación \\(f:V \\rightarrow W\\) es lineal si la `imagen’ de la combincación lineal es la combinación lineal de las imágenes. Es decir,\n\\[f(\\alpha {\\textbf u} + \\beta {\\textbf v} )=\\alpha f({\\textbf u}) + \\beta f({\\textbf v})\\,.\\]\nLa imagen de una aplicación es el resultado de aplicar al vector una aplicación.\nEjemplos:\n\nLa aplicación \\(f\\): \\({\\cal R}^3 \\rightarrow {\\cal R}^2\\) definida por \\(f(x,y,z)=(x+y,y+2z)\\) es lineal.\n\nDemostración: Definimos \\({\\textbf u}=[u_1,u_2,u_3]\\) y \\({\\textbf v}=[v_1,v_2,v_3]\\). Entonces \\(f\\) es lineal si \\[f(\\alpha (u_1,u_2,u_3) + \\beta(v_1,v_2,v_3))=\\alpha f(u_1,u_2,u_3) + \\beta f(v_1,v_2,v_3)=\\] \\[f(\\alpha u_1 + \\beta v_1,\\alpha u_2 + \\beta v_2,\\alpha u_3 + \\beta v_3 )=\\alpha [u_1 + u_2,u_2 + 2u_3] +\n\\beta [v_1 + v_2,v_2 + 2v_3]\\] \\[[\\alpha u_1 + \\beta v_1 + \\alpha u_2 + \\beta v_2,\\alpha u_2 + \\beta v_2 + 2 (\\alpha u_3 + \\beta v_3)]=\n[\\alpha u_1 + \\alpha u_2 + \\beta v_1 + \\beta v_2, \\alpha u_2 + 2\\alpha u_3 + \\beta v_2 + 2\\beta v_3]\\]\n\nLa aplicación \\(f\\): \\({\\cal R}^3 \\rightarrow {\\cal R}^2\\) definida por \\(f(x,y,z)=(x+y+1,y+2z)\\) no es lineal.\n\n\nLa aplicación \\({\\textbf R}\\): \\({\\cal R}^2 \\rightarrow {\\cal R}^2\\) definida por \\[{\\textbf x}'={\\textbf R}{\\textbf x}\\,,\\] \\[{\\textbf R}=\\left( \\begin{array}{cc}\ncos(\\theta) & -sin(\\theta)\\\\\nsin(\\theta) & cos(\\theta)\n   \\end{array} \\right)\\,,\\] rota el vector un ángulo \\(\\theta\\) en sentido contrario a las agujas del reloj. Si queremos cambiar el sentido de la rotación solo debemos tomar el signo de \\(\\theta\\) negativo. Aqui \\({\\textbf x}'\\) es el vector rotado o la imagen de la aplicación rotación.\n\n{atriz Identidad}\nSe define la matriz identidad \\({\\textbf I}\\) como una matriz diagonal compuesta por unos\n\\[{\\textbf I}=\\left(\\begin{array}{ccc}\n  1 & 0 & 0\\\\\n  0 & 1 & 0\\\\\n  0 & 0 & 1\n\\end{array}\\right)\\,.\\]\nEl producto matricial de cualquier matriz \\({\\textbf A}\\) por la matriz identidad \\({\\textbf I}\\) es igual a la matriz original \\[{\\textbf A}{\\textbf I}={\\textbf A}\\,.\\]\n{atriz transpuesta}\nSea \\({\\textbf A}\\) una matriz \\(m\\times n\\) con elementos \\([a_{ij}]\\,\\,{ para}\\,\\,i=1,2,...,m; j=1,2,...,n\\). Se define el elemento de su transpuesta como \\[{\\textbf A}^T=[a_{ji}]\\,.\\]\nLa inversa de la matriz transpuesta es \\[({\\textbf A}^T)^{-1}={\\textbf A}\\,,\\] si \\({\\textbf A}\\) es ortogonal. \\ {emostración}: \\[({\\textbf A}^T)^{-1}={\\textbf A}\\] \\[{\\textbf A}^T({\\textbf A}^T)^{-1}={\\textbf A}^T{\\textbf A}={\\textbf I}\\] \\[{\\textbf A}{\\textbf A}^T({\\textbf A}^T)^{-1}={\\textbf A}{\\textbf I}\\]\n{atriz Ortogonal}\nUna matriz \\({\\textbf A}\\) es ortogonal si \\({\\textbf A}{\\textbf A}^T={\\textbf A}^T{\\textbf A}={\\textbf I}\\,.\\)\n{atriz Diagonal}\nUna matriz es diagonal si únicamente contiene algunos elementos diferentes de cero en la diagonal principal (ceros en la matriz triangular superior e inferior).\n\\[{\\textbf D}=\\left(\\begin{array}{ccc}\n  a_{11} & 0 & 0\\\\\n  0 & a_{22} & 0\\\\\n  0 & 0 & a_{33}\n\\end{array}\\right)\n\\]\n{atriz Simétrica}\nUna matriz simétrica se define como aquella matriz \\({\\textbf A}\\) tal que sea igual a su traspuesta \\[{\\textbf A}={\\textbf A}^T\\,\\,(a_{ij}=a_{ji})\\]\n\\[{\\textbf A}=\\left(\\begin{array}{cccc}\n  a_{11} & a_{12} & a_{31}\\\\\n  a_{12} & a_{22} & a_{32}\\\\\n  a_{31} & a_{32} & a_{33}\\\\\n\\end{array}\\right)\n\\]\n{atriz Antisimétrica}\nUna matriz antisimétrica se define como aquella matriz \\({\\textbf A}\\) tal que sea igual a su traspuesta \\[{\\textbf A}=-{\\textbf A}^T\\,\\,(a_{ij}=-a_{ji})\\]\n\\[{\\textbf A}=\\left(\\begin{array}{cccc}\n  0 & a_{12} & a_{13}\\\\\n  -a_{12} & 0 & a_{23}\\\\\n  -a_{13} & -a_{23} & 0\\\\\n\\end{array}\\right)\n\\] \\ NOTA: Cualquier matriz \\({\\textbf A}\\) se puede descomponer en una parte simétrica y otra antisimétrica:\n\\[A=\\frac{1}{2}({\\textbf A}+{\\textbf A}^T) + \\frac{1}{2}({\\textbf A}-{\\textbf A}^T)\\,.\\]\n{atriz Singular}\nUna matriz singular es aquella matriz cuadrada cuyo determinante es igual a cero. Las matrices singulares no tienen matriz inversa. \\ \\ Ejemplo:\n\\[{\\textbf S}=\\left(\\begin{array}{cc}\n  3 & 2\\\\\n  6 & 4\n\\end{array}\\right)\n\\]\nSi nos fijamos las 2 filas de la matriz singular \\({\\textbf S}\\) son linealmente dependientes, es decir, podemos recuperar la segunda fila multiplicando la primera fila por 2. Si la matriz tiene columnas o filas linealmente dependientes, el determinante es cero.\nSi \\({\\textbf A}\\) es ortogonal\n\\[{\\textbf A}^{-1}={\\textbf A}^T\\] \\[{\\textbf A}^{-1}({\\textbf A}^T)^{-1}={\\textbf I}\\] \\[{\\textbf A}^{-1}\\underbrace{({\\textbf A}^T)^{-1}({\\textbf A}^T)}_{{\\textbf I}}={\\textbf A}^T\\]\nUna matriz cualquiera se puede convertir en matriz cuadrada si es multiplicada por su transpuesta.\n{eterminante de una matriz}\nEl determinante de una matriz \\({\\textbf A}\\) \\(3\\times 3\\) se puede calcular como \\[|{\\textbf A}|=\\left|\\begin{array}{cccc}\n  a_{11} & a_{12} & a_{13}\\\\\n  a_{21} & a_{22} & a_{23}\\\\\n  a_{31} & a_{32} & a_{33}\\\\\n\\end{array}\n  \\right|=a_{11}a_{22}a_{33} + a_{12}a_{23}a_{31} + a_{13}a_{21}a_{32}-(a_{31}a_{22}a_{13} + a_{32}a_{23}a_{11} +\n          a_{33}a_{21}a_{12})\\,.\\]\nPara matrices de orden superior se puede utilizar la formula de adjuntos. El determinante de una matriz \\(n\\times n\\) es el producto escalar entre cualquier fila o columna con sus adjuntos \\[|{\\textbf A}|=a_{i1}C_{i1} + a_{i2}C_{i2}+...+a_{in}C_{in}\\,,\\] donde los adjuntos \\(C_{ij}\\) son subdeterminantes (de orden \\(n-1\\), sin contar la columna j y fila i) con el signo adecuado \\[C_{ij}=(-1)^{i+j}|{\\textbf M}_{ij}|\\,.\\]\n{ango de una matriz} \\ \\ {efinición 1}: El rango de una matriz se define como el número de filas o columnas de la matriz que son linealmente independientes. \\ \\ {efinición 2}: El orden de la mayor submatriz cuadrada no nula\nAsí para calcular el rango debemos de hacer cero todos los elementos posibles de la matriz hasta obtener una submatriz no nula que nos indicará el rango de la matriz.\n\\[\\left[\\begin{array}{cccc}\n  1 & 2 & 1\\\\\n  -2 & -3 & 1\\\\\n  3 & 5 & 0\\\\\n\\end{array}\\right]\\underbrace{\\rightarrow}_{2r_1+r_2}\n\\left[\\begin{array}{cccc}\n1 & 2 & 1\\\\\n  0 & 1 & 3\\\\\n  3 & 5 & 0\\\\\n\\end{array}\\right]\\underbrace{\\rightarrow}_{-3r_1+r_3}\n\\left[\\begin{array}{cccc}\n1 & 2 & 1\\\\\n  0 & 1 & 3\\\\\n  0 & -1 & -3\\\\\n\\end{array}\\right]\\underbrace{\\rightarrow}_{r_2+r_3}\n\\left[\\begin{array}{cccc}\n1 & 2 & 1\\\\\n  0 & 1 & 3\\\\\n  0 & 0 & 0\\\\\n\\end{array}\\right]\\underbrace{\\rightarrow}_{-2r_2+r_1}\n\\left[\\begin{array}{cccc}\n1 & 0 & -5\\\\\n  0 & 1 & 3\\\\\n  0 & -1 & -3\\\\\n\\end{array}\\right]\\,,\\] y entonces el rango de la matriz es 2. El rango lo podíamos haber encontrado simplimente viendo que sustrayendo la segunda fila de la matriz a la primera obtenemos la tercera fila; solamente hay 2 vectores linealmente independientes.\n{atriz inversa}\nLa matriz cuadrada \\({\\textbf A}\\) es invertible si existe una matriz \\({\\textbf A}^{-1}\\) tal que \\[{\\textbf A}^{-1}{\\textbf A}={\\textbf I}\\,\\,\\,{ and}\\,\\,\\,{\\textbf A}{\\textbf A}^{-1}={\\textbf I}\\,.\\]\n{roposición} Si \\({\\textbf A}\\) es invertible entonces la única solución de \\({\\textbf A}{\\textbf x}={\\textbf b}\\) es \\[{\\textbf x}={\\textbf A}^{-1}{\\textbf b}\\,.\\]\n{roposición} Si \\({\\textbf A}\\) es una matriz cuadrada de dimensión \\(2\\times 2\\) y \\(|{\\textbf A}|=a_{11}a_{22} - a_{12}a_{21}\\) no es cero, entonces \\[{\\textbf A}^{-1}=\\left(\\begin{array}{cc}\n  a_{11} & a_{12} \\\\\n  a_{21} & a_{22} \\\\\n\\end{array}\n  \\right)^{-1} =\n  \\frac{1}{|{\\textbf A}|}\n  \\left(\\begin{array}{cc}\n  a_{22} & -a_{12} \\\\\n  -a_{21} & a_{11} \\\\\n\\end{array}\n  \\right)\n\\,.\\]\n{álculo de matriz inversa con método Gauss-Jordan}\nEl método de Gauss-Jordan usa la siguiente identidad matemática \\([{\\textbf A}|{\\textbf I}]\\rightarrow [{\\textbf I}|{\\textbf A}^{-1}]\\). \\ {jemplo:} Calcula la matriz inversa de \\[\\left(\\begin{array}{cc}\n  2 & 3 \\\\\n  4 & 7 \\\\\n\\end{array}\n  \\right)\\]\n\\[[{\\textbf A}  {\\textbf I}]=\n  \\left(\n        \\begin{array}{ccccc}\n  2 & 3 & & 1 & 0\\\\\n  4 & 7 & & 0 & 1\\\\\n        \\end{array}\n  \\right)\n  \\underbrace{\\rightarrow}_{-2r_1+r_2}\n  \\left(\n        \\begin{array}{ccccc}\n  2 & 3 & & 1 & 0\\\\\n  0 & 1 & & -2 & 1\\\\\n        \\end{array}\n  \\right)\n  \\underbrace{\\rightarrow}_{-3r_2+r_1}\n  \\left(\n        \\begin{array}{ccccc}\n  2 & 0 & & 7 & -3\\\\\n  0 & 1 & & -2 & 1\\\\\n        \\end{array}\n  \\right)\\rightarrow\\] \\[\\underbrace{\\rightarrow}_{r_1/2}\n  \\left(\n        \\begin{array}{ccccc}\n  1 & 0 & & 7/2 & -3/2\\\\\n  0 & 1 & & -2 & 1\\\\\n        \\end{array}\n  \\right)\n  \\,.\n  \\]\nEntonces\n\\[{\\textbf A}^{-1}=\\left(\n        \\begin{array}{ccccc}\n  7/2 & -3/2\\\\\n  -2 & 1\\\\\n        \\end{array}\n  \\right)\\,.\\]\n \\[{\\textbf A}=\\left[\\begin{array}{cccc}\n  1 & -1 & 2\\\\\n  2 & 0 & 3\\\\\n  0 & 1 & -1\\\\\n\\end{array}\\right]\\rightarrow\n{\\textbf A}^{-1}=\\left[\\begin{array}{cccc}\n  3 & -1 & 3\\\\\n  -2 & 1 & -1\\\\\n  -2 & 1 & -2\\\\\n\\end{array}\\right]\\,.\\]\n{actorización LU}\nEs la descomposición de una matriz en una matriz triangular inferior \\({\\textbf L}\\) y una matriz triangular superior \\({\\textbf U}\\), es decir \\[{\\textbf A}={\\textbf L}{\\textbf U}\\,,\\] donde \\[{\\textbf L}=\\left(\\begin{array}{cccc}\n  a_{12} & 0\\\\\n  a_{21} & a_{22}\\\\\n\\end{array}\\right)\\,\\] es una matriz triangular inferior (L; lower) y \\[\n{\\textbf U}=\\left(\\begin{array}{cccc}\n  a_{12} & a_{21}\\\\\n  0 & a_{12}\\\\\n\\end{array}\\right)\\,,\\] es una matriz triangular superior (U; superior).\\ Esta factorización se suele usar para resolver un sistema de ecuaciones lineal \\({\\textbf A}{\\textbf x}={\\textbf b}\\) y no es única. Es decir, pueden existir más de una factorización. Si sustituimos la definición de arriba \\[{\\textbf A}{\\textbf x}=({\\textbf L}{\\textbf U}){\\textbf x}={\\textbf b}\\,,\\] lo que implica que \\[{\\textbf L}({\\textbf U}{\\textbf x})={\\textbf b}\\,.\\] \\ Si definimos \\({\\textbf U}{\\textbf x}={\\textbf z}\\), entonces tenemos que \\[{\\textbf L}{\\textbf z}={\\textbf b}\\,.\\] Como \\({\\textbf L}\\) es una matriz triangular inferior, podemos resolver para \\({\\textbf z}\\) utilizando sustitución hacia delante. Luego, como \\({\\textbf U}\\) es una matriz triangular superior, resolvemos \\({\\textbf U}{\\textbf x}={\\textbf z}\\) por sustitución en reversa.\n{jemplo:}\nEncuentre la descomposición LU de la matriz \\[\\left(\\begin{array}{ccccc}\n  2 & 5\\\\\n-3 & -4\\\\\n        \\end{array}\\right)\\,.\n\\]\nSi multiplicamos la primera fila por \\(L_{21}=3/2\\) y le sumamos la segunda fila hacemos cero el elemento \\(a_{21}=-3\\) \\[{\\textbf U}=\\left(\\begin{array}{ccccc}\n  2 & 5\\\\\n0 & 7/2\\\\\n        \\end{array}\\right)\\,.\n\\] Esta matriz ya es una matriz triangular superior, es decir, la matriz \\({\\textbf U}\\). Para encontrar la matriz triangular inferior solo debemos de conocer el valor del elemento \\(L_{21}\\). Ese elemento es el multiplicador con signo opuesto usado en la eliminación de Gauss-Jordan. Es decir, \\(L_{21}=-3/2\\). La matriz \\({\\textbf L}\\) es \\[{\\textbf L}=\\left(\\begin{array}{ccccc}\n  1 & 0\\\\\n-3/2 & 1\\\\\n        \\end{array}\\right)\\,.\\]\nVamos a comprobar \\[\\left(\\begin{array}{ccccc}\n  1 & 0\\\\\n-3/2 & 1\\\\\n        \\end{array}\\right)\n    \\left(\\begin{array}{ccccc}\n  2 & 5\\\\\n0 & 7/2\\\\\n        \\end{array}\\right)\n    =\n    \\left(\\begin{array}{ccccc}\n  2 & 5\\\\\n-3 & -4\\\\\n        \\end{array}\\right)\\,.\\]\n{jemplo:}\nResuelva el siguiente sistema de ecuaciones con la descomposición LU \\[2x_1+3x_2+4x_3=6\\] \\[4x_1+5x_2+10x_3=16\\] \\[4x_1+8x_2+2x_3=2\\]\nLa matriz de coeficientes es \\[{\\textbf A}=\\left(\\begin{array}{ccccc}\n  2 & 3 & 4\\\\\n  4 & 5 & 10\\\\\n  4 & 8 & 2 \\\\\n        \\end{array}\\right)\\,.\n\\]\nSi factorizamos \\[{\\textbf L}=\\left(\\begin{array}{ccccc}\n  1 & 0 & 0\\\\\n  2 & 1 & 0\\\\\n  2 & -2 & 1 \\\\\n        \\end{array}\\right)\\,,\n\\]\ny\n\\[{\\textbf U}=\\left(\\begin{array}{ccccc}\n  2 & 3 & 4\\\\\n  0 & -1 & 2\\\\\n  0 & 0 & 0 \\\\\n        \\end{array}\\right)\\,.\\]\nSi utilizamos la identidad \\({\\textbf L}{\\textbf z}={\\textbf b}\\) obtenemos \\[\\left(\\begin{array}{ccccc}\n  1 & 0 & 0\\\\\n  2 & 1 & 0\\\\\n  2 & -2 & 1 \\\\\n        \\end{array}\\right)\n    \\left(\\begin{array}{c}\n  z_1 \\\\\n  z_2 \\\\\n  z_3 \\\\\n        \\end{array}\\right)=\n    \\left(\\begin{array}{c}\n  6 \\\\\n  16 \\\\\n  2 \\\\\n        \\end{array}\\right)\\,.\\]\nSi resolvemos para \\({\\textbf z}\\) \\[z_1=6\\] \\[z_2=16-2z_1=4\\] \\[z_3=2+2z_2-2z_1=-2\\]\nAsí que \\[{\\textbf z}=\\left(\\begin{array}{c}\n  6 \\\\\n  4 \\\\\n  -2 \\\\\n        \\end{array}\\right)\\,.\\]\nSi utilizamos ahora la definición \\({\\textbf U}{\\textbf x}={\\textbf z}\\,,\\) \\[\\left(\\begin{array}{ccccc}\n  2 & 3 & 4\\\\\n  0 & -1 & 2\\\\\n  0 & 0 & 0 \\\\\n        \\end{array}\\right)\n    \\left(\\begin{array}{c}\n  x_1 \\\\\n  x_2 \\\\\n  x_3 \\\\\n        \\end{array}\\right)=\n    \\left(\\begin{array}{c}\n  6 \\\\\n  4 \\\\\n  -2 \\\\\n        \\end{array}\\right)\\,,\\] y obtenemos \\[x_3=1\\] \\[x_2=\\frac{4-2x_3}{-1}=-2\\] \\[x_1=\\frac{6-4x_3-3x_2}{2}=4\\]\nPor lo tanto la solución del sistema de ecuaciones es \\[{\\textbf x}=\\left(\\begin{array}{c}\n  4 \\\\\n  -2\\\\\n  1 \\\\\n        \\end{array}\\right)\\,.\\]\n{alores propios y vectores propios}\nSea \\({\\textbf A}\\) una matriz cuadrada, un número real \\(\\lambda\\) se dice que es un valor propio de \\({\\textbf A}\\) si existe un vector, diferente del vector cero, \\({\\textbf x}\\) tal que \\[{\\textbf A}{\\textbf x}=\\lambda{\\textbf x}\\,.\\]\nEs decir, \\({\\textbf x}\\) es un vector que al transformarlo mediante la multiplicación por \\({\\textbf A}\\) el vector resultante mantiene la misma dirección; solamente se modifica su longitud (magnitud) y/o sentido. El valor propio \\(\\lambda\\) nos informa si el vector propio \\({\\textbf x}\\) se acorta o alarga o cambia de signo cuando es multiplicado por \\({\\textbf A}\\).\n{efinición} El número \\(\\lambda\\) es un valor propio si y solo si \\[|{\\textbf A}-\\lambda{\\textbf I}|=0\\,.\\]\n{jemplo:}\nCalcula los valores propios y vectores propios de la matriz \\[{\\textbf A}=\\left(\\begin{array}{ccccc}\n  1 & 2\\\\\n  2 & 4\\\\\n        \\end{array}\\right)\\,.\\]\nSabemos \\[|{\\textbf A}-\\lambda{\\textbf I}|=\\left|\\begin{array}{ccccc}\n  1-\\lambda & 2\\\\\n  2 & 4-\\lambda\\\\\n        \\end{array}\\right|=\\lambda^2-5\\lambda=0\\,.\\]\nEl polinomio de arriba se llama polinomio característico y es igual a cero cuando \\(\\lambda\\) es un valor propio. Resolviendo obtenemos dos soluciones \\(\\lambda=0\\) y \\(\\lambda=5\\). Ahora para encontrar los vectores propios debemos resolver el sistema \\(({\\textbf A}-\\lambda{\\textbf I}){\\textbf x}=0\\) separadamente para las dos \\(\\lambda\\): \\[({\\textbf A}-0{\\textbf I}){\\textbf x}=\n\\left(\\begin{array}{ccccc}\n  1 & 2\\\\\n  2 & 4\\\\\n      \\end{array}\\right)\n      \\left(\\begin{array}{c}\n  x_1 \\\\\n  x_2 \\\\\n      \\end{array}\\right)=\n      \\left(\\begin{array}{c}\n  0 \\\\\n  0 \\\\\n      \\end{array}\\right)\\,\\,\\,\\rightarrow\\,\\,\\,{\\textbf x}=\\left(\\begin{array}{c}\n  2 \\\\\n  -1 \\\\\n      \\end{array}\\right)\\,\\,\\,{ para}\\,\\,\\,\\lambda_1=0\\,,\\]\ny\n\\[({\\textbf A}-5{\\textbf I}){\\textbf x}=\n\\left(\\begin{array}{ccccc}\n  -4 & 2\\\\\n  2 & -1\\\\\n      \\end{array}\\right)\n      \\left(\\begin{array}{c}\n  x_1 \\\\\n  x_2 \\\\\n      \\end{array}\\right)=\n      \\left(\\begin{array}{c}\n  0 \\\\\n  0 \\\\\n      \\end{array}\\right)\\,\\,\\,\\rightarrow\\,\\,\\,{\\textbf x}=\\left(\\begin{array}{c}\n  1 \\\\\n  2 \\\\\n      \\end{array}\\right)\\,\\,\\,{ para}\\,\\,\\,\\lambda_2=5\\,.\\]\n Calcule los valores y vectores propios de la matriz de rotación\n\\[{\\textbf R}=\n\\left(\\begin{array}{ccccc}\n  cos\\theta & -sen\\theta\\\\\n  sen\\theta & cos\\theta\\\\\n      \\end{array}\\right)\\,.\\]\n{ultiplicación de matrices}\nEl producto matricial de dos matrices \\({\\textbf A}\\) (\\(m\\times n\\)) y \\({\\textbf B}\\) (\\(n\\times p\\)) se define como \\[{\\textbf C}={\\textbf A}{\\textbf B}\\,,\\] donde \\({\\textbf C}\\) es una matriz \\(m\\times p\\), con el elementi \\((i,j)\\) definido por \\[c_{ij}=\\sum^n_{k=1} a_{ik} b_{kj}\\,,\\] para todo \\(i=1,2,...,m; j=1,2,...,p\\).\n{ultiplicación de matrices}\nSea \\({\\textbf A}\\) una matriz \\(m\\times n\\), y \\({\\textbf v}\\) un vector \\(n\\times 1\\), entonces el elemento del producto \\[{\\textbf z}={\\textbf A}{\\textbf v}\\] viene dado por \\[z_{i}=\\sum^n_{k=1} a_{ik} v_{k}\\,,\\] para todo \\(i=1,2,....,m\\). Similarmente, si \\({\\textbf u}\\) es un vector \\(m\\times 1\\), entonces el elemento del producto \\[{\\textbf z}^T={\\textbf u}^T{\\textbf A}\\] viene dado por \\[z_{i}=\\sum^n_{k=1} a_{ki} u_{k}\\,,\\] para todo \\(i=1,2,....,n\\). Finalmente, el escalar que resulta del producto \\(\\alpha={\\textbf u}^T{\\textbf A}{\\textbf v}\\,,\\) viene dado por \\[\\alpha=\\sum^m_{j=1}\\sum^n_{k=1}a_{jk} u_j v_k\\,.\\]\n{roposición:}\nSea \\({\\textbf C}={\\textbf A}{\\textbf B}\\) una matriz \\(m\\times p\\), entonces \\[{\\textbf C}^T={\\textbf B}^T {\\textbf A}^T\\,.\\]\n{roposición} Sea \\({\\textbf A}\\) y \\({\\textbf B}\\) matrices cuadradas \\(n\\times n\\) invertibles. Sea el producto matricial \\({\\textbf C}={\\textbf A}{\\textbf B}\\), entonces \\[{\\textbf C}^{-1}={\\textbf B}^{-1} {\\textbf A}^{-1}\\,.\\]\n{erivada de matrices}\n{roposición:}\nSea \\[{\\textbf y}={\\textbf A}{\\textbf x}\\,,\\] donde \\({\\textbf y}\\) es \\(m\\times 1\\), \\({\\textbf x}\\) es \\(n\\times 1\\), \\({\\textbf A}\\) es \\(m\\times n\\), y \\({\\textbf A}\\) no depende de \\({\\textbf x}\\), entonces la derivada de \\({\\textbf y}\\) es \\[\\frac{\\partial{\\textbf y}}{\\partial{\\textbf x}}={\\textbf A}\\,.\\]\n{roposición:}\nSea \\[{\\textbf y}={\\textbf A}{\\textbf x}\\,,\\] donde \\({\\textbf y}\\) es \\(m\\times 1\\), \\({\\textbf x}\\) es \\(n\\times 1\\), \\({\\textbf A}\\) es \\(m\\times n\\), y \\({\\textbf A}\\) no depende de \\({\\textbf x}\\). Supongamos que \\({\\textbf x}\\) es una función del vector \\({\\textbf z}\\), mientras que \\({\\textbf A}\\) es independiente de \\({\\textbf z}\\). Entonces \\[\\frac{\\partial{\\textbf y}}{\\partial{\\textbf z}}={\\textbf A}\\frac{\\partial{\\textbf x}}{\\partial{\\textbf z}}\\,.\\]\n{roposición:}\nSea el escalar \\(\\alpha\\) definido como \\[\\alpha={\\textbf x}^T{\\textbf A}{\\textbf x}\\,,\\] donde \\({\\textbf x}\\) es \\(n\\times 1\\), \\({\\textbf A}\\) es \\(n\\times n\\), y \\({\\textbf A}\\) es independiente de \\({\\textbf x}\\), entonces \\[\\frac{\\partial{\\alpha}}{\\partial{{\\textbf x}}}={\\textbf x}^T({\\textbf A}+{\\textbf A}^T)={\\textbf x}^T{\\textbf A}^T + {\\textbf x}^T{\\textbf A}={\\textbf x}({\\textbf A}^T+{\\textbf A})\\]\n{roposición:}\nSea el escalar \\(\\alpha\\) definido como \\[\\alpha={\\textbf y}^T{\\textbf A}{\\textbf x}\\,,\\] donde \\({\\textbf y}\\) es \\(m\\times 1\\), \\({\\textbf x}\\) es \\(n\\times 1\\), y \\({\\textbf A}\\) es \\(m\\times n\\), y \\({\\textbf A}\\) es independiente de \\({\\textbf x}\\) y \\({\\textbf y}\\), entonces \\[\\frac{\\partial{\\alpha}}{\\partial{{\\textbf x}}}={\\textbf y}^T{\\textbf A}\\] y \\[\\frac{\\partial{\\alpha}}{\\partial{{\\textbf y}}}={\\textbf x}^T{\\textbf A}\\,.\\]\n{efinición} Sea \\({\\textbf A}\\) una matriz \\(m\\times n\\) cuyos elementos son funciones de un escalar \\(\\alpha\\). Entonces la derivada de \\({\\textbf A}\\) con respecto \\(\\alpha\\) es una matriz \\(m\\times n\\) compuesta por las derivadas de elemento por elemento: \\[\\frac{\\partial{\\textbf A}}{\\partial{\\alpha}}=\\left(\\begin{array}{cccc}\n  \\frac{\\partial{a_{11}}}{\\partial{\\alpha}} & \\frac{\\partial{a_{12}}}{\\partial{\\alpha}} & .... &\n   \\frac{\\partial{a_{1n}}}{\\partial{\\alpha}}\\\\\n  \\frac{\\partial{a_{21}}}{\\partial{\\alpha}} & \\frac{\\partial{a_{22}}}{\\partial{\\alpha}} & .... &\n   \\frac{\\partial{a_{2n}}}{\\partial{\\alpha}}\\\\\n    .    &   .    &      &   . \\\\\n    .    &   .    &      &   . \\\\\n    .    &   .    &      &   . \\\\\n  \\frac{\\partial{a_{m1}}}{\\partial{\\alpha}} & \\frac{\\partial{a_{m2}}}{\\partial{\\alpha}} & .... &\n   \\frac{\\partial{a_{mn}}}{\\partial{\\alpha}}\\\\\n\\end{array}\n  \\right)\n\\,.\\]"
  },
  {
    "objectID": "chap4.html#cuadrados-mínimos-y-regresión",
    "href": "chap4.html#cuadrados-mínimos-y-regresión",
    "title": "Notas del Curso Análisis de Datos",
    "section": "Cuadrados mínimos y regresión",
    "text": "Cuadrados mínimos y regresión\nEn esta sección se van a introducir algunos modelos lineales estadísticos o modelos de regresión. Aqui se incluyen ajustes lineales por cuadrados mínimos, coeficientes de correlación, regresión múltiple, etc.\n\nMétodos de cuadrados mínimos\nEstos métodos son utilizados para ajustar un modelo dependiente de un conjunto compuesto por \\(k\\) variables independientes \\(x_k;i=1,2,...,k\\).\n{A) Mínimos cuadrados lineales} \\ Empezamos aplicando el método en términos de estimación lineal. Nos referimos a lineal en cuanto a los coeficientes \\(b_0, b_1,...,b_k\\), es decir, \\(y=b_0 + b_1x_1 + \\epsilon\\) es lineal, pero \\(y=b_0+sin(b_1x_1)\\) no lo es. \\\n\n{juste de una recta a un conjunto de datos} \\ Queremos usar los mejores coeficientes \\(b_0\\) y \\(b_1\\) en el sentido que se reduzca la desviación estándar de la recta ajustada versus los datos. Sea \\(i=1,2,...,N\\) observaciones, entonces \\[y_i=\\hat{y_i} + \\epsilon\\,,\\] donde \\[\\hat{y_i}=b_0+b_1 x_i\\] es el estimador estadístico y \\(\\epsilon\\) es el residuo (medida de la diferencia de la recta ajustada versus conjunto de puntos). Para encontrar \\(b_0, \\, b_1\\) debemos de minimizar la suma de los errores cuadrados (SEC), donde SEC es la varianza total que no es explicada por nuestro modelo de regresión lineal \\[SEC=\\sum^N_{i=1} \\epsilon^2_i=\\sum^N_{i=1} (y_i - \\hat{y_i})^2=\\sum^N_{i=1} [y_i - (b_0+b_1 x_i)]^2\\,.\\] \\\n\nLa suma de los cuadrados totales se define como \\[SCT=\\sum^N_{i=1} (y_i - \\bar{y})^2\\,,\\] y la suma de los cuadrados residuales \\[SCR=\\sum^N_{i=1} (\\hat{y_i} - \\bar{y})^2\\,.\\] \\ La SCT es proporcional a la varianza de los datos y SCR es proporcional a la cantidad de varianza explicada por nuestra regresión. La varianza total (o SCT) se puede descomponer en función de SCR y SEC como: \\[SCT=\\sum^N_{i=1}(y_i-\\bar{y})^2 = \\sum^N_{i=1}(\\hat{y_i} + \\epsilon_i - \\bar{y})^2\n=\\sum^N_{i=1}((\\hat{y_i}- \\bar{y}) + \\epsilon_i)^2=\\sum^N_{i=1}(\\hat{y_i}- \\bar{y})^2 +\\] \\[+\\sum^N_{i=1}\\epsilon_i^2 + 2\\sum^N_{i=1}(\\hat{y_i}- \\bar{y})\\epsilon_i\\,,\\] y por lo tanto \\[SCT=SCR + SEC\\,\\] \\ ya que por las ecuaciones normales \\(2\\sum^N_{i=1}(\\hat{y_i}- \\bar{y})\\epsilon_i=0\\).\n\nEl problema de mínimos cuadrados consiste en minimizar la SEC con respecto los coeficientes, cuyas condiciones son \\[\\frac{\\partial{SEC}}{\\partial{b_0}}=0; \\frac{\\partial{SEC}}{\\partial{b_1}}=0\\,.\\]\n\nSubstituyendo obtenemos para \\(b_0\\) \\[\\frac{\\partial{SEC}}{\\partial{b_0}} =\\frac{\\partial}{\\partial{b_0}}\n  \\left\\{\n    \\sum^N_{i=1} [y_i - (b_0+b_1 x_i)]^2\n  \\right\\}=\n  -2\\sum^N_{i=1} [y_i - (b_0+b_1 x_i)]=\\] \\[=-2\\left(\\sum^N_{i=1}y_i -N b_0 - b_1\\sum^N_{i=1} x_i  \\right)=0\\,,\\] y para \\(b_1\\) \\[\\frac{\\partial{SEC}}{\\partial{b_1}} =\\frac{\\partial{}}{\\partial{b_1}}\n  \\left\\{\n    \\sum^N_{i=1} [y_i - (b_0+b_1 x_i)]^2\n  \\right\\}=\n  -2\\sum^N_{i=1} x_i [y_i - (b_0+b_1 x_i)]=\\] \\[=-2\\left(\\sum^N_{i=1}x_i y_i - b_0\\sum^N_{i=1}x_i - b_1\\sum^N_{i=1} x^2_i \\right)=0\\,.\\]\n\nSi resolvemos para \\(b_0\\)\n\\[b_0=\\bar{y}-b_1\\bar{x}\\,,\\]\ny sustituimos en la segunda ecuación, obtenemos:\n\\[b_1=\\frac{N \\sum\\limits^N_{i=1} x_i y_i - \\sum\\limits^N_{i=1}x_i \\sum\\limits^N_{i=1}y_i}\n           {N\\sum\\limits^N_{i=1}x^2_i-\\left(\\sum\\limits^N_{i=1}x_i \\right)^2}=\n      \\frac{\\sum\\limits^N_{i=1} x_i y_i - \\frac{1}{N}\\sum\\limits^N_{i=1}x_i \\sum\\limits^N_{i=1}y_i}\n           {\\sum\\limits^N_{i=1}x^2_i-\\frac{1}{N}\\left(\\sum\\limits^N_{i=1}x_i \\right)^2}=\\] \\[=\\frac{(N-1)C_{xy}}{(N-1)s^2_x}=\\frac{\\sum\\limits^N_{i=1} (x_i-\\bar{x})(y_i-\\bar{y})}\n                                      {\\sum\\limits^N_{i=1} (x_i-\\bar{x})^2}=\\frac{&lt;x'y'&gt;}{&lt;x'^2&gt;}\\,,\\] donde \\(C_{xy}=&lt;x'y'&gt;\\) es la covarianza entre \\(x\\) e \\(y\\), y \\(s^2_x=&lt;x'^2&gt;\\) es la varianza de \\(x\\). Así, una vez hemos calculado las medias de las variables \\(x\\) e \\(y\\), podemos encontrar el coeficiente \\(b_1\\) y, a partir de este, calcular el segundo coeficiente \\(b_0\\).\n\nSi sustituimos \\(b_0=\\bar{y}-b_1\\bar{x}\\) en la ecuación de la regresión lineal \\(\\hat{y_i}=b_0+b_1 x_i\\)\nobtenemos\n\\[\\hat{y_i}=\\bar{y} +b_1(x_i-\\bar{x})\\,,\\] o\n\\[\\hat{y_i}=\\bar{y} +b_1{x'_i}\\,,\\] o finalmente\n\\[\\hat{y'_i}=b_1{x'_i}\\,,\\]\nque nos informa que cuando \\(x'_i=0\\,(x_i=\\bar{x})\\) entonces \\(\\hat{y'_i}=0 \\,(\\hat{y}=\\bar{y})\\), es decir, la recta pasa por el punto \\((\\bar{x},\\bar{y})\\), de tal forma, que puesto que \\(\\partial{SEC}/{\\partial{b_0}}=0\\) minimiza la suma del error, \\(\\sum \\epsilon_i=0\\), los puntos estan dispersos respecto a la recta ajustada de tal forma que los residuos positivos (\\(\\epsilon&gt;0\\)) siempre se cancelan con los residuos negativos (\\(\\epsilon&lt;0\\)). El parámetro \\(b_0\\) se interpreta como la intersección (corte con el eje \\(y\\)) y \\(b_1\\) es la pendiente de la recta ajustada.\n\nEl cociente\n\\[100(SCR/SCT)\\,,\\]\nes el porcentaje de varianza explicada por nuestra regresión lineal (varianza explicada/varianza total) y nos informa de la bondad del ajuste denominado coeficiente de correlación, \\(r^2\\). Si la regresión se ajusta perfectamente a todos los datos, todos los residuos son cero y por lo tanto \\(SEC=0\\) y \\(SCR/SCT=r^2=1\\). A medida que el ajuste empeora el coeficiente \\(r^2\\) disminuye hasta un mínimo posible de \\(r^2=0\\).\n{rror estándar de la estimación}\nUna medida de la magnitud absoluta de la bondad del ajuste es el error estándar del estimado, \\(s_{\\epsilon}\\), definido como\n\\[s_{\\epsilon}=[SEC/(N-2)]^{1/2}=\\left[ \\frac{1}{N-2}\\sum\\limits^N_{i=1} (y-\\hat{y})^2\\right]^{1/2}\\,.\\]\nEl número de grados de libertad, \\(N-2\\), se debe a que necesitamos estimar dos parámetros para encontrar realizar la regresión lineal. Si \\({\\epsilon}\\) es una variable aleatoria que sigue una distribución Normal de media cero y desviación estándar \\(s_{\\epsilon}\\), entonces, el \\(68.3\\%\\) de las observaciones caen dentro del intervalo \\(\\pm 1s_{\\epsilon}\\) unidades de la recta ajustada, \\(95.4\\%\\) caerá dentro del intervalo \\(\\pm 2 s_{\\epsilon}\\) unidades de la recta, y \\(99.7\\%\\) caerán en el intervalo \\(\\pm 3 s_{\\epsilon}\\) unidades de la recta. \\(s_{\\epsilon}\\) es la desviación estándar de \\(y\\) alrededor de su media, i.e., la recta ajustada \\(b_0 + b_1 x\\).\nGeneralización de mínimos cuadrados en notación matricial\nSupongamos el modelo dependiente de \\(k\\) variables independientes \\(X_k\\) \\[Y=b_0 + b_1 X_1 + b_2 X_2 + .... + b_k X_k + \\epsilon\\,,\\] y supongamos que hacemos N observaciones independientes \\(y_1, y_2,....,y_N\\) de \\(Y\\). Por lo tanto podemos escribir el modelo como \\[y_i=b_0 + b_1 x_{i1} + b_2 x_{i2} + ... + b_k x_{ik} + \\epsilon_i\\,,\\] donde \\(x_{ij}\\) es la observación \\(i\\) del de la variable independiente \\(j\\). Es decir, \\[N;\\,\\,{ observaciones}\\] \\[k;\\,\\,{ variables\\,\\,independientes}\\] \\[k+1;\\,\\,{ coeficientes}\\]\nSi escribimos en notación matricial \\[{\\textbf Y}=\\left(\\begin{array}{c}\n  y_1\\\\\n  y_2\\\\\n  ...\\\\\n  ...\\\\\n  ...\\\\\n  y_N\n      \\end{array}\\right)\\,\\,\\,{\\textbf X}=\\left(\\begin{array}{cccc}\n  1 & x_{11} & ... & x_{1k}\\\\\n  1 & x_{21} & ... & x_{2k}\\\\\n  ... & ... & ... & ...\\\\\n  ... & ... & ... & ...\\\\\n  1 & x_{N1} & ... & x_{Nk}\\\\\n      \\end{array}\\right)\\] \\[{\\textbf B}=\\left(\\begin{array}{c}\n  b_0\\\\\n  b_1\\\\\n  ...\\\\\n  ...\\\\\n  ...\\\\\n  b_k\n      \\end{array}\\right)\\,\\,\\,{\\textbf E}=\\left(\\begin{array}{c}\n  \\epsilon_1\\\\\n  \\epsilon_2\\\\\n  ...\\\\\n  ...\\\\\n  ...\\\\\n  \\epsilon_N\n      \\end{array}\\right)\\,.\\] \\ \\ De esta forma, podemos escribir nuestro modelo en notación matricial como \\[{\\textbf Y}={\\textbf X}{\\textbf B} + {\\textbf E}\\,.\\] \\ \\ Si restringimos el modelo a una variable independiente (\\(k=1\\)), i.e. dos coeficientes \\((b_0, b_1)\\), entonces \\[{\\textbf B}=\\left(\\begin{array}{c}\n  b_0\\\\\n  b_1\\\\\n      \\end{array}\\right)\\,,\\] es la matriz de coeficientes, e \\[{\\textbf X}=\\left(\\begin{array}{cc}\n  1& x_{11}\\\\\n  1& x_{21}\\\\\n  ... & ...\\\\\n  ... & ...\\\\\n   ... & ...\\\\\n1 & x_{N1}\n\\end{array}\\right)\\,,\\] es la matriz de variables independientes. Si utilizamos la definición de los residuos como \\[{\\textbf E}={\\textbf Y}-{\\textbf X}{\\textbf B}\\,,\\] podemos encontrar la suma de los residuos al cuadrado como \\[SEC=\\sum^N_{i=1} \\epsilon^2_i=\\sum^N_{i=1} \\epsilon_i \\epsilon_i=\n{\\textbf E}^T{\\textbf E}=({\\textbf Y}-{\\textbf X}{\\textbf B})^T({\\textbf Y}-{\\textbf X}{\\textbf B})=\\] \\[{\\textbf Y}^T{\\textbf Y} - {\\textbf Y}^T{\\textbf X}{\\textbf B}-{\\textbf B}^T{\\textbf X}^T{\\textbf Y}+{\\textbf B}^T\n{\\textbf X}^T{\\textbf X}{\\textbf B}\\,.\\]\nSi queremos minimizar esa suma de errores entonces \\[\\frac{\\partial{SEC}}{\\partial{\\textbf B}}=0\\,.\\] Para calcular las derivadas de \\({SEC}\\) respecto de los coeficientes \\({\\textbf b}\\) vamos a asumir las siguientes consideraraciones \\ \\ (i) \\({\\textbf Y}^T{\\textbf X}{\\textbf B}={\\textbf B}^T{\\textbf X}^T{\\textbf Y}\\) ya que son matrices elemento (\\(1\\times1\\)) y siempre son simétricas. Por lo tanto \\(-{\\textbf Y}^T{\\textbf X}{\\textbf B}-{\\textbf B}^T{\\textbf X}^T{\\textbf Y}=-2{\\textbf B}^T{\\textbf X}^T{\\textbf Y}\\), y su derivada \\[-2{\\textbf X}^T{\\textbf Y}\\,.\\] \\ \\ (ii) \\(\\partial{{\\textbf B}^T{\\textbf X}^T{\\textbf X}{\\textbf B}}/\\partial{\\textbf B}=2{\\textbf X}^T{\\textbf X}{\\textbf B}\\). Para demostrar esto tomemos el caso para el ajuste lineal (2 parámetros). Definamos los elementos de \\({\\textbf X}^T{\\textbf X}\\) como \\(c_{ij}\\,\\,i,j=1,2\\) y \\(c_{12}=c_{21}\\) por ser simétrica. Entonces \\[{\\textbf B}^T{\\textbf X}^T{\\textbf X}{\\textbf B}=c_{11}b_0^2+c_{22}b_1^2 + 2c_{12} b_0 b_1\\,,\\] y su derivada respecto \\(b_0\\) es \\[2c_{11}b_0 + 2c_{12}b_1\\] y respecto a \\(b_1\\) es \\[2c_{12}b_0 + 2c_{22}b_1\\,.\\] Si acomodamos las derivadas en un vector columna \\(2\\times1\\) obtenemos \\[\\left(\\begin{array}{c}\n  2c_{11}b_0 + 2c_{12}b_1 \\\\\n  2c_{12}b_0 + 2c_{22}b_1 \\\\\n        \\end{array}\\right)=2{\\textbf X}^T{\\textbf X}{\\textbf B}\\,.\\] y por lo tanto \\[\\frac{\\partial{S}}{\\partial{\\textbf B}}=-2{\\textbf X}^T{\\textbf Y}+2{\\textbf X}^T{\\textbf X}{\\textbf\nB}\\,,\\] y las ecuaciones normales quedan \\[-2{\\textbf X}^T{\\textbf Y}+2{\\textbf X}^T{\\textbf X}{\\textbf\nB}=0\\,.\\]\nFinalmente, el problema de mínimos cuadrados es \\[({\\textbf X}^T{\\textbf X}){\\textbf B}={\\textbf X}^T{\\textbf Y}\\,.\\] Y resolviendo para \\({\\textbf B}\\) la forma general del método de regresión por mínimos cuadrados es \\[{\\textbf B}=({\\textbf X}^T{\\textbf X})^{-1}{\\textbf X}^T{\\textbf Y}\\,,\\] o equivalentemente \\[{\\textbf B}^T={\\textbf Y}^T{\\textbf X}({\\textbf X}^T{\\textbf X})^{-1}\\,.\\]\n \\ Algunas de las consideraciones de un modelo de regresión múltiple son:\n \\ La esperanza del vector de coeficientes \\(k\\times 1\\), \\(b\\), se obtiene a partir de las consideraciones 1,4, y 5. La consideración 5 implica que el estimador de los coeficientes \\(b=({\\textbf X}^T{\\textbf X})^{-1}{\\textbf X}^T { y}\\) se puede escribir cómo\n\\[b=({\\textbf X}^T{\\textbf X})^{-1}{\\textbf X}^T({\\textbf X} \\beta + \\epsilon)={\\textbf I}\\beta +\n    ({\\textbf X}^T{\\textbf X})^{-1}{\\textbf X}^T \\epsilon\\,.\\] \\\nDe las consideraciones 1, y 4 obtenemos \\[E[b]=E[\\beta + ({\\textbf X}^T{\\textbf X})^{-1}{\\textbf X}^T\\epsilon]=\n  \\beta + ({\\textbf X}^T{\\textbf X})^{-1}{\\textbf X}^T \\underbrace{E[\\epsilon]}_{0}=\\beta\\,,\\] lo que implica que \\(b\\) es insesgado.\n \\ Usando el resultado de arriba, bajo las consideraciones 1-6, la matriz de varianza de los estimadores de los coeficientes \\(b\\) (o momento centrado de orden 2) viene dada por \\[{ var(b)}=E[(b-\\beta)(b-\\beta)^T]=E[({\\textbf X}^T{\\textbf X})^{-1}{\\textbf X}^T\\epsilon \\epsilon^T{\\textbf X}\n    ({\\textbf X}^T{\\textbf X})^{-1}]=\\] \\[=({\\textbf X}^T{\\textbf X})^{-1}{\\textbf X}^T E[\\epsilon \\epsilon^T]{\\textbf X}({\\textbf X}^T{\\textbf X})^{-1}=\n    ({\\textbf X}^T{\\textbf X})^{-1}{\\textbf X}^T(\\sigma^2{\\textbf I}){\\textbf X}({\\textbf X}^T{\\textbf X})^{-1}=\n    \\sigma^2({\\textbf X}^T{\\textbf X})^{-1}\\,.\\] \\ \\ Los elementos de la diagonal principal de esta matriz son las varianzas asociadas a los estimadores de los coeficientes \\(b\\), y los elementos fuera de la diagonal principal representan la covarianza entre esos estimadores. \\ Si asumimos que las variables del problema han sido estandarizadas, es decir la media ha sido extraida de las variables \\(x_{ij}\\) e \\(y_i\\) y hemos dividido por las desviaciones estándar \\[{\\textbf X}^T{\\textbf X}=\\left(\\begin{array}{cccc}\n   \\sum\\limits_{i=1}^N{x_{i1}x_{i1}} & \\sum\\limits_{i=1}^N{x_{i1}x_{i2}} & ... & \\sum\\limits_{i=1}^N{x_{i1}x_{ik}}\\\\\n   \\sum\\limits_{i=1}^N{x_{i2}x_{i1}} & \\sum\\limits_{i=1}^N{x_{i2}x_{i2}} & ... & \\sum\\limits_{i=1}^N{x_{i2}x_{ik}}\\\\\n                . & . & . \\\\\n        . & . & . \\\\\n        . & . & . \\\\\n   \\sum\\limits_{i=1}^N{x_{ik}x_{i1}} & \\sum\\limits_{i=1}^N{x_{ik}x_{i2}} & ... & \\sum\\limits_{i=1}^N{x_{ik}x_{ik}}\\\\\n        \\end{array}\\right)\\,,\\] es una matriz \\(k\\times k\\) y en notación índice se puede escribir como \\[[{\\textbf X}^T{\\textbf X}]_{nm}=(N-1)\\rho_{x_{in} x_{im}}\\,\\,;(i=1,2,...,N)\\,,\\] que se puede interpretar como la matriz de correlaciones entre las variables independientes. La matriz \\[{\\textbf X}^T{\\textbf Y}=\\left(\\begin{array}{c}\n  \\sum\\limits_{i=1}^N y_i x_{i1}\\\\\n  \\sum\\limits_{i=1}^N y_i x_{i2}\\\\\n                   . \\\\\n           . \\\\\n           . \\\\\n  \\sum\\limits_{i=1}^N y_i x_{ik}\\\\\n        \\end{array}\\right)\\,,\\] es una matriz \\(k\\times 1\\) y en notación índice es \\[[{\\textbf Y}^T{\\textbf X}]_n=(N-1)\\rho_{y_i x_{in}}\\,\\,;(i=1,2,...,N)\\,.\\] Esta matriz se puede interpretar como la matriz de correlación entre las variables independientes y dependientes. \\ El modelo multivariado en notación índice es \\[(N-1)\\rho_{x_{in} x_{im}}b_m=(N-1)\\rho_{y_i x_{in}}\\] o \\[\\rho_{x_{in} x_{im}}b_m=\\rho_{y_i x_{in}}\\,;m=n=1,2,....,k; i=1,2,...,N\\] y para una única observación \\(i\\) dada obtenemos \\[\\rho_{x_{n} x_{m}}b_m=\\rho_{y x_{n}}\\,\\] Ahora supongamos, por simplicidad, que solo tenemos dos variables independientes. Entonces: \\[\\rho_{x_{1} x_{1}}b_1+\\rho_{x_{1} x_{2}}b_2=\\rho_{y x_{1}}\\] \\[\\rho_{x_{2} x_{1}}b_1+\\rho_{x_{2} x_{2}}b_2=\\rho_{y x_{2}}\\,,\\] y puesto que \\(\\rho_{x_{1} x_{1}}=\\rho_{x_{2} x_{1}}=1\\), y \\(\\rho_{x_{1} x_{2}}=\\rho_{x_{2} x_{1}}\\), podemos escribir el sistema como: \\[\\left(\\begin{array}{cc}\n  1 & \\rho_{x_{1} x_{2}} \\\\\n  \\rho_{x_{1} x_{2}} & 1 \\\\\n        \\end{array}\\right)\n  \\left(\\begin{array}{c}\n  b_1 \\\\\n  b_2 \\\\\n        \\end{array}\\right)=\n     \\left(\\begin{array}{c}\n  \\rho_{y x_{1}} \\\\\n  \\rho_{y x_{2}} \\\\\n        \\end{array}\\right)\\,.\\] De forma que los coeficientes quedan como \\[\\left(\\begin{array}{c}\n  b_1 \\\\\n  b_2 \\\\\n        \\end{array}\\right)=\\left(\\begin{array}{cc}\n  1 & \\rho_{x_{1} x_{2}} \\\\\n  \\rho_{x_{1} x_{2}} & 1 \\\\\n        \\end{array}\\right)^{-1}\n     \\left(\\begin{array}{c}\n  \\rho_{y x_{1}} \\\\\n  \\rho_{y x_{2}} \\\\\n        \\end{array}\\right)=\\frac{1}{1-\\rho^2_{x_{1} x_{2}}}\n    \\left(\\begin{array}{cc}\n  1 & -\\rho_{x_{1} x_{2}} \\\\\n  -\\rho_{x_{1} x_{2}} & 1 \\\\\n        \\end{array}\\right)\\left(\\begin{array}{c}\n  \\rho_{y x_{1}} \\\\\n  \\rho_{y x_{2}} \\\\\n        \\end{array}\\right)\\,.\\] \\[b_1=\\frac{1}{1-\\rho^2_{x_{1} x_{2}}}({\\rho_{y x_{1}} - \\rho_{x_{1} x_{2}} \\rho_{y x_{2}}})\\] \\[b_2=\\frac{1}{1-\\rho^2_{x_{1} x_{2}}}({\\rho_{y x_{2}} - \\rho_{x_{1} x_{2}} \\rho_{y x_{1}}})\\,.\\]\nFinalmente puntualizar que el problema de mínimos cuadrados se puede resolver utilizando la descomposición \\(LU\\). Para ello solo es necesario un cambio de variable en la ecuación \\[{\\textbf X}^T{\\textbf X}{\\textbf B}={\\textbf X}^T{\\textbf Y}\\] para obtener un sistema de ecuaciones tipo \\({\\textbf A}{\\textbf x}={\\textbf b}\\), donde ahora \\({\\textbf A}={\\textbf X}^T{\\textbf X}\\), \\({\\textbf x}={\\textbf B}\\), y \\({\\textbf b}={\\textbf X}^T{\\textbf Y}\\).\n{2) Mínimos cuadrados con restricciones}\n{NOTA: Multiplicadores de Lagrange} \\ Dada la función \\(f(x)=f(x_1,x_2,...,x_N)\\) que depende de \\(N\\) variables y \\(p\\) restricciones \\(g_1(x)=d_1, g_2(x)=d_2,....,g_p(x)=d_p\\) entonces el teorema de Lagrange nos dice que para minimizar la función \\(f(x)\\) bajo esas \\(p\\) restricciones debemos resolver el sistema de ecuaciones \\[\\frac{\\partial}{\\partial{x_i}}\\left[ f(x) +\\sum\\limits_{j=1}^p \\lambda_j g_j(x)\\right]=0\\,\\,\\,; i=1,2,...,N\\] \\[g_j(x)=d_j\\,\\,\\,; j=1,2,...,p\\]\nVamos a usar los multiplicadores de Lagrange para resolver el problema de mínimos cuadrados \\[{\\textbf Y}={\\textbf X}{\\textbf B}\\,,\\] pero incluyendo \\(p\\) restricciones de la forma \\[{\\textbf G}{\\textbf B}={\\textbf d}\\,.\\]\nQueremos minimizar la función: \\[{\\textbf \\cal L}=({\\textbf Y}-{\\textbf X}{\\textbf B})^T({\\textbf Y}-{\\textbf X}{\\textbf B}) +{\\textbf \\lambda}^T({\\textbf G}{\\textbf B}-{\\textbf d})\\,.\\] Aqui hemos introducido \\(p\\) incógnitas pero también tenemos \\(p\\) nuevas ecuaciones \\({\\textbf G}{\\textbf B}={\\textbf d}\\). Derivando \\({\\textbf \\cal L}\\) e igualando a cero \\[\\frac{\\partial{\\textbf \\cal L}}{\\partial{\\textbf B}}=-2{\\textbf X}^T{\\textbf Y}+2{\\textbf X}^T{\\textbf X}{\\textbf B} + {\\textbf G}^T{\\textbf \\lambda}=0\\,,\\] lo cual tiene la solución \\[{\\textbf B}=( {\\textbf X}^T {\\textbf X} )^{-1} ({\\textbf X}^T {\\textbf Y} -\\frac{1}{2}{\\textbf G}^T{\\textbf \\lambda})\\,,\\] que para \\({\\textbf \\lambda}=0\\) se reduce a la expresión de los mínimos cuadrados sin restricciones \\[{\\textbf B}=( {\\textbf X}^T {\\textbf X} )^{-1} ({\\textbf X}^T {\\textbf Y})\\,.\\]\nSi sutituimos en la ecuación de las restricciones \\[{\\textbf G}( {\\textbf X}^T {\\textbf X} )^{-1} ({\\textbf X}^T {\\textbf Y} -\\frac{1}{2}{\\textbf G}^T{\\textbf \\lambda})={\\textbf d}\\,,\\] y resolvemos para \\({\\textbf \\lambda}\\), obtenemos \\[\\frac{1}{2}{\\textbf \\lambda}=\\left[ {\\textbf G}({\\textbf X}^T {\\textbf X})^{-1}{\\textbf G}^T\\right]^{-1} \\left[ {\\textbf G}({\\textbf X}^T {\\textbf X})^{-1}{\\textbf X}^T{\\textbf Y}-{\\textbf d}\\right]\\,.\\]\nFinalmente, si sustituimos en la solución para la matriz de coeficientes \\({\\textbf B}\\) obtenemos \\[{\\textbf B}=({\\textbf X}^T {\\textbf X})^{-1}\\left( {\\textbf X}^T {\\textbf Y} - {\\textbf G}^T\\left[{\\textbf G}({\\textbf X}^T{\\textbf X})^{-1}{\\textbf G}^T \\right]^{-1}\n           \\left[{\\textbf G}({\\textbf X}^T{\\textbf X})^{-1}{\\textbf X}^T {\\textbf Y}-{\\textbf d}\\right] \\right)\\,.\\]\nUn ejemplo sería ajustar una recta a un conjunto de observaciones pero imponiendo que la recta pase por algún punto determinado del espacio \\(xy\\).\n{3) Mínimos cuadrados pesados} \\ En ocasiones debido a las incertidumbres asociadas a la medidas de las variables independientes es conveniente asignarles un peso diferencial en el problema de mínimos cuadrados. Supongamos que unas variables independientes son conocidas con mayor precisión que otras. Entonces el modelo de mínimos cuadrados pesados es: \\[SEC=({\\textbf Y}-{\\textbf X}{\\textbf B})^T{\\textbf W}_{\\epsilon}({\\textbf Y}-{\\textbf X}{\\textbf B})\\,,\\] donde \\({\\textbf W}_{\\epsilon}\\) es una matriz diagonal con los elementos \\(\\sigma_j^{-2}\\), el inverso de la varianza de cada variable independiente. \\ Sin embargo, en general, los errores en los datos estan correlacionados, y \\({\\textbf W}_{\\epsilon}\\) no es diagonal. Una elección razonable para \\({\\textbf W}_{\\epsilon}\\) es el inverso de la matriz de covarianzas. Si asumimos que \\({\\textbf Y}\\) se compone de un valor medio mas un error o fluctuación respecto la media \\[{\\textbf Y}=\\bar{\\textbf Y}+{\\textbf Y}'\\,,\\] entonces la matriz de covarianzas es \\(&lt;{\\textbf Y}'{\\textbf Y}'^T&gt;\\) y \\[{\\textbf W}_{\\epsilon}=&lt;{\\textbf Y}'{\\textbf Y}'^T&gt;^{-1}\\,.\\]\n{jemplo (1):} Emery and Thompson (sección 3.12.4). \\ En la tabla adjunta se muestran 5 observaciones de la variable independiente (\\(x_i\\)) y dependiente (\\(y_i\\)). Se pide ajustar una recta al conjunto de datos y calcular las medidas de error (varianza \\(s^2\\) y coeficiente de correlación al cuadrado \\(r^2\\)) asociadas al ajuste lineal.  % %insert table\nLos coeficientes del modelo lineal se pueden calcular con las expresiones: \\[\\hat{b}_1=\\frac{\\left[N \\sum\\limits^N_{i=1} x_i y_i - \\sum\\limits^N_{i=1}x_i \\sum\\limits^N_{i=1}y_i\\right]}\n           {N\\sum\\limits^N_{i=1}x^2_i-\\left(\\sum\\limits^N_{i=1}x_i \\right)^2}=\\] \\[\\frac{[(5)(7)-(0)(5)]}{[(5)(10)-10^2]}=0.7\\]\n\\[b_0=\\bar{y}-b_1\\bar{x}=5/5-(0.7)(0)=1\\]\nEn notación matricial obtenemos el mismo resultado:\n\\[{\\textbf Y}=\\left(\\begin{array}{c}\n  0 \\\\ 0 \\\\ 1 \\\\ 1 \\\\ 3\\\\\n        \\end{array}\\right)\\]\n\\[{\\textbf X}=\\left(\\begin{array}{cc}\n  1 & -2 \\\\\n  1 & -1 \\\\\n  1 & 0 \\\\\n  1 & 1 \\\\\n  1 & 2\\\\\n        \\end{array}\\right)\\]\n\\[{\\textbf X}^T{\\textbf X}= \\left(\\begin{array}{ccccc}\n   5 &  0 \\\\\n  0 & 10 \\\\\n        \\end{array}\\right)\\]\n\\[{\\textbf X}^T{\\textbf Y}= \\left(\\begin{array}{ccccc}\n   5  \\\\\n  7  \\\\\n        \\end{array}\\right)\\]\n\\[({\\textbf X}^T{\\textbf X})^{-1}= \\left(\\begin{array}{ccccc}\n   1/5 &  0 \\\\\n   0 & 1/10 \\\\\n        \\end{array}\\right)\\]\n\\[ ({\\textbf X}^T{\\textbf X})^{-1}{\\textbf X}^T{\\textbf Y}=\n    \\left(\\begin{array}{ccccc}\n   1/5 &  0 \\\\\n   0 & 1/10 \\\\\n        \\end{array}\\right)\\left(\\begin{array}{ccccc}\n   5  \\\\\n  7  \\\\\n        \\end{array}\\right)=\n    \\left(\\begin{array}{ccccc}\n   1  \\\\\n  0.7  \\\\\n        \\end{array}\\right)\\,.\\]\n{} Como ya vimos, para calcular la varianza de nuestro ajuste usamos\n\\[s^2=\\frac{1}{N-2}\\sum\\limits^N_{i=1}(y_i-\\hat{y_i})^2=\\frac{1}{N-2}SEC\\,,\\]\ndonde \\(SEC\\) es la suma de los errores cuadrados y \\(N-2\\) resulta debido a que en la regresión lineal se requiere la estimación de dos parámetros. En notación matricial \\[SEC={\\textbf Y}^T{\\textbf Y} - {\\textbf Y}^T{\\textbf X}{\\textbf B}-{\\textbf B}^T{\\textbf X}^T{\\textbf Y}+{\\textbf B}^T\n{\\textbf X}^T{\\textbf X}{\\textbf B}=\n{\\textbf Y}^T{\\textbf Y}-2{\\textbf B}^T{\\textbf X}^T{\\textbf Y}+{\\textbf B}^T{\\textbf X}^T{\\textbf X}{\\textbf B}=\\]\n\\[={\\textbf Y}^T{\\textbf Y}-2{\\textbf B}^T{\\textbf X}^T{\\textbf Y}+{\\textbf B}^T{\\textbf X}^T{\\textbf Y}=\n{\\textbf Y}^T{\\textbf Y}-{\\textbf B}^T{\\textbf X}^T{\\textbf Y}\\,,\\]\ndonde hemos usado la identidad \\({\\textbf X}^T{\\textbf X}{\\textbf B}={\\textbf X}^T{\\textbf Y}\\).\nSi sustituimos las matrices de nuestro ejemplo, obtenemos\n\\[SEC=\\left(\\begin{array}{ccccc}\n  0 & 0 & 1 & 1 & 3\\\\\n        \\end{array}\\right)\n      \\left(\\begin{array}{c}\n  0 \\\\ 0 \\\\ 1 \\\\ 1 \\\\ 3\\\\\n        \\end{array}\\right)-\n    \\left(\\begin{array}{ccccc}\n  1 & 0.7\\\\\n        \\end{array}\\right)\n    \\left(\\begin{array}{ccccc}\n   1 &  1 & 1 & 1 & 1\\\\\n  -2 & -1 & 0 & 1 & 2 \\\\\n        \\end{array}\\right)\n    \\left(\\begin{array}{c}\n  0 \\\\ 0 \\\\ 1 \\\\ 1 \\\\ 3\n        \\end{array}\\right)=\\] \\[=11-\\left(\\begin{array}{ccccc}\n  1 & 0.7\\\\\n        \\end{array}\\right)\n    \\left(\\begin{array}{c}\n        5 \\\\ 7\\\\\n        \\end{array}\\right)=11-9.9=1.1\\]\n{} \\(SEC\\) puede ser calculado directamente con la expresión: \\[SEC=\\sum \\limits^N_{i=1}(\\hat{y_i}-y_i)^2=\n   (-0.4)^2 + (-0.3)^2+(0)^2 + (0.7)^2 + (0.6)^2=1.1\\]\nY entonces la desviación estándar de nuestro ajuste lineal es \\[s=\\sqrt{\\frac{1}{N-2}SEC}=\\sqrt{\\frac{1}{5-2}1.1}=\\sqrt{1.1/3}\\simeq0.366\\,.\\]\n{} El coeficiente de correlación se puede escribir como\n\\[r^2=\\frac{SCR}{SCT}=\\frac{\\sum\\limits^N_{i=1}(\\hat{y_i}-\\bar{y})^2}{\\sum\\limits^N_{i=1}\n({y_i}-\\bar{y})^2}=\\frac{4.9}{6}\\simeq0.8167\\]\n\nAjuste de curvas con mínimos cuadrados  \n\n{} En general, podemos escribir nuestro modelo lineal como \\[Y=b_0 + b_1x + b_2 x^2 + ... + b_k x^k + \\epsilon\\,.\\] \\ \\ El procedimiento es el mismo que para el caso de la línea recta, pero ahora la matriz \\({\\textbf X}\\) tiene una columna mas. Es decir, para \\(k=2\\) y para \\(N\\) observaciones independientes las ecuaciones independientes son \\[y_1=b_0 + b_1x_1 + b_2 x_1^2 + \\epsilon_1\\] \\[y_2=b_0 + b_1x_2 + b_2 x_2^2 + \\epsilon_2\\] \\[...\\] \\[...\\] \\[...\\] \\[y_N=b_0 + b_1x_N + b_2 x_N^2 + \\epsilon_N\\,\\] y puden resolverse matricialmente para \\({\\textbf B}\\) como \\[{\\textbf B}=({\\textbf X}^T{\\textbf X})^{-1}{\\textbf X}^T{\\textbf Y}\\,,\\] donde \\({\\textbf X}\\) tiene una columna mas que el caso del ajuste de una recta, i.e. \\(k+1\\) columnas.\n{} Ejemplo (2): Ajustes por cuadrados mínimos con restricciones.\n\n{} Supongamos que queremos ajustar dos polinomios \\(f(x)\\) y \\(g(x)\\) de orden d-1 a dos conjunto de datos contínuos de \\(M\\) y \\(N\\) observaciones, respectivamente, \\(x_1, x_2, ....,x_M\\leq a\\) y \\(x_{M+1}, x_{M+2}, ....,x_N&gt;a\\), tal que, queremos minimizar:\n\\[\\sum\\limits^M_{i=1}(f(x_i)-y_i)^2 + \\sum\\limits^N_{i=M+1}(g(x_i)-y_i)^2\\,,\\]\nsujeto a las restricciones:\n\\[f(a)=g(a)\\,\\,y\\,\\,f'(a)=g'(a)\\,.\\]\n\nPrimero debemos de construir las matrices del problema lineal de cuadrados mínimos:\n\\[ {\\textbf X}=\\left(\\begin{array}{cccccccc}\n  1 & x_1   & .  .  . & x_1^{d-1}  & 0 & 0 & .  .  . & 0 \\\\\n  . & .         &  & .                  & . & . &  & . \\\\\n  . & .         &  & .                  & . & . &  & . \\\\\n  . & .         &  & .                  & . & . &  & . \\\\\n  1 & x_M & .  .  . & x_M^{d-1} & 0 & 0 & .  .  . & 0 \\\\\n  0 & 0       & .  .  . & 0                 & 1 & x_{M+1} & .  .  . & x^{d-1}_{M+1} \\\\\n  . & .         && .                  & . & . &  & . \\\\\n  . & .         & & .                  & . & . &  & . \\\\\n  . & .         &  & .                  & . & . &  & . \\\\\n  0 & 0       & .  .  . & 0                & 1  & x_N & .  .  . & x_N^{d-1} \\\\\n  \\end{array}\\right)\n  \\]\n\\[{\\textbf Y}=\\left(\\begin{array}{c}\n    y_1 \\\\ y_2 \\\\ . . . \\\\ y_M \\\\ y_{M+1}\\\\ y_{M+2}\\\\...\\\\y_N\n      \\end{array}\\right)\\,,\\]\n\\[{\\textbf G}=\\left(\\begin{array}{cccccccc}\n        1 & a & .  .  .&a^{d-1}&-1&-a& .  .  .&-a^{d-1}\\\\\n        0 & 1 & .  .  .&(d-1)a^{d-2}&0&-1& .  .  .&-(d-1)a^{d-2}\\\\\n          \\end{array}\\right)\\,,\n          {\\textbf d}=\\left(\\begin{array}{c}\n            0\\\\\n            0\\\\\n              \\end{array}\\right)\\,.\\]\nSegundo debemos de calcular los coeficientes del ajuste \\({\\textbf B}\\) con la expresión para mínimos cuadrados con restricciones.\n\\[{\\textbf B}=({\\textbf X}^T {\\textbf X})^{-1}\\left( {\\textbf X}^T {\\textbf Y} - {\\textbf G}^T\\left[{\\textbf G}({\\textbf X}^T{\\textbf X})^{-1}{\\textbf G}^T \\right]^{-1}\n           \\left[{\\textbf G}({\\textbf X}^T{\\textbf X})^{-1}{\\textbf X}^T {\\textbf Y}-{\\textbf d}\\right] \\right)\\,.\\]\nApliquemos el caso particular del ajuste de dos rectas\n\\(f(x)=b_1 + b_2 x\\) y \\(g(x)=b_3 + b_4 x\\):\n\ndonde \\(h\\) es el valor donde ambas rectas interseccionan. Por ello exigimos que ambas rectas tomen el mismo valor y sus derivadas sean iguales en \\(x=h\\). Se resuleve el sistema de ecuaciones:\n\\[{\\textbf X}\\,{\\textbf B}={\\textbf Y}\\] \\[({\\it 1})b_1+({\\it x_1})b_2 + ({\\it 0})b_3 + ({\\it 0})b_4=y_1\\] \\[({\\it 1})b_1+({\\it x_2})b_2 + ({\\it 0})b_3 + ({\\it 0})b_4=y_2\\] \\[({\\it 1})b_1+({\\it x_3})b_2 + ({\\it 0})b_3 + ({\\it 0})b_4=y_3\\] \\[({\\it 1})b_1+({\\it x_4})b_2 + ({\\it 0})b_3 + ({\\it 0})b_4=y_4\\] \\[({\\it 0})b_1+({\\it 0})b_2 + ({\\it 1})b_3 + ({\\it x_5})b_4=y_5\\] \\[({\\it 0})b_1+({\\it 0})b_2 + ({\\it 1})b_3 + ({\\it x_6})b_4=y_6\\] \\[({\\it 0})b_1+({\\it 0})b_2 + ({\\it 1})b_3 + ({\\it x_7})b_4=y_7\\] \\[({\\it 0})b_1+({\\it 0})b_2 + ({\\it 1})b_3 + ({\\it x_8})b_4=y_8\\,,\\]  \nbajo el sistema de ecuaciones de restricciones:\n\\[{\\textbf G}\\,{\\textbf B}={\\textbf d}\\] \\[({\\it 1})b_1+({\\it h})b_2 + (-{\\it 1})b_3 + (-{\\it 1})b_4=0\\] \\[({\\it 1})b_1+({\\it 0})b_2 + (-{\\it 1})b_3 + ({\\it 0})b_4=0\\]\n\nEn notación matricial:\n\n\\[{\\textbf X}=\\left(\\begin{array}{cccccccc}\n  1 & x_1  & 0 & 0 \\\\\n  1 & x_2   & 0 & 0 \\\\\n  1 & x_3   & 0 & 0\\\\\n  1 & x_4   & 0 & 0 \\\\\n  0 & 0  & 1 & x_5 \\\\\n  0 & 0   & 1 & x_6 \\\\\n  0 & 0   & 1 & x_7\\\\\n  0 & 0   & 1 & x_8 \\\\\n  \\end{array}\\right)\\,,\n{\\textbf Y}=\\left(\\begin{array}{c}\n    y_1 \\\\ y_2 \\\\ y_3 \\\\ y_4 \\\\ y_5\\\\ y_6\\\\y_7\\\\y_8\n      \\end{array}\\right)\\,,\n{\\textbf G}=\\left(\\begin{array}{cccccccc}\n        1 & h & -1&-h\\\\\n        0 & 1& 0&-1\\\\\n          \\end{array}\\right)\\,,\n{\\textbf d}=\\left(\\begin{array}{c}\n            0\\\\\n            0\\\\\n              \\end{array}\\right)\\,.\n  \\]\n\n\n\nCuadrados Mínimos no-lineales\n\nLos mínimos cuadrados no-lineales se aplican cuando queremos ajustar un conjunto de observaciones a un modelo que no es lineal en cuanto a los coeficientes.\n\nLinealización del problema no lineal En ocasiones se puede linealizar el problema y resolverlo utilizando la solución de mínimos cuadrados lineales. Por ejemplo, supongamos que queremos ajustar un conjunto de \\(N\\) observaciones a una función no-lineal exponencial: \\[\\hat{y}=a e^{bx}\\,.\\]\n\nEste ejemplo se puede linealizar: \\[\\hat{y}=a e^{bx}\\rightarrow ln(\\hat{y})=ln(a) + bx\\,,\\] y resolver el problema lineal: \\[{\\cal Y}={\\cal B}_1 + {\\cal B}_2 {\\cal X}\\,,\\] donde hemos hecho el cambio de variables: \\[{\\cal Y}=ln(\\hat{y})\\,,{\\cal B}_1=ln(a)\\,,{\\cal B}_2=b\\,\\,\\,{ y}\\,{\\cal X}=x\\,.\\]\nResolvemos para \\({\\cal B}_1\\) y \\({\\cal B}_2\\):\n\\[{\\cal B}=\\left(\\begin{array}{c}\n            {\\cal B}_1\\\\\n            {\\cal B}_2\\\\\n              \\end{array}\\right)=({\\cal X}^T {\\cal X})^{-1}\\,{\\cal X}^T{\\cal Y}\n\\,,\\] y calculamos los coeficientes originales \\(a\\) y \\(b\\) con el cambio de variables.\n\nResolución numérica del problema no lineal\nCuando no es posible linealizar la función no-lineal que queremos ajustar, debemos minimizar la suma de los errores cuadráticos medios y resolver numéricamente. La \\(SEC\\) del modelo exponencial\n\n\\[SEC=\\sum\\limits^N_{i=1}(y_i-\\hat{y_i})^2=\\sum\\limits^N_{i=1}\n              {\\underbrace{\\left(y_i - a e^{bx_i}\\right)}_{\\epsilon_i}}^2\\,\\]\nes minimizada derivando con respecto los coeficientes:\n\\[\\frac{\\partial{SEC}}{\\partial{a}}=\n      2\\sum\\limits^N_{i=1}\\left(y_i - a e^{bx_i}\\right)\\frac{\\partial{\\epsilon_i}}{\\partial{a}}=0\\]\n\\[\\frac{\\partial{SEC}}{\\partial{b}}=\n2\\sum\\limits^N_{i=1}\\left(y_i - a e^{bx_i}\\right)\\frac{\\partial{\\epsilon_i}}{\\partial{b}}=0\\,.\\]\nDebemos resolver el sistema de ecuaciones de arriba, lo cual se puede hacer con descomposiciones algebráicas tipo \\({\\textbf L}{\\textbf U}\\), minimizando \\(SEC\\) con la función de Matlab fminsearch.m (o implementando un método numérico manual), o directamente utilizando la función de mínimos cuadrados no-lineales de Matlab nlinfit.m.\nOtro ejemplo no-lineal es ajustar la siguiente función trigonométrica: \\[\\hat{y}=\\phi_1 e^{\\phi_2 x} cos(\\phi_3 x + \\phi_4)\\,,\\] donde la suma de los errores cuadráticos es:\n\\[SEC=\\sum\\limits^N_{i=1}(y_i-\\hat{y_i})^2=\\sum\\limits^N_{i=1}\n              {\\underbrace{\\left(y_i - \\phi_1 e^{\\phi_2 x_i} cos(\\phi_3 x_i + \\phi_4)\\right)}_{\\epsilon_i}}^2\\,,\\]\ny sus derivadas respecto los coeficientes \\(\\phi_1\\), \\(\\phi_2\\), \\(\\phi_3\\), y \\(\\phi_4\\) son:\n\\[\\frac{\\partial{SEC}}{\\partial{\\phi_1}}=\n     2\\sum\\limits^N_{i=1}\\left(y_i - \\phi_1 e^{\\phi_2 x_i} cos(\\phi_3 x_i + \\phi_4)\\right)\n     \\frac{\\partial{\\epsilon_i}}{\\partial{\\phi_1}}=0\\]\n\\[\\frac{\\partial{SEC}}{\\partial{\\phi_2}}=2\\sum\\limits^N_{i=1}\\left(y_i - \\phi_1 e^{\\phi_2 x_i} cos(\\phi_3 x_i + \\phi_4)\\right)\n\\frac{\\partial{\\epsilon_i}}{\\partial{\\phi_2}}=0\\]\n\\[\\frac{\\partial{SEC}}{\\partial{\\phi_3}}=\n    2\\sum\\limits^N_{i=1}\\left(y_i - \\phi_1 e^{\\phi_2 x_i} cos(\\phi_3 x_i + \\phi_4)\\right)\n    \\frac{\\partial{\\epsilon_i}}{\\partial{\\phi_3}}=0\\]\n\\[\\frac{\\partial{SEC}}{\\partial{\\phi_4}}=\n    2\\sum\\limits^N_{i=1}\\left(y_i - \\phi_1 e^{\\phi_2 x_i} cos(\\phi_3 x_i + \\phi_4)\\right)\n    \\frac{\\partial{\\epsilon_i}}{\\partial{\\phi_4}}=0\\]\nRelación entre regresión y correlación\n\nEl coeficiente de correlación, \\(r\\), nos informa de que tanto dos (o mas) variables covarian en el espacio-tiempo. Para dos variables aleatorias \\(x\\) e \\(y\\), el coeficiente de correlación es\n\\[r=\\rho_{xy}=\\frac{\\frac{1}{N-1}\\sum\\limits^N_{i=1}(x_i-\\bar{x})(y_i-\\bar{y})}\n     {\\sqrt{\\frac{1}{N-1}\\sum\\limits^N_{i=1}(x_i-\\bar{x})}\n     \\sqrt{\\frac{1}{N-1}\\sum\\limits^N_{i=1}(y_i-\\bar{y})}}=\\frac{C_{xy}}{s_x s_y}\\,,\\]\ndonde \\(C_{xy}\\) es la covarianza de \\(x\\) e \\(y\\), y \\(s_x\\) y \\(s_y\\) son las correspondientesdesviaciones estándar.\n\nPropiedades del coeficiente de correlación\n\n\n\\(r\\) es adimensional.\n\nla magnitud de \\(r\\) se encuantra acotada entre \\(-1\\) y \\(1\\), ya que es una normalización de la covarianza por el producto de la desviación estándar de las dos variables aleatorias.\n\n\nSi \\(r=\\pm1\\) entonces el ajuste es perfecto. Para \\(r=0\\) los puntos estan dispersos aleatoriamente y no existe relación alguna entre las variables. Normalmente encontramos el estadístico \\(r^2\\) en lugar de \\(r\\). \\(r^2\\) se puede reescribir como\n\\[r^2=SCR/SCT=\\frac{SCT-SEC}{SCT}=1-\\frac{SEC}{SCT}=\\frac{C^2_{xy}}{(s_x s_y)^2}\\,,\\]\nlo que nos informa del porcentaje de la varianza explicada (\\(r^2={ varianza\\,explicada}/{ varianza\\,total}\\)) como vimos anteriormente. Un valor de \\(r=0.75\\) significa que la regresión lineal de \\(y\\) sobre \\(x\\), es decir \\(\\hat{y}\\), explica \\(100*r^2=56.25\\%\\) de la varianza total de la muestra.\nFinalmente puntualizar que también podemos calcular el coeficiente de correlación utilizando los estimados de los coeficientes de regresión. Para el caso de una línea recta sabemos\n\\[\\hat{b}_1=\\frac{C_{xy}}{s^2_x}\\,.\\]\nSi sustituimos en la definición de \\(r\\) obtenemos\n\\[r=\\hat{b}_1\\frac{s^2_x}{s_x s_y}=\\hat{b}_1\\frac{s_x}{s_y}\\,.\\]\nCoeficiente de correlación ajustado\n\nCon el fin de considerar los grados de libertad del modelo de regresión lineal, el coeficiente de correlación debe ser ajustado en función del número de variables independientes \\(k\\). Para ello hay que considerar la verdadera varianza de los errores\n\\[Var(SEC)=\\frac{SEC}{N-k-1}\\,,\\]\ny de la variable dependiente\n\\[Var(SCT)=\\frac{SCT}{N-1}\\,.\\]\nPuesto que los grados de libertad no son iguales (\\(N-1\\) vs \\(N-k-1\\)), el coeficiente de correlación ajustado al cuadrado (\\(\\tilde{r}^2\\)) se define como\n\\[\\tilde{r}^2=1-\\frac{Var(SEC)}{Var(SCT)}=1-\\frac{SEC/N-k-1}{SCT/N-1}=\n1-\\frac{N-1}{N-k-1}(1-r^2)=\\] \\[=1-(1-r^2)\\frac{N-1}{N-k-1}\\,,\\]\ndonde para \\(k=0\\) obtenemos la definición clásica del coeficiente de correlación.\n\nIntervalo de confianza para el coeficiente de correlación\nPodemos calcular intervalos de confianza para el coeficiente de correlación \\(r\\) por medio de la denominada transformación Z de Fisher. Básicamente transforma \\(r\\) en una variable normal estándar \\(Z\\)\n\\[Z=\\frac{1}{2}\\frac{ln(1+r)}{ln(1-r)}\\,,\\]\ncon desviación estándar\n\\[\\sigma(Z)=\\frac{1}{\\sqrt{(N-3)}}\\,,\\]\ny media\n\\[\\mu(Z)=\\frac{1}{2}\\frac{ln(1+\\rho_0)}{ln(1-\\rho_0)}\\,,\\]\ndonde \\(\\mu(Z)\\) es la media esperada (media poblacional) del estadístico \\(Z\\).\nEl intervalo de confianza se escribe entonces como\n\\[Z-Z_{\\alpha/2}&lt;Z&lt;Z+Z_{\\alpha/2}\\]\n*Ejemplo**:\nSupongamos \\(N=21\\) y \\(r=0.8\\). Encuentra el intervalo de confianza al \\(95\\%\\) para el coeficiente de correlación poblacional \\(\\rho_0\\).\n\\[Z=\\frac{1}{2}\\frac{ln(1+0.8)}{ln(1-0.8)}=1.0986\\]\nPuesto que \\(Z\\) esta Normalmente distribuida, entonces todos los valores deben de caer dentro de 1.96 desviaciones estándar de \\(Z\\). Entonces, al \\(95\\%\\), la verdadera media \\(\\mu_Z\\) esta contenida en \\[Z-1.96\\sigma(Z) &lt; \\mu(Z) &lt; Z + 1.96\\sigma(Z)\\] \\[Z-1.96\\frac{1}{\\sqrt{(21-3)}} &lt; \\mu(Z) &lt; Z + 1.96\\frac{1}{\\sqrt{(21-3)}}\\] \\[0.6366&lt;\\mu(Z)&lt;1.5606\\]\nLos límites encontrados para \\(\\mu(Z)\\) los podemos transformar en términos de la verdadera correlación \\[\\mu(Z)=0.6366=\\frac{1}{2} \\left\\{ \\frac{1+\\rho}{1-\\rho} \\right\\} \\rightarrow  \\rho=0.56\\,,\\] donde hemos usado \\[e^{2\\mu}=\\frac{1+\\rho}{1-\\rho}\\] \\[e^{2\\mu}-1=\\rho(1+e^{2\\mu})\\] \\[\\rho=\\frac{(e^{2\\mu(Z)}-1)}{(e^{2\\mu(Z)}+1)}\\,.\\]\nPodemos afirmar con un 95% de confianza que la verdadera correlación \\(\\rho\\) esta en el intervalo \\(0.56&lt;\\rho,0.92\\), dado un tamaño muestral \\(N=21\\) y una correlación muestral \\(r=0.8\\).\n{erdaderos grados de libertad} \\ Ya hemos definido anteriormente los grados de libertad se define como el número de muestras independientes \\(N\\) menos el número de parámetros que se quieren estimar. Esta definición es un tanto incorrecta puesto que debemos de también asegurar que las \\(N\\) muestras son efectivamente independientes, es decir, no estan autocorreladas en el espacio-tiempo. Para considerar esto \\(N\\) debe reesecribirse como\n\\[N^{*}=\\frac{N}{\\left[ \\sum\\limits^{\\infty}_{\\tau=-\\infty}\nC_{xx}(\\tau)C_{yy}(\\tau) + C_{xy}(\\tau)C_{yx}(\\tau)\n\\right]/\\left[ C_{xx}(0)C_{yy}(0)\\right]}=\\]\n\\[\\frac{N}{\\left[\\sum\\limits^{\\infty}_{\\tau=-\\infty}\n               \\rho_{xx}(\\tau)\\rho_{yy}(\\tau) +\n           \\rho_{xy}(\\tau)\\rho_{yx}(\\tau)\\right]}\\,.\\]\nEn general, series de datos suelen estar correlacionados en el espacio-tiempo y \\(N^{*}&lt;&lt;N\\). Cuanto mayores las escalas de correlación espaciales-temporales, menores los \\(N^*\\). Esto nos hace pensar que es muy importante la selección de las escalas espaciales-temporales sobre las que queremos calcular un estadístico. Para extraer las escalas de interés podemos usar métodos espectrales y filtros. El proceso de filtrado se encarga de eliminar aquellas escalas que esperamos no contribuyen a la verdadera correlación pero pueden adicionar correlación artificial debido a errores instrumentales y de muestreo."
  },
  {
    "objectID": "chap4.html#propagación-de-errores",
    "href": "chap4.html#propagación-de-errores",
    "title": "Notas del Curso Análisis de Datos",
    "section": "Propagación de errores",
    "text": "Propagación de errores\n*Regla 1** Si \\(x\\) e \\(y\\) tienen errores aleatorios independientes \\(\\delta{x}\\) y \\(\\delta{y}\\),nentonces el error en la suma \\(z=x+y\\) es\n\\[\\delta{z}=\\sqrt{\\delta{x}^2 + \\delta{y}^2}\\,.\\]\n*Regla 2**\nSi \\(x\\) e \\(y\\) tienen errores aleatorios independientes \\(\\delta{x}\\) y \\(\\delta{y}\\), entonces el error en la multiplicación \\(z=xy\\) es\n\\[\\frac{\\delta{z}}{z}=\\sqrt{\\left(\\frac{\\delta{x}^2}{x}\\right) + \\left(\\frac{\\delta{y}^2}{y}\\right)}\\,.\\]\n*Regla 3**\n\nSi \\(z=f(x)\\), donde \\(f()\\) es una función dada, entonces\n\\[\\delta{z}=|f'(x)|\\delta{x}\\,.\\]\n*Formula general para propagación del error**\nSea \\(x_1, x_2,....,x_N\\) medidas con incertidumbres \\(\\delta{x_1},\\delta{x_2},....,\\delta{x_3}\\). Supongamos que queremos determinar \\(q\\), el cual es una función de \\(x_1,x_2,...,x_N\\): \\[q=f(x_1, x_2,...,x_N)\\,.\\] El error asociado a \\(q\\) es entonces\n\\[\\delta{q}=\\sqrt{\\left( \\frac{\\partial{q}}{\\partial{x_1}}\\delta{x_1}\\right)^2 + ... +\n                  \\left( \\frac{\\partial{q}}{\\partial{x_N}}\\delta{x_N}\\right)^2}\\] Si \\(q=x_1+x_2\\) entonces obtenemos la regla 1:\n\\[\\frac{\\partial{q}}{\\partial{x_1}}=1\\,,\\] \\[\\frac{\\partial{q}}{\\partial{x_2}}=1\\,,\\]\n\\[\\delta{q}=\\sqrt{\\delta{x_1}^2 + \\delta{x_2}^2}\\,.\\]\nSi \\(q=x_1x_2\\) entonces obtenemos la regla 2:\n\\[\\frac{\\partial{q}}{\\partial{x_1}}=x_2\\,,\\] \\[\\frac{\\partial{q}}{\\partial{x_2}}=x_1\\,,\\]\n\\[\\delta{q}=\\sqrt{x^2_2\\delta{x_1}^2 + x^2_1\\delta{x_2}^2}=\\sqrt{q^2\\left[ \\left(\\frac{\\partial{x_1}}{x_1}\\right)^2 + \\left(\\frac{\\partial{x_2}}{x_2}\\right)^2\\right]}\\,.\\]\n\\[\\frac{\\delta{q}}{q}=\\sqrt{\\left(\\frac{\\delta{x_1}^2}{x_1}\\right) + \\left(\\frac{\\delta{x_2}^2}{x_2}\\right)}\\,.\\]\nDemstración:*\nQueremos calcular la desviación estándar de la función\n\\[q=q(x_1, x_2,....,x_N)\\,,\\]\nque depende de \\(N\\) variables independientes \\(x_1\\), \\(x_2\\), ….,\\(x_N\\). El desarrollo de Taylor de la función \\(q\\) alrededor de la media \\(\\bar{q}\\) se puede escribir: \\[q - \\bar{q}=\\left(x_1 - \\bar{x}_1\\right)\\frac{\\partial{q}}{\\partial{x_1}} +\n                          \\left(x_2 - \\bar{x}_2\\right)\\frac{\\partial{q}}{\\partial{x_2}}+  \\,.\\,.\\,.\\,\n                          + \\left(x_N - \\bar{x}_N\\right)\\frac{\\partial{q}}{\\partial{x_N}}\\,.\\]\nLa varianza de la función \\(q\\) es:\n\\[s^2_q=\\frac{1}{N-1}\\sum\\limits^N_{i=1}\\left( q_i-\\bar{q} \\right)^2=\n               \\frac{1}{N-1}\\left[\n               \\left(x_1 - \\bar{x}_1\\right)\\frac{\\partial{q}}{\\partial{x_1}} +\n               \\left(x_2 - \\bar{x}_2\\right)\\frac{\\partial{q}}{\\partial{x_2}} +\n              \\,.\\,.\\,.\\, +\n               \\left(x_N - \\bar{x}_N\\right)\\frac{\\partial{q}}{\\partial{x_N}}\n               \\right]^2=\\]\n\\[=\\frac{1}{N-1}\\left[\n\\left(x_1 - \\bar{x}_1\\right)^2\\left(\\frac{\\partial{q}}{\\partial{x_1}}\\right)^2 +\n\\left(x_2 - \\bar{x}_2\\right)^2\\left(\\frac{\\partial{q}}{\\partial{x_2}}\\right)^2 +\n2\\left(x_1 - \\bar{x}_1\\right)\\left(x_2 - \\bar{x}_2\\right)\n\\frac{\\partial{q}}{\\partial{x_1}}\\frac{\\partial{q}}{\\partial{x_2}}+\\,.\\,.\\,.\\right]=\\]\n\\[=s^2_{x_1}\\left(\\frac{\\partial{q}}{\\partial{x_1}}\\right)^2 +\n   s^2_{x_2}\\left(\\frac{\\partial{q}}{\\partial{x_2}}\\right)^2+\n   2 s_{{x_1}{x_2}}\n   \\frac{\\partial{q}}{\\partial{x_1}}\\frac{\\partial{q}}{\\partial{x_2}}+\\,.\\,.\\,.\\]\ny finalmente la expresión general para la propagación del error es:\n\\[s_{q}=\\sqrt{s^2_{x_1}\\left(\\frac{\\partial{q}}{\\partial{x_1}}\\right)^2 +\n               s^2_{x_2}\\left(\\frac{\\partial{q}}{\\partial{x_2}}\\right)^2+\n                2 s_{{x_1}{x_2}}\n                \\frac{\\partial{q}}{\\partial{x_1}}\\frac{\\partial{q}}{\\partial{x_2}}+\\,.\\,.\\,.}\\]\nEjemplo:\nLa ecuación para el cálculo de la salinidad a partir de la conductividad (C) y temperatura (T) es (Unesco EOS-80)\n\\[S=a_0+a_1 R_T^{1/2}+a_2 R_T+a_3 R_T^{3/2}+a_4 R_T^{2}+a_5 R_T^{5/2}+\\Delta{S}\\,,\\]\ndonde \\[R_T=\\frac{R}{R_p r_t}\\,\\,\\,,\\,\\,R=\\frac{C(S,T,0)}{C(35,15,0)}\\,,\\]\n\\(C(35,15,0)\\) es la conductividad de un agua de salinidad práctica 35 a los \\(15\\,^\\circ{ C}\\), \\[r_t=c_0 + c_1 T + c_2 T^2 + c_3T^3 + c_4T^4\\,,\\]\n\\[R_p=1+\\frac{P(e_1+e_2P+e_3P^2)}{(1+d_1T+d_2T^2+(d_3 + d_4T)R)}\\,,\\] y \\[\\Delta{S}=\\frac{T-15}{1+k(T-15)}(b_0+b_1 R_T^{1/2}+b_2 R_T+b_3 R_T^{3/2}+b_4 R_T^{2}+b_5 R_T^{5/2})\\,,\\]\ncon los coeficientes \\(a_i\\,,b_i\\,,c_i\\,,d_i\\,,\\,\\,y\\,e_i\\) \n\\(a_0=0.0080\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,b_0=0.0005\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,c_0=0.6766097\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,d_1=3.426\\,e^{-2}\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,e_1=2.070\\,e^{-5}\\)  \\(a_1=-0.1692\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,b_1=-0.0056\\,\\,\\,\\,\\,\\,\\,\\,\\,c_1=2.00564\\,e^{-2}\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,d_2=4.464\\,e^{-4}\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,e_2=-6.370\\,e^{-10}\\)  \n\\(a_2=25.3851\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,b=-0.0066\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,c_2=1.104259\\,e^{-4}\\,\\,\\,\\,\\,\\,\\,\\,\\,d_3=4.215\\,e^{-1}\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,e_3=3.989\\,e^{-15}\\)\n\n\\(a_3=14.0941\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,b=-0.0375\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,c_3=-6.9698\\,e^{-7}\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,d_4=-3.107\\,e^{-3}\\)   \\(a_4=-7.0261\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,b_4=0.0636\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,c_4=1.0031\\,e^{-9}\\)\n\n\\(a_5=2.7081\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,b_5=-0.0144\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\)\n\n\\(\\sum a_i=35.0000\\,\\,\\,\\,\\sum b_i=0.0000\\)\n\n\\(k=0.0162\\)\n\nSi el error de precisión del termistor y de la celda de conductividad del CTD es \\(\\delta{T}=0.001\\,^\\circ{ C}\\) y \\(\\delta{C}=0.001\\,{ S}{ m}^{-1}\\), respectivamente, calcule la incertidumbre asociada al cálculo de la salinidad \\(S\\) a partir de la formula general de propagación del error para \\(C=5\\,{ S}\\,{ m}^{-1}\\), \\(T=28\\,^\\circ\\,{ C}\\), y \\(P=50\\,{ dbar}\\). Suponga que \\(\\delta{P}=0\\), es decir, el altímetro no tiene errores de precisión.\n\nResultado:\nEl error estándar asociado a la salinidad es\n\\[\\delta{S}=\\sqrt{\\left( \\frac{\\partial{S}}{\\partial{T}}\\delta{T} \\right)^2 + \\left( \\frac{\\partial{S}}{\\partial{C}}\\delta{C}\\right)^2}\\,,\\]\ndonde\n\\[\\frac{\\delta{S}}{\\delta{T}}=\n      \\left[(1+k(T-15))^{-1} - (k(T-15))(1+k(T-15))^{-2}\\right]\\] \\[\\left (b_0+b_1 R_T^{1/2}+b_2 R_T+b_3 R_T^{3/2}+b_4 R_T^{2}+b_5 R_T^{5/2}\\right)\\,,\\] y\n\\[\\frac{\\delta{S}}{\\delta{C}}=\n                 \\frac{1}{2} a_1 \\frac{R_T^{-1/2}}{C(35,T,0)} +\n         a_2 \\frac{1}{C(35,T,0)} +\n             \\frac{3}{2}a_3 \\frac{R_T^{1/2}}{C(35,T,0)}+ \\]\n\\[+2 a_4 \\frac{R_T}{C(35,T,0)} +\\frac{5}{2} a_5 \\frac{R_T^{3/2}}{C(35,T,0)}+\\frac{T-15}{(1+k(T-15))}\\]\n\\[\\left(b_1 \\frac{R_T^{-1/2}}{C(35,T,0)} +\n         b_2 \\frac{1}{C(35,T,0)} +\n             \\frac{3}{2}b_3 \\frac{R_T^{1/2}}{C(35,T,0)}+\n               2 b_4 \\frac{R_T}{C(35,T,0)} +\n     +\\frac{5}{2} b_5 \\frac{R_T^{3/2}}{C(35,T,0)}\n     \\right)\\,.\\]\nPor lo tanto, dada una medida de conductividad \\(C\\) y temperatura \\(T\\) podemos calcular \\({\\delta{S}}/{\\delta{T}}\\) y \\({\\delta{S}}/{\\delta{C}}\\) y obtener la desviación estándar asociado al cálculo de la salinidad \\(\\delta{S}\\).\nEjercicio:\nCalcule el error estándar asociado a la densidad referenciada a la superfície que se obtiene al usar el algoritmo del estado del agua de mar (Unesco EOS-80) \\[\\rho(S,T,0)=\\rho_w + \\left(b_0+b_1 T+b_2 T^2+b_3 T^{3}+b_4 T^{4}\\right) S +\\]\n\\[+\\left(c_0+c_1 T+c_2 T^2\\right) S^{3/2} + d_0 S^2\\,,\\]\ncon los coeficientes \\(b_i\\), \\(c_i\\), y \\(d_0\\)\n\\(b_0=8.24493\\,e^{-1}\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,c_0=-5.72466\\,e^{-3}\\)  \\(b_1=-4.0899\\,e^{-3}\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,c_1=1.0227\\,e^{-4}\\)\n\\(b_2=7.6438\\,e^{-5}\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,c_2=-1.6546\\,e^{-6}\\)\n\\(b_3=-8.2467\\,e^{-7}\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\)   \\(b_4=5.3875\\,e^{-9}\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,d_0=4.8314\\,e^{-4}\\),,\n\ny la densidad de referencia para agua pura (\\(\\rho_w\\)) definida como:\n\\[\\rho_w=a_0+a_1 T+a_2 T^2+a_3 T^{3}+a_4 T^{4}+a_5 T^{5}\\,,\\]\ndonde los coeficientes \\(a_i\\) son:\n\\(a_0=999.842594\\)  \\(a_1=6.793952\\,e^{-2}\\)  \\(a_2=-9.095290\\,e^{-3}\\)  \\(a_3=1.001685\\,e^{-4}\\)  \\(a_4=-1.120083\\,e^{-6}\\) \\(a_5=6.536332\\,e^{-9}\\)."
  },
  {
    "objectID": "chap4.html#métodos-de-interpolación",
    "href": "chap4.html#métodos-de-interpolación",
    "title": "Notas del Curso Análisis de Datos",
    "section": "Métodos de Interpolación",
    "text": "Métodos de Interpolación\nInterpolación es el procedimiento para el cual obtenemos valores de propiedades en posiciones o tiempos que nunca fueron muestreados a partir de observaciones existentes en otras localizaciones o tiempos. En oceanografía necesitamos interpolar (i) para rellenar huecos cuando el instrumento dejo de medir, (ii) para obtener mapas espaciales de contornos (2D), (iii) para calcular alguna propiedad derivada en un punto concreto\n\nInterpolación Lineal\nLa interpolación mas sencilla, pero no por ello peor, es la interpolación lineal. Esta interpolación se basa en ajustar una línea recta entre los puntos conocidos, e interpolar cualquier punto intermedio como un punto a lo largo de la recta. Este tipo de interpolación puede ser usado para rellenar huecos en nuestras series temporales. La interpolación lineal de unas serie y(t) se puede escribir como \\[y(t_i)=y(t_0)+\\frac{y(t_1)-y(t_0)}{t_1 - t_0}(t_i-t_0)\\,.\\]\n\n\nInterpolación polinómica\nEn el caso que queramos interpolar entre mas de dos puntos simultaneamente, debemos de usar polinomios de orden superior a la recta (orden 1). Es decir, \\[y(x)=a_0 + a_1 x + a_2 x^2 + ... + a_{n-1}x^{n-1} + a_n x_n\\,.\\] Si tomamos \\(N\\) observaciones (\\(y_i(x)\\,\\,;i=0,2,...,N-1\\)) obtenemos un sistema de \\(N\\) ecuaciones\n\\[\\left(\\begin{array}{ccccc}\n  1 & x_0 & ... & x_0^{N-1} & x_0^N\\\\\n  1 & x_1 & ... & x_1^{N-1} & x_1^N\\\\\n  . & . & . & . & .\\\\\n  . & . & . & . & .\\\\\n  . & . & . & . & .\\\\\n  1 & x_N & ... & x_N^{N-1} & x_N^N\\\\\n     \\end{array}\\right)\n    \\left(\\begin{array}{c}\n  a_0 \\\\\n  a_1 \\\\\n  . \\\\\n  . \\\\\n  . \\\\\n  a_N \\\\\n     \\end{array}\\right)=\n     \\left(\\begin{array}{c}\n  y_0 \\\\\n  y_1 \\\\\n  . \\\\\n  . \\\\\n  . \\\\\n  y_N \\\\\n     \\end{array}\\right)\\,,\\] lo cual se puede resolver con eliminación Gauss-Jordan. Este método es muy lento y por ello se usa el método de Lagrange.\n{ étodo de Lagrange}\nDe nuevo asumimos \\[p(x)=a_0 + a_1 x + a_2 x^2 + ... + a_{n-1}x^{n-1} + a_n x_n=\\sum\\limits^{N}_{k=0} a_k x^k\n=\\sum^{N+1}_{i=1} y_i {\\cal L}_i(x)\\,,\\] donde \\[{\\cal L}_i(x)=\\prod^{N+1}_{\\substack{k=1\\\\ k\\ne i}} \\frac{x-x_k}{x_i-x_k}\\,,\\] son los denominados polinomios de Lagrange, y \\(\\prod\\) es el operador producto. Puesto que este operador cuando \\(k\\ne i\\) no incluye el producto, a pesar que varíe de \\(1\\) a \\(N+1\\), obtendremos un polinomio de orden \\(N\\).\nEsta suma de polinomios de Lagrange es el polinomio de menor grado que interpola un conjunto de datos, es decir, \\[p(x_j)=\\sum^{N+1}_{i=1} y_i {\\cal L}_i(x_j)=y_j\\,.\\] {emostración:}\nLos polinomios de Lagrange para \\(i\\ne j\\) son iguales a cero, y para \\(i=j\\) son iguales a 1. Veamos esto:\n\\[{ (1)}\\,\\,\\,\\,\\,{ Para}\\,\\,\\,{i \\ne j}:\\,\\,\\,\\,\\,{\\cal L}_i(x_j)=\\prod^{N+1}_{\\substack{k=1\\\\ k\\ne i}} \\frac{x_j-x_k}{x_i-x_k}=\\] \\[=\\frac{x_j-x_1}{x_i-x_1} \\frac{x_j-x_2}{x_i-x_2} ...\\frac{x_j-x_j}{x_i-x_j}....\\frac{x_j-x_{N+1}}{x_i-x_{N+1}}\\]\n\\[{ (2)}\\,\\,\\,\\,\\,{ Para}\\,\\,\\,{i = j}:\\,\\,\\,\\,\\,{\\cal L}_i(x_j)=\\prod^{N+1}_{\\substack{k=1\\\\ k\\ne i}} \\frac{x_j-x_k}{x_j-x_k}=1\\]\nDe modo que \\[{\\cal L}_i(x_j)=\\delta_{ij}=\\begin{cases}\n\\begin{array}{c}\n   1 \\,\\,\\,\\,\\text{si}\\,\\,\\,\\,i= j\\\\\n   0 \\,\\,\\,\\,\\text{si}\\,\\,\\,\\,i\\ne j\\\\\n\\end{array}\n\\end{cases}\\,.\\]\nFinalmente podemos concluir entonces que \\[p(x_j)=\\sum^{N+1}_{i=1} y_i {\\cal L}_i(x_j)=\\sum^{N+1}_{i=1} y_i \\delta_{ij}=y_j\\,.\\] es un polinomio de grado no mayor a \\(N\\) y que \\(p(x_j)=y_j\\).\nEl polinomio se puede reescribir en terminos de la función \\(Q_i\\) como \\[p(x)=\\sum^{N+1}_{i=1} y_i[Q_i(x)/Q_i(x_i)]\\,,\\] donde \\[Q_i(x)=(x-x_1)(x-x_2)....(x-x_{i-1})(x-x_{i+1})...(x-x_{N-1})\\,,\\] es el producto de todas las diferencias excepto la posición \\(i\\) (i.e., \\(x-x_{i}\\)). Si expandemos \\(p(x)\\) \\[p(x)=y_1\\frac{(x-x_2)(x-x_3)...(x-x_{N+1})}{(x_1-x_2)(x_1-x_3)...(x_1-x_{N+1})} +\n       y_2\\frac{(x-x_1)(x-x_3)...(x-x_{N+1})}{(x_2-x_1)(x_2-x_3)...(x_2-x_{N+1})} +\\] \\[+...+y_{N+1}\\frac{(x-x_1)(x-x_2)...(x-x_{N})}{(x_{N+1}-x_1)(x_{N+1}-x_2)...(x_{N+1}-x_{N})}\\,.\\]\n{ jemplo:}\nConsidere los puntos (0,2), (1,2), (2,0) y (3,0) para los cuales queremos ajustar un polinomio de orden 3 \\[y(x)=2\\frac{(x-1)(x-2)(x-3)}{(0-1)(0-2)(0-3)} + 2\\frac{(x-0)(x-2)(x-3)}{(1-0)(1-2)(1-3)} + 0\n+ 0=\\] \\[=\\frac{2}{3}x^3 -3x^2 +\\frac{7}{3}x +2\\,.\\] \n\n\nSpline cúbico\nLa interpolación por spline cúbicos es un método de ajuste de polinomios de orden 3 por segmentos. Veamos este método con un ejemplo.\nSupongamos que queremos interpolar los siguientes datos \\((x_i,y_i)=[(2,-1), (3,2), (5,-7)]\\) con una spline cúbica. Primero definimos un polinomio cúbico para cada intervalo: \\[s(x)=a_1x^3 + b_1 x^2 +c_1x +d_1\\,\\,\\,\\,\\,{ si}\\,\\,\\,\\,\\,x\\in[2,3]\\] \\[s(x)=a_2x^3 + b_2 x^2 +c_2x +d_2\\,\\,\\,\\,\\,{ si}\\,\\,\\,\\,\\,x\\in[3,5]\\]\nA continuación debemos de asegurar que los polinomios pasan por los puntos del problema: \\[s(2)=8a_1 + 4b_1  +2c_1 +d_1=-1\\] \\[s(3)=27a_1 + 9b_1  +3c_1 +d_1=2\\] \\[s(3)=27a_2 + 9b_2  +3c_2 +d_2=2\\] \\[s(5)=125a_2 + 25b_2  +5c_2 +d_2=-7\\]\nAhora calculamos la primera y segundas derivadas para cada intervalo \\[s'(x)=3a_1x^2+2b_1x+c_1\\,\\,\\,\\,\\,{ si}\\,\\,\\,\\,\\,x\\in[2,3]\\] \\[s'(x)=3a_2x^2+2b_2x+c_2\\,\\,\\,\\,\\,{ si}\\,\\,\\,\\,\\,x\\in[3,5]\\] \\[s''(x)=6a_1x+2b_1\\,\\,\\,\\,\\,{ si}\\,\\,\\,\\,\\,x\\in[2,3]\\] \\[s''(x)=6a_2x+2b_2\\,\\,\\,\\,\\,{ si}\\,\\,\\,\\,\\,x\\in[3,5]\\] y aseguramos que sean contínuas. Para ello debemos de igualar las derivadas entre intervalos de manera que no hayan discontinuidades \\[3a_1(3)^2+2b_1(3)+c_1=3a_2(3)^2+2b_2(3)+c_2\\rightarrow 27a_1+6b_1+c_1=27a_2+6b_2+c_2\\] \\[6a_1(3)+2b_1=6a_2(3)+2b_2\\rightarrow 18a_1+2_b1=18a_2+2_b2\\]\nEn este momento tenemos 6 equaciones y 8 incógnitas. Debemos por lo tanto encontrar dos ecuaciones mas. Para ello vamos a forzar que en los extremos la segunda derivada sea nula, es decir, no haya curvatura \\[s''(x_0)=s''(2)=0\\rightarrow 6a_1(2)+2b_1=0 \\rightarrow 12a_1 + 2b_1=0\\] \\[s''(x_N)=s''(5)=0\\rightarrow 6a_2(5)+2b_2=0 \\rightarrow 30a_2+2b_2=0\\]\nAhora ya tenemos un sistema determinado, es decir, 8 ecuaciones y 8 incógnitas \\[8a_1 + 4b_1  +2c_1 +d_1=-1\\] \\[27a_1 + 9b_1  +3c_1 +d_1=2\\] \\[27a_2 + 9b_2  +3c_2 +d_2=2\\] \\[125a_2 + 25b_2  +5c_2 +d_2=-7\\] \\[27a_1+6b_1+c_1=27a_2+6b_2+c_2\\] \\[18a_1+2b_1=18a_2+2b_2\\] \\[12a_1 + 2b_1=0\\] \\[30a_2+2b_2=0\\]\nLo cual en notación matricial se puede escribir\n\\[\\left(\\begin{array}{cccccccc}\n  8 & 4 & 2 & 1 & 0 & 0 & 0 & 0\\\\\n  27 & 9 & 3 & 1 & 0 & 0 & 0 & 0\\\\\n  0 & 0 & 0 & 0 & 27 & 9 & 3 & 1\\\\\n  0 & 0 & 0 & 0 & 125 & 25 & 5 & 1\\\\\n  27 & 6 & 1 & 0 & -27 & -6 & -1 & 0\\\\\n  18 & 2 & 0 & 0 & -18 & -2 & 0 & 0\\\\\n  12 & 2 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n  0 & 0 & 0 & 0 & 30 & 2 & 0 & 0\\\\\n    \\end{array}\\right)\\left(\\begin{array}{c}\n  a_1 \\\\\n  b_1 \\\\\n  c_1 \\\\\n  d_1 \\\\\n  a_2 \\\\\n  b_2 \\\\\n  c_2 \\\\\n  d_2 \\\\\n    \\end{array}\\right)=\n    \\left(\\begin{array}{c}\n  -1 \\\\\n  2 \\\\\n  2 \\\\\n  -7 \\\\\n  0 \\\\\n  0 \\\\\n  0 \\\\\n  0 \\\\\n    \\end{array}\\right)\\]\nEste sistema se puede resolver facilmente si la matriz de datos es invertible. Los coeficientes resultantes son \\[a_1=-1.25\\,\\,\\,\\,\\,b_1=7.5\\,\\,\\,\\,c_1=-10.75\\,\\,\\,\\,d_1=0.5\\] \\[a_1=0.625\\,\\,\\,\\,\\,b_1=-9.375\\,\\,\\,\\,c_1=39.875\\,\\,\\,\\,d_1=-50.125\\,,\\] y los polinomios de nuestra spline cúbica es \\[s(x)=-1.25x^3+7.5x^2-10.75x+0.5\\,\\,\\,\\,\\,{ si}\\,\\,\\,\\,\\,x\\in[2,3]\\] \\[s(x)=0.625x^3+-9.375x^2-39.875x+50.125\\,\\,\\,\\,\\,{ si}\\,\\,\\,\\,\\,x\\in[3,5]\\]"
  },
  {
    "objectID": "chap4.html#interpolación-objetiva",
    "href": "chap4.html#interpolación-objetiva",
    "title": "Notas del Curso Análisis de Datos",
    "section": "Interpolación Objetiva",
    "text": "Interpolación Objetiva\nUn mapa objetivo se obtiene como una regresión múltiple (donde el error cuadrático medio es mínimo) de un conjunto de observaciones discretas. Se utiliza en oceanografía para obtener mapas continuos (mallas regulares) a partir de datos discretos distribuidos irregularmente en el espacio. Las variables representadas en el mapa objetivo pueden modificarse de una realización a otra, con lo que deben de considerarse las anomalías de las variables en lugar de las variables en si mismas. Se debe de definir un ensemble (promedio), climatología, o candidato y extraerlo a cada variable para obtener anomalías. Este proceso de elección de la media es una parte delicada de la interpolación objetiva. En general, para el océano, la media es desconocida ya que solamente tenemos pocas realizaciones de nuestro muestreo. Una forma de operar es estimar la media ajustando un polinomio de bajo orden a nuestros datos discretos, extraer esta a los datos y proceder con la interpolación objetiva. La media se añade de nuevo despues de calcular el mapa objetivo de las fluctuaciones. \\\nTípicamente se debe asumir dos condiciones: \\\n\nEl error (o ruido) asociado a la interpolación no esta correlacionado con nuestra señal (variable a interpolar) \\[&lt;\\phi_i\\epsilon_i&gt;=0\\] \\\nEl error no esta correlacionado de una estación a la otra \\[&lt;\\epsilon_i \\epsilon_j&gt;=\n\\begin{cases}\n\\begin{array}{c}\n   &lt;\\epsilon^2&gt; \\,\\,\\,\\,\\text{si} \\,\\,\\,\\,i=j\\\\\n   0    \\,\\,\\,\\,        \\text{si} \\,\\,\\,\\,i\\ne j\\\\\n\\end{array}\n\\end{cases}\\]\n\nSupongamos entonces fluctuaciones respecto un estado climatológico o media \\[\\phi_i'=\\phi_i-\\bar{\\phi}\\,; i=1,2,...,N\\] Ahora vamos a intentar aproximar el valor de \\(\\phi'\\) en un punto de una malla, \\(\\phi_g\\), en términos de una combinación lineal de los valores en estaciones vecinas \\(\\phi_i\\) (señal). Entonces el problema de mínimos cuadrados es \\[\\phi'_g=\\sum\\limits^N_{i=1}b_i\\phi_i'\\] donde \\(\\phi'_g\\) son anonmalías en la malla regular y \\(\\phi_i'\\) son anomalías en las estaciones. Los mejores coeficientes son aquellos que minimizan el error cuadrático medio, es decir,\n\\[SEC=\\sum\\limits^N_{i=1}\\left(\\phi'_g-\\sum\\limits_{i=1}^N b_i\\phi'_i \\right)^2\n     =\\sum\\limits^N_{i=1}\\left(\\phi'_g-\\sum\\limits_{i=1}^N b_i\\phi'_i \\right)\\left(\\phi'_g-\\sum\\limits_{i=1}^N b_i\\phi'_i \\right)=\\] \\[\\sum\\limits^N_{i=1}\\left[\\phi'_g \\phi'_g -2\\sum\\limits_{i=1}^N b_i\\phi'_g\\phi'_i\n      +\\sum\\limits^N_{i=1}\\sum\\limits^N_{j=1} b_i b_j \\phi'_i \\phi'_j \\right]=\\] \\[=\\sum\\limits^N_{i=1}\\phi'_g \\phi'_g -2 \\sum\\limits^N_{i=1} b_i \\sum\\limits^N_{i=1} \\phi'_g\\phi'_i + \\sum\\limits^N_{i=1}\\sum\\limits^N_{j=1}\nb_i b_j \\sum\\limits^N_{i=1} \\phi'_i \\phi'_j\\,.\\]\nEl error normalizado se puede escribir como\n\\[\\epsilon=\\frac{SEC}{\\sum\\limits^N_{i=1}\\phi'_g \\phi'_g}=1-2 \\sum\\limits^N_{i=1} b_i \\frac{\\sum\\limits^N_{i=1} \\phi'_g\\phi'_i}{\\sum\\limits^N_{i=1}\\phi'_g \\phi'_g} + \\sum\\limits^N_{i=1}\\sum\\limits^N_{j=1} b_i b_j \\frac{\\sum\\limits^N_{i=1} \\phi'_i \\phi'_j}{\\sum\\limits^N_{i=1}\\phi'_g \\phi'_g}=\\] \\[=1-2 \\sum\\limits^N_{i=1} b_i r_{gi} + \\sum\\limits^N_{i=1}\\sum\\limits^N_{j=1} b_i b_j r_{ij}\\,,\\]\ndonde\n\\[r_{gi}=\\frac{\\sum\\limits^N_{i=1} \\phi'_g\\phi'_i}{\\sum\\limits^N_{i=1}\\phi'_g \\phi'_g}\\,\\,\\,\\,\\,{ y}\\,\\,\\,\\,\\,r_{ij}=\\frac{\\sum\\limits^N_{i=1} \\phi'_i \\phi'_j}{\\sum\\limits^N_{i=1}\\phi'_g \\phi'_g}\\,,\\]\nson matrices de correlación entre el punto de malla y las estaciones, y entre las estaciones, respectivamente.\nSi derivamos \\(\\epsilon\\) respecto los coeficientes, obtenemos la condición de minimización:\n\\[\\frac{\\partial{\\epsilon}}{\\partial{b_i}}=-2 r_{gi} + 2\\sum\\limits^N_{j=1}b_j r_{ij}=0\\,\\,\\,\\,\\,\\,\\,\\,\\,i=1,2,...,N\\,.\\] \\[r_{gi}=\\sum\\limits^N_{j=1}b_j r_{ij}\\,,\\] y los coeficientes en notación índice son\n\\[b_j=(r_{ij})^{-1}r_{gi}\\,\\]\nFinalmente, el valor de la medida en el punto de malla es\n\\[\\phi'_g=r_{gi}(r_{ij})^{-1}\\phi_i'\\]\no en notación matricial para un único punto de malla\n\\[\\phi'_g=r_{gs}(r_{ss})^{-1}\\phi'\\,,\\]\ndonde \\(\\phi'_g\\) es el valor de la anomalía en un punto de malla (matriz elemento, \\(1\\times 1\\)), \\(r_{gs}\\) es un vector fila compuesto por las correlaciones entre el punto de malla y las estaciones de medida, \\(r_{ss}\\) es una matriz de correlaciones entre todas las estaciones, y \\(\\phi'\\) es un vector columna con las anomalías en las estaciones.\nEn el caso de que tengamos \\(N\\) puntos de malla y \\(k\\) estaciones de medida, las ecuaciones básicas de la interpolación objetiva se pueden escribir en notación matricial como\n\\[\\left(\\begin{array}{c}\n  \\phi'_{g_1} \\\\\n  \\phi'_{g_2} \\\\\n  . \\\\\n  . \\\\\n  . \\\\\n  \\phi'_{g_N} \\\\\n     \\end{array}\\right)=\n\\left(\\begin{array}{ccccc}\n  r_{g_11} & r_{g_12} &... &  r_{g_1k}\\\\\n  r_{g_21} & r_{g_22} &... &  r_{g_2k}\\\\\n  . & . & . & . \\\\\n  . & . & . & . \\\\\n  . & . & . & . \\\\\n  r_{g_N1} & r_{g_N2} & ... & r_{g_Nk}\\\\\n     \\end{array}\\right)\n\\left(\\begin{array}{ccccc}\n  r_{11} & r_{12} &... &  r_{1k}\\\\\n  r_{21} & r_{22} &... &  r_{2k}\\\\\n  . & . & . & . \\\\\n  . & . & . & . \\\\\n  . & . & . & . \\\\\n  r_{k1} & r_{k2} & ... & r_{kk}\\\\\n     \\end{array}\\right)^{-1}       \\left(\\begin{array}{c}\n  \\phi'_1 \\\\\n  \\phi'_2 \\\\\n  . \\\\\n  . \\\\\n  . \\\\\n  \\phi'_k \\\\\n     \\end{array}\\right)\n     \\,.\\] \\[\\left( N \\times 1 \\right)\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\n  \\left( N \\times k \\right)\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\n  \\,\\,\\,\\,\\,\\,\\,\\,\\left( k \\times k \\right)\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\n  \\left( k \\times 1 \\right)\\,\\,\\,\\,\\,\\] \\\nEl error normalizado \\(\\epsilon\\) asociado al mapa interpolado es \\[\\epsilon=1-2 \\sum\\limits^N_{j=1} b_j r_{gi} + \\sum\\limits^N_{i=1}\\sum\\limits^N_{j=1} b_i b_j r_{ij}=\n           1-2 \\sum\\limits^N_{j=1} b_j \\sum\\limits^N_{j=1} b_j r_{ij} + \\sum\\limits^N_{i=1}\\sum\\limits^N_{j=1} b_i b_j r_{ij}=\\] \\[=1-2\\sum\\limits^N_{i=1}\\sum\\limits^N_{j=1} b_i b_j r_{ij} + \\sum\\limits^N_{i=1}\\sum\\limits^N_{j=1} b_i b_j r_{ij} =1-\\sum\\limits^N_{i=1}\\sum\\limits^N_{j=1} b_i b_j r_{ij}=\\] \\[= 1- \\sum\\limits^N_{j=1} b_j \\sum\\limits^N_{j=1} b_j r_{ij}  =1-\\sum\\limits^N_{i=1}r_{gi}b_i\\,,\\]\no en notación matricial para un único punto de malla \\[\\epsilon=1-r_{gs}(r_{ss})^{-1}r^T_{gs}\\,.\\]\nEn el caso de \\(N\\) puntos de malla y \\(k\\) estaciones \\[\\left(\\begin{array}{ccc}\n  \\epsilon_{1} \\\\\n  \\epsilon_{2} \\\\\n  . \\\\\n  . \\\\\n  . \\\\\n  \\epsilon_{N} \\\\\n     \\end{array}\\right)=\\text{Diag}\\left[\n     {\\textbf I}-\n\\left(\\begin{array}{ccccc}\n  r_{g_11} & r_{g_12} &... &  r_{g_1k}\\\\\n  r_{g_21} & r_{g_22} &... &  r_{g_2k}\\\\\n  . & . & . & . \\\\\n  . & . & . & . \\\\\n  . & . & . & . \\\\\n  r_{g_N1} & r_{g_N2} & ... & r_{g_Nk}\\\\\n     \\end{array}\\right)\n\\left(\\begin{array}{ccccc}\n  r_{11} & r_{12} &... &  r_{1k}\\\\\n  r_{21} & r_{22} &... &  r_{2k}\\\\\n  . & . & . & . \\\\\n  . & . & . & . \\\\\n  . & . & . & . \\\\\n  r_{k1} & r_{k2} & ... & r_{kk}\\\\\n     \\end{array}\\right)^{-1}\n\\left(\\begin{array}{ccccc}\n  r_{g_11} & r_{g_21} &... &  r_{g_N1}\\\\\n  r_{g_12} & r_{g_22} &... &  r_{g_N2}\\\\\n  . & . & . & . \\\\\n  . & . & . & . \\\\\n  . & . & . & . \\\\\n  r_{g_1k} & r_{g_2k} & ... & r_{g_Nk}\\\\\n     \\end{array}\\right) \\right]\n     \\,.\\] \\[\\left( N \\times 1 \\right)\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\n  \\left( N \\times k \\right)\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\n  \\,\\,\\,\\,\\,\\,\\,\\,\\left( k \\times k \\right)\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\n  \\left( k \\times N \\right)\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\]\nEste error es tal vez la característica mas importante de la interpolación objetiva. En general solo se muestra el mapa interpolado en las regiones donde \\(\\epsilon\\) emenor que un cierto valor. El error solo depende en las localizaciones de las estaciones y no en un valor particular de la medida. Es por ello que esta técnica puede ser usada para el diseño de experimentos. Es decir, nos sirve para saber cual debe ser la distribución óptima de las estaciones de medida para obtener el error mínimo en el mapa.\n\nNota: Si un punto de observación, \\(i=k\\), coincide con un punto de malla, entonces \\(r_{gk}=r_{kk}=1\\), y esperamos que el método de regresión nos de \\(b_k=1\\) y todos los demas pesos sean igual a cero. En este caso el valor interpolado en el punto de malla es igual al valor medido en la estación \\(\\phi'*g = r*{gk} (r\\_{kk})\\^{-1} \\phi'\\_k =1(1)\\^{-1}\\phi'\\_k=\\phi'\\_k\\) . En este caso el error es cero, \\(\\epsilon=1-r_{gk}(r_{kk})^{-1}r^T_{gk}=1-1(1)^{-1}1=0\\), ya que hemos asumido que los datos son perfectos. Si los puntos de las estaciones estan decorrelacionados con el punto de malla en cuestión (i.e., muy lejos de las estaciones), entonces \\(b_i=0\\) y \\(\\epsilon=1\\), y recuperamos la media o climatología\n\\[\\phi'_g=\\sum\\limits^N_{i=1}b_i\\phi'_i=0\\,\\,\\rightarrow\\,\\,\\phi'_g=\\phi_g-\\bar{\\phi}=0\\,\\,\\,\\,\\,\\text{and}\\,\\,\\,\\,\\,\\phi_g=\\bar{\\phi}\\]\nError observacional:\nVamos asumir ahora que las mediciones en las estaciones no son perfectas, es decir,\n\\[\\phi'_i=E[\\phi'_i] + \\delta_i\\,.\\]\nIgual que anteriormente asumimos que el error de instrumentación \\(\\delta_i\\) no esta correlacionado con la señal verdadera \\[&lt;E[\\phi_i]\\delta_i&gt;=0\\,,\\] y que el error instrumental entre estaciones tampoco esta correlacionado\n\\[&lt;\\epsilon_i \\epsilon_j&gt;=\n\\begin{cases}\n\\begin{array}{c}\n   &lt;\\delta^2&gt; \\,\\,\\,\\,\\text{si}\\,\\,\\,\\,i=j\\\\\n        0     \\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\text{si}\\,\\,\\,\\,i\\ne j\\\\\n\\end{array}\n\\end{cases}\\]\nEn este caso obtenemos\n\\[\\epsilon=1-2 \\sum\\limits^N_{i=1} b_i r_{gi} + \\sum\\limits^N_{i=1}\\sum\\limits^N_{j=1} b_i b_j r_{ij} + \\eta\\sum\\limits_{i=1}^N b_i^2\\,,\\]\ndonde \\(\\eta\\) es el cociente entre la varianza del error (ruido) y la varianza de las medidas, es decir el cociente ruido-señal \\[\\eta=\\frac{&lt;\\delta^2&gt;}{&lt;\\phi'_g\\phi'_g&gt;}\\,.\\]\nLa minimización del error nos da la condición \\[r_{gi}=\\sum\\limits^N_{j=1}b_j r_{ij}+\\eta b_i\\,,\\] y los coeficientes en notación índice son \\[b_j=(r_{ij} + \\eta I_{ij})^{-1}r_{gi}\\,,\\] donde \\(I_{ij}\\) es la matriz identidad. De esta expresión se deduce que cuando \\(\\eta\\) es grande (mucho ruido en la medida) entonces los coeficientes son mas pequeños respecto al caso de observaciones perfectas. Consecuentemente nuestro mapa interpolado se acerca mas a la climatología ya que la anomalía es menor para coeficientes mas pequeños\n\\[\\downarrow \\sum\\limits^N_{i=1}b_i\\phi'_i \\,\\,\\,\\,\\,\\rightarrow \\,\\,\\,\\,\\,\\downarrow\\phi'_g\\,\\,\\,\\,\\,\\,\\text{y}\\,\\,\\,\\,\\,\\phi_g\\rightarrow\\bar{\\phi}\\,.\\]\nIncluyendo errores observacionales, la interpolación objetiva tendera a la climatología o media y las nuevas observaciones serán incluidas pero con menos pesos. También podemos añadir diferentes errores ruido-señal en la diagonal principal para darle menos peso a las estaciones de medida que tienen mas incertidumbre asociada. Es conveniente entonces añadir errores observacionales al esquema de interpolación objetiva. Otra razón para ello es el caso en que existan dos estaciones que coincidan exactamente con un punto de malla. En este caso, si no hemos añadido error observacional la matriz de correlaciones entre las estaciones se convierte singular y el esquema de interpolación objetiva no se puede resolver. Por ejemplo para la interpolación en un único punto de malla a partir de dos estaciones de medida, la matriz de correlaciones sería singular\n\\[r_{ss}=\n       \\left(\\begin{array}{cc}\n  1 & 1 \\\\\n  1 & 1\n     \\end{array}\\right)\\,.\\]\nLa anomalía interpolada en notación matricial en un punto de malla es ahora \\[\\phi'_g=r_{gs}(r_{ss}+\\eta I)^{-1}\\phi'\\,,\\]\ny el error asociado al mapa interpolado en notación matricial en un único punto de malla es\n\\[\\epsilon=1-r_{gs}(r_{ss}+\\eta I)^{-1}r^T_{gs}\\,.\\]\nPara el caso de \\(N\\) puntos de malla y \\(k\\) estaciones obtenemos el mismo sistema que para el caso de medidas perfectas pero añadiendo el error ruido-señal en la diagonal principal de la matriz de correlaciones entre estaciones\n\\[\\left(\\begin{array}{ccc}\n  \\epsilon_{1} \\\\\n  \\epsilon_{2} \\\\\n  . \\\\\n  . \\\\\n  . \\\\\n  \\epsilon_{N} \\\\\n     \\end{array}\\right)=\\text{Diag}\\left[\n     {\\textbf I}-\n\\left(\\begin{array}{ccccc}\n  r_{g_11} & r_{g_12} &... &  r_{g_1k}\\\\\n  r_{g_21} & r_{g_22} &... &  r_{g_2k}\\\\\n  . & . & . & . \\\\\n  . & . & . & . \\\\\n  . & . & . & . \\\\\n  r_{g_N1} & r_{g_N2} & ... & r_{g_Nk}\\\\\n     \\end{array}\\right)\n\\left(\\begin{array}{ccccc}\n  r_{11}+\\eta & r_{12} &... &  r_{1k}\\\\\n  r_{21} & r_{22}+\\eta &... &  r_{2k}\\\\\n  . & . & . & . \\\\\n  . & . & . & . \\\\\n  . & . & . & . \\\\\n  r_{k1} & r_{k2} & ... & r_{kk}+\\eta\\\\\n     \\end{array}\\right)^{-1}\n\\left(\\begin{array}{ccccc}\n  r_{g_11} & r_{g_21} &... &  r_{g_N1}\\\\\n  r_{g_12} & r_{g_22} &... &  r_{g_N2}\\\\\n  . & . & . & . \\\\\n  . & . & . & . \\\\\n  . & . & . & . \\\\\n  r_{g_1k} & r_{g_2k} & ... & r_{g_Nk}\\\\\n     \\end{array}\\right) \\right]\n     \\,.\\]\n\\[\\left( N \\times 1 \\right)\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\n  \\left( N \\times k \\right)\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\n  \\,\\,\\,\\,\\,\\,\\,\\,\\left( k \\times k \\right)\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\n  \\left( k \\times N \\right)\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\]"
  },
  {
    "objectID": "chap4.html#funciones-empíricas-ortogonales-feos",
    "href": "chap4.html#funciones-empíricas-ortogonales-feos",
    "title": "Notas del Curso Análisis de Datos",
    "section": "Funciones Empíricas Ortogonales (FEOs)",
    "text": "Funciones Empíricas Ortogonales (FEOs)\n\nInterpretación de los sistemas `propios’\nAntes de entrar en la teoría para las FEOs, vamos a ver que los vectores propios son equivalentes a modos de oscilación de sistemas físicos. Imaginemos oscilaciones verticales de bolas en una cuerda. Las bolas tienen masa \\(m\\) y estan separadas por cuerdas elásticas de longitud \\({\\cal d}\\) en el equilibrio. Supongamos que los desplazamientos \\(y_n\\) son tan pequeños que la tensión de la cuerda \\({\\textbf T}\\) se puede considerar constante. El ángulo de cada cuerda es \\(\\theta\\) como se ilustra en la figura. Entonces, la ecuación del movimiento para la bola \\(n\\) es\n\\[m\\frac{d^2y_n}{dt^2}=-T sen\\theta_{n-1} - T sen\\theta_{n}\\,.\\]\nBajo la suposición de que los desplazamientos son pequeños, el \\(sen\\theta_n=tan\\theta_n\\), es decir, \\(sen\\theta_{n-1}=tan\\theta_{n-1}=y_n-y_{n-1}/d\\) y \\(sen\\theta_n=tan\\theta_n=(y_n-y_{n+1})/d\\). Entonces la ecuación queda \\[m\\frac{d^2y_n}{dt^2}=-T\\frac{y_n-y_{n-1}}{d} - T\\frac{y_n-y_{n+1}}{d}\\,,\\] y si reagrupamos \\[m\\frac{d^2y_n}{dt^2}=\\frac{T}{d}(y_{n-1}-2y_n+y_{n+1})\\,.\\] Vamos ahora a substituir una solución oscilatoria del tipo \\[y_n=Y_n e^{i\\omega t}\\] en la ecuación del movimiento \\[-m\\omega^2 Y_n e^{i\\omega t}=\\frac{T}{d}(Y_{n-1} e^{i\\omega t}-2Y_n e^{i\\omega t}+Y_{n+1} e^{i\\omega t})\\] \\[\\frac{-m\\omega^2 d}{T} Y_n e^{i\\omega t}=\\frac{T}{d}(Y_{n-1} e^{i\\omega t}-2Y_n e^{i\\omega t}+Y_{n+1} e^{i\\omega t})\\,.\\] Si definimos \\[\\lambda=\\frac{m\\omega^2 d}{T}\\,,\\] el sistema de ecuaciones a resolver es \\[-\\lambda Y_n=(Y_{n-1}-2Y_n +Y_{n+1})\\] \\[Y_n(2-\\lambda)-Y_{n-1}-Y_{n+1}=0\\,,\\] con las condiciones de frontera \\(Y_0=Y_{n+1}=0\\) en las paredes.\nVamos a suponer ahora el caso de dos bolas. Para la primera bola \\(n=1\\) \\[Y_1(2-\\lambda)-Y_0-Y_2=0\\,,\\] y para la bola \\(n=2\\) \\[Y_2(2-\\lambda)-Y_1-Y_3=0\\,.\\] Si aplicamos las condiciones de frontera \\(Y_0=Y_3=0\\), nos queda uns sistema “propio”, donde \\(\\lambda\\) son los autovalores.\n\\[\\left|\\begin{array}{cc}\n  2-\\lambda & -1 \\\\\n   -1 & 2-\\lambda\\\\\n        \\end{array}\\right|=0\\,.\\]\nEl polinomio característico es\n\\[\\lambda^2-4\\lambda+3=0\\,,\\]\ny tienes las raíces \\(\\lambda_1=1\\) y \\(\\lambda_2=3\\).\nResolvamos para los vectores propios:\\\n\n\\(\\lambda_1=1\\) entonces\n\n\\[Y_1-Y_2=0\\]\ny el vector propio es\n\\[{\\textbf y}=\\frac{1}{\\sqrt{2}}\\left(\\begin{array}{c}\n   1 \\\\\n   1 \\\\\n        \\end{array}\\right)\\] \\\n\n\\(\\lambda_2=3\\) entonces\n\n\\[-Y_1-Y_2=0\\,,\\]\ny el vector propio es\n\\[{\\textbf y}=\\frac{1}{\\sqrt{2}}\\left(\\begin{array}{c}\n   1 \\\\\n   -1 \\\\\n        \\end{array}\\right)\\,.\\]\nEstas soluciones representan los modos oscilatorios de un sistema físico de dos bolas oscilando verticalmente. Los modos de oscilación se muestran en la figura. Los modos oscilan independientemente uno de otro, y la evolución del sistema es una combinación lineal de los dos modos. De esta forma, lo que estamos haciendo al resolver el problema de las bolas en una cuerda elástica es precisamente la solución de un sistema “propio”.\n\n\nDefinición de FEOs\nUn análisis en FEOs busca estructuras en los datos que explican la mayor cantidad de varianza de un conjunto de datos bidimensional. La primera dimensión es la dimension en la que deseamos encontrar una estructura, y la otra dimension es la dimensión en la que se muestrean las diferentes realizaciones. Por ejemplo, un conjunto de series temporales de datos distribuidos espacialmente (i.e. arreglo de anclajes). La primera dimensión es espacio y la segunda es tiempo. Las estructuras en la dimensión espacial son las FEOs, mientras que las estructuras en la dimensión de muestreo se denominan Componentes Principales (CPs). \\ Tanto las FEOs como las PCs son ortogonales en sus dimensiones. Las FEOs/PCs pueden entenderse de diferentes formas: \\ (i) Transforma variables correlacionadas en un conjunto de variables no correlacionadas que expresan mejor la relación dinámica entre los datos originales. \\ (ii) Identifica y ordena los vectores ortogonales (o dimensiones) a lo largo de los cuales nuestro conjunto de datos presenta la mayor varianza. \\ (iii) Una vez definidas las FEOs, es posible encontrar la mejor aproximación de los datos originales con el mínimo número de vectores ortogonales.\nEn general, aplicaremos el análisis en FEOs para describir de manera mas sencilla conjuntos de datos organizados en matrices \\(M\\times N\\):}\n\n\nUna matriz espacio-tiempo: Medidas de una variable en \\(M\\) localizaciones y \\(N\\) tiempos.\n\nUna matriz parámetro-tiempo: Medidas de \\(M\\) variables en una localización y \\(N\\) tiempos.\n\nUna matriz parámetro-espacio: Medidas de \\(M\\) variables tomadas en \\(N\\) localizaciones al mismo tiempo.\n\n\nNota: Un error común es considerar que las FEOs se corresponden con modos de físicos oceánicos. Eso no es cierto!. Los modos físicos en el océano son modos de oscilación que se obtienen considerando las ecuaciones que rigen el movimiento y condiciones de frontera; las FEOs son simplemente funciones ortogonales que explican la mayor cantidad de varianza de un conjunto de datos.\nAunque los procesos físicos dominantes son representados por los primeros modos de oscilación, no existe una correspondencia uno a uno entre modos físicos y FEOs.\n\n\nTeoría\nSupongamos M localizaciones de medición con series temporales de temperatura de N elementos. Queremos descomponer la serie temporal de temperatura en una localización dada \\(k\\) como una combinación lineal de \\(M\\) funciones ortogonales \\({\\textbf b}_i\\) cuyas amplitudes son pesadas con \\(M\\) coeficientes dependientes del tiempo, es decir,\n\\[{\\textbf T}(t)=\\sum\\limits^M_{i=1}[\\alpha_i(t){\\textbf b}_{i}]\\,,\\]\ndonde \\(\\alpha_i(t)\\) es la amplitud del modo ortogonal \\(i\\) al tiempo \\(t=t_n(1\\le n\\le N)\\). Los coeficientes \\(\\alpha_i(t)\\) nos informan de como varian los modos \\({\\textbf b}_{i}\\) con el tiempo. Necesitamos tantas funciones ortogonales como estaciones con series temporales tenemos para poder describir la varianza total de los datos originales de temperatura a cada tiempo. Sin mebargo, en términos prácticos podemos explicar una gran cantidad de varianza de los datos originales con las primeras FEOs. Podemos ver el problema al revés, es decir, tenemos \\(N\\) funciones temporales cuyas amplitudes son pesadas por \\(M\\) coeficientes que varian en el espacio. En este caso hablamos de PCs. Ya sea la reducción de los datos en funciones ortogonales espaciales (FEOs) o temporales (PCs), obtenemos los mismos resultados.\nPuesto que queremos \\({\\textbf b}_{i}\\) ser ortogonal, requerimos \\[{\\textbf b}^T_i {\\textbf b}_{j}=\\delta_{ij}\\,,\\] y los coeficientes temporales \\(\\alpha_i={\\textbf b}^T_{i}{\\textbf T}(t)\\). Son precisamente estos coeficientes temporales las CPs, es decir, la proyección de los datos originales sobre las FEOs o la expresión de los datos originales en la nueva base de vectores ortogonales (el nuevo sistema de coordenadas).\nEl objetivo del análisis es encontrar una base de vectores ortogonales tal que las funciones \\(\\alpha_i(t)\\) no esten correlacionadas \\[&lt;\\alpha_i \\alpha_j&gt;=&lt;{\\textbf b}^T_{i}{\\textbf T}{\\textbf b}^T_{j}{\\textbf T}&gt;=&lt;{\\textbf b}^T_{i}{\\textbf T}{\\textbf T}^T{\\textbf b}_{j}&gt;={\\textbf b}^T_{i}&lt;{\\textbf T} {\\textbf T}^T&gt;{\\textbf b}_{j}=\\delta_{ij}&lt;\\alpha^2_i&gt;\\,,\\] donde \\[&lt;\\alpha_i^2&gt;=\\frac{1}{N}\\sum\\limits^N_{n=1}=\\alpha^2_i(t_n)\\,.\\] Es decir, la matriz de covarianza de \\(\\alpha_i(t)\\) será una matriz diagonal \\({\\textbf D}\\). Para \\(M\\) posiciones se puede reescribir como\n\\[{\\textbf B}^T&lt;{\\textbf T}{\\textbf T}^T&gt;{\\textbf B}={\\textbf D}\\,\\,\\,\\,\\,\\,\\,\\,\\,{ o}\\,\\,\\,\\,\\,\\,\\,\\,{\\textbf B}^T{\\textbf T}{\\textbf T}^T{\\textbf B}=N{\\textbf D}\\]\ndonde \\({\\textbf B}\\) es una matriz ortogonal cuyas columnas son los vectores ortogonales \\({\\textbf b}_i\\) y \\({\\textbf D}\\) es una matriz diagonal compuesta por las varianzas de las funciones temporales \\(\\alpha_i(t)\\). Si multiplicamos por \\({\\textbf B}\\) llegamos a un sistema propio\n\\[&lt;{\\textbf T}{\\textbf T}^T&gt;{\\textbf B}={\\textbf B}{\\textbf D}\\,.\\]\nEste tipo de sistema propio es conocido. La diagonal de \\({\\textbf D}\\) esta compuesta por valores propios y las columnas de \\({\\textbf B}\\) son los vectores propios. Los vectores propios son denominados FEOs, y los valores propios son las varianzas de las amplitudes \\(\\alpha_i\\). Básicamente hemos realizado una transformación de coordenadas de tal forma que los vectores propios \\({\\textbf b}_i\\)\nindican combinaciones lineales de datos que no estan correlacionados (i.e., \\(&lt;\\alpha_i \\alpha_j&gt;=\\delta_{ij}&lt;\\alpha^2_i&gt;\\)). Esta descomposición de los datos en FEOs es óptima en el sentido de mínimos cuadrados. Imaginemos que queremos un conjunto de \\(K\\) vectores que mejor aproxima \\({\\textbf T}\\)\n\\[&lt;(\\hat{\\textbf T}-{\\textbf T})^T(\\hat{\\textbf T}-{\\textbf T})&gt;=&lt;{\\textbf T}^T{\\textbf T}&gt;\n  -\\sum\\limits^K_{i=1}&lt;\\alpha^2_i&gt;\\,.\\]\nEl problema en mínimos cuadrados, bajo las restricciones que las funciones \\({\\textbf b}_i\\) sean ortogonales, se puede escribir a partir de los multiplicadores de Lagrange\n\\[{\\textbf \\cal L}=\\sum\\limits^K_{i=1}\\left[{\\textbf b}^T_i&lt;{\\textbf T}{\\textbf T}^T&gt;{\\textbf b}_i-\n            \\lambda_i({\\textbf b}_i^T{\\textbf b}_i-1)\\right]\\,.\\]\nSi derivamos \\({\\textbf \\cal L}\\) respecto de \\({\\textbf b}_i\\) obtenemos el sistema propio\n\\[&lt;{\\textbf T}{\\textbf T}^T&gt;{\\textbf b}_i=\\lambda_i{\\textbf b}_i\\,.\\]\nDe este análisis deducimos que las primeras \\(K\\) funciones ortogonales o FEOs son las mejores funciones que explican la máxima varianza de los datos originales, donde los valores porpios estan ordenados de mayor a menor. Es decir, no existe un subset de datos mas pequeño que \\(K\\) funciones ortogonales que produce el error cuadrático medio menor. En este sentido las FEOs son los mejores `descriptores’ de la varianza de los datos.\nUna matriz de datos de \\(M\\) localizaciones y \\(N\\) tiempos se puede descomponer\n\\[{\\textbf C}{\\textbf B}={\\textbf B}{\\textbf \\Lambda}\\,,\\]\ndonde\n\\[{\\textbf C}={\\textbf T}{\\textbf T}^T=\\left(\\begin{array}{cccc}\n   \\sum\\limits_{i=1}^N{T_{1}(t_i)T_{1}(t_i)} & \\sum\\limits_{i=1}^N{T_{1}(t_i)T_{2}(t_i)} & ... & \\sum\\limits_{i=1}^N{T_{1}(t_i)T_{M}(t_i)}\\\\\n   \\sum\\limits_{i=1}^N{T_{2}(t_i)T_{1}(t_i)} & \\sum\\limits_{i=1}^N{T_{2}(t_i)T_{2}(t_i)} & ... & \\sum\\limits_{i=1}^N{T_{2}(t_i)T_{M}(t_i)}\\\\\n                . & . & . \\\\\n        . & . & . \\\\\n        . & . & . \\\\\n   \\sum\\limits_{i=1}^N{T_{M}(t_i)T_{1}(t_)} & \\sum\\limits_{i=1}^N{T_{M}(t_i)T_{2}(t_i)} & ... & \\sum\\limits_{i=1}^N{T_{M}(t_i)T_{M}(t_i)}\\\\\n        \\end{array}\\right)\\,,\\]\nes la matriz de covarianza entre las series temporales de temperatura en localizaciones espaciales, unas con otras;\n\\[{\\textbf T}=\\left(\\begin{array}{cccc}\n   T_{1}(t_1) & T_{1}(t_2) & ... & T_{1}(t_N)\\\\\n   T_{2}(t_1) & T_{2}(t_2) & ... & T_{2}(t_N)\\\\\n                . & . & . \\\\\n        . & . & . \\\\\n        . & . & . \\\\\n   T_{M}(t_1) & T_{M}(t_2) & ... & T_{M}(t_N)\\\\\n        \\end{array}\\right)\\] es la\nmatriz de datos de temperatura;\n\\[{\\textbf B}=\\left(\\begin{array}{cccc}\n   b_{11} & b_{12} & ... & b_{1M}\\\\\n   b_{21} & b_{22} & ... & b_{2M}\\\\\n                . & . & . \\\\\n        . & . & . \\\\\n        . & . & . \\\\\n   b_{M1} & b_{M2} & ... & b_{MM}\\\\\n        \\end{array}\\right)\\,,\\] es la matriz con los vectores propios \\({\\textbf b}_i\\) como columnas, y \\[{\\textbf \\Lambda}=\\left(\\begin{array}{cccc}\n   \\lambda_1 & 0 & ... & 0\\\\\n  0 & \\lambda_2 & ... &0\\\\\n                . & . & . \\\\\n        . & . & . \\\\\n        . & . & . \\\\\n   0 & 0 & ... & \\lambda_M\\\\\n        \\end{array}\\right)\\,,\\]\nes la matriz diagonal compuesta por los valores propios \\(\\lambda_i=&lt;\\alpha_i^2&gt;\\)\n\nOtra forma de entender las FEOs (PCs) consiste en un análisis en valores propios de las matrices de dispersión de nuestras matrices de datos. La matriz de dispersión es el producto matricial de la matriz con su transpuesta, o a la inversa. La primera matriz de dispersión es\n\\[{\\textbf C}={\\textbf T}{\\textbf T}^T=\\left(\\begin{array}{cccc}\n   T_{1}(t_1) & T_{1}(t_2) & ... & T_{1}(t_N)\\\\\n   T_{2}(t_1) & T_{2}(t_2) & ... & T_{2}(t_N)\\\\\n                . & . & . \\\\\n        . & . & . \\\\\n        . & . & . \\\\\n   T_{M}(t_1) & T_{M}(t_2) & ... & T_{M}(t_N)\\\\\n        \\end{array}\\right)\n\\left(\\begin{array}{cccc}\n   T_{1}(t_1) & T_{2}(t_1) & ... & T_{M}(t_1)\\\\\n   T_{1}(t_2) & T_{2}(t_2) & ... & T_{M}(t_2)\\\\\n                . & . & . \\\\\n        . & . & . \\\\\n        . & . & . \\\\\n   T_{1}(t_N) & T_{2}(t_N) & ... & T_{M}(t_N)\\\\\n        \\end{array}\\right)=\\]\n\\[=\\left(\\begin{array}{cccc}\n   \\sum\\limits_{i=1}^N{T_{1}(t_i)T_{1}(t_i)} & \\sum\\limits_{i=1}^N{T_{1}(t_i)T_{2}(t_i)} & ... & \\sum\\limits_{i=1}^N{T_{1}(t_i)T_{M}(t_i)}\\\\\n   \\sum\\limits_{i=1}^N{T_{2}(t_i)T_{1}(t_i)} & \\sum\\limits_{i=1}^N{T_{2}(t_i)T_{2}(t_i)} & ... & \\sum\\limits_{i=1}^N{T_{2}(t_i)T_{M}(t_i)}\\\\\n                . & . & . \\\\\n        . & . & . \\\\\n        . & . & . \\\\\n   \\sum\\limits_{i=1}^N{T_{M}(t_i)T_{1}(t_i)} & \\sum\\limits_{i=1}^N{T_{M}(t_i)T_{2}(t_i)} & ... & \\sum\\limits_{i=1}^N{T_{M}(t_i)T_{M}(t_i)}\\\\\n        \\end{array}\\right)\\,,\\]\n\\[\\left( M \\times M \\right)\\]\ny la segunda es\n\\[{\\textbf C}={\\textbf T}^T{\\textbf T}=\n\\left(\\begin{array}{cccc}\n   T_{1}(t_1) & T_{2}(t_1) & ... & T_{M}(t_1)\\\\\n   T_{1}(t_2) & T_{2}(t_2) & ... & T_{M}(t_2)\\\\\n                . & . & . \\\\\n        . & . & . \\\\\n        . & . & . \\\\\n   T_{1}(t_N) & T_{2}(t_N) & ... & T_{M}(t_N)\\\\\n        \\end{array}\\right)\\left(\\begin{array}{cccc}\n   T_{1}(t_1) & T_{1}(t_2) & ... & T_{1}(t_N)\\\\\n   T_{2}(t_1) & T_{2}(t_2) & ... & T_{2}(t_N)\\\\\n                . & . & . \\\\\n        . & . & . \\\\\n        . & . & . \\\\\n   T_{M}(t_1) & T_{M}(t_2) & ... & T_{M}(t_N)\\\\\n\\end{array}\\right)=\\]\n\\[=\\left(\\begin{array}{cccc}\n   \\sum\\limits_{i=1}^M{T_{i}(t_1)T_{i}(t_1)} & \\sum\\limits_{i=1}^M{T_{i}(t_1)T_{i}(t_2)} & ... & \\sum\\limits_{i=1}^i{T_{1}(t_1)T_{i}(t_N)}\\\\\n   \\sum\\limits_{i=1}^M{T_{i}(t_2)T_{i}(t_1)} & \\sum\\limits_{i=1}^M{T_{i}(t_2)T_{i}(t_2)} & ... & \\sum\\limits_{i=1}^M{T_{i}(t_2)T_{i}(t_N)}\\\\\n                . & . & . \\\\\n        . & . & . \\\\\n        . & . & . \\\\\n   \\sum\\limits_{i=1}^M{T_{i}(t_N)T_{i}(t_1)} & \\sum\\limits_{i=1}^M{T_{i}(t_N)T_{i}(t_2)} & ... & \\sum\\limits_{i=1}^M{T_{i}(t_N)T_{i}(t_N)}\\\\\n        \\end{array}\\right)\\,.\\] \\[\\left( N \\times N \\right)\\]\nAmbas matrices de dispersión obtenidas del producto de la matriz de datos consigo misma son matrices de covarianza simétricas. La primera matriz de dispersión \\({\\textbf T}{\\textbf T}^T\\) es una matriz \\(M \\times M\\) con lo que hemos eliminado la dimensión temporal o dimensión de muestreo. En este caso, la matriz de dispersión es una matriz de covarianzas temporales (determinadas por sus variaciones temporales) de las estaciones unas con otras. En la segunda matriz \\({\\textbf T}^T{\\textbf T}\\) se invierten las dimensiones, la matriz resultante es \\(N\\times N\\), y es una matriz de covarianzas espaciales (determinadas por sus variaciones espaciales) entre los diferenetes tiempos.\n\nEjemplo: Un ejmplo sencillo viene dado por el diagrama de dispersión de la figura. La primera EOF o vector propio que explica la mayor varianza sería la recta que se ajusta al conjunto de puntos y la segunda EOF sería la línea perpendicular a la ajustada.\nRelación entre FEOs y CPs\nComo ya hemos visto nuestro sistema propio es\n\\[{\\textbf B}^T&lt;{\\textbf T}{\\textbf T}^T&gt;{\\textbf B}={\\textbf D}\\,\\,\\,\\,\\,\\,\\,\\,\\,{ o}\\,\\,\\,\\,\\,\\,\\,\\,{\\textbf B}^T{\\textbf T}{\\textbf T}^T{\\textbf B}=N{\\textbf D}\\,,\\]\ndonde \\({\\textbf B}\\) es una matriz cuyas columnas son los vectores propios y \\({\\textbf D}\\) es una matriz cuadrada con los \\(M\\) valores propios en la diagonal principal. Si queremos expresar los datos originales en términos de los vectores propios, entonces debemos usar la definición\n\\[\\alpha_i(t)={\\textbf b}^T_{i}{\\textbf T}(t)\\,,\\]\nque en notación matricial se puede expresar como\n\\[{\\textbf Z}={\\textbf B}^T{\\textbf T}\\,,\\]\ny finalmente para recuperar los datos originales a partir de la base de FEOs usamos\n\\[{\\textbf T}={\\textbf B}{\\textbf Z}\\,,\\]\nya que \\({\\textbf B}{\\textbf B}^T={\\textbf I}\\). La matriz \\({\\textbf Z}\\) contiene los vectores de las CPs, que no son mas que las amplitudes por las cuales multiplicamos las FEOs para obtener los datos originales de vuelta. De esta forma podemos ir de un espacio (vectores propios) al otro (datos originales) con la matriz de FEOs. Supongamos que tenemos un conjunto de vectores propios ortogonales y normalizados. El primero de ellos por ejemplo sería\n\\[{\\textbf e}=\\left(\\begin{array}{c}\n   e_{11} \\\\\n   e_{21} \\\\\n         .\\\\\n         .\\\\\n     .\\\\\n   e_{M1} \\\\\n        \\end{array}\\right)\\,.\\]\nSi ponemos todos los vectores propios en columna obtenemos la matriz cuadrada \\({\\textbf B}\\), la cual es ortonormal \\({\\textbf B}^T{\\textbf B}={\\textbf I}\\). Si queremos proyectar un vector propio sobre los datos originales y obtener la amplitud de este vector propio en cada tiempo, debemos de hacer\n\\[{\\textbf e}^T{\\textbf T}=\\left(\\begin{array}{ccccccc}\n   e_{11} &\n   e_{21} & . & . & . & e_{M1} \\\\\n        \\end{array}\\right)\n    \\left(\\begin{array}{cccc}\n   T_{1}(t_1) & T_{1}(t_2) & ... & T_{1}(t_N)\\\\\n   T_{2}(t_1) & T_{2}(t_2) & ... & T_{2}(t_N)\\\\\n                . & . & . \\\\\n        . & . & . \\\\\n        . & . & . \\\\\n   T_{M}(t_1) & T_{M}(t_2) & ... & T_{M}(t_N)\\\\\n        \\end{array}\\right)=\n    \\left(\\begin{array}{ccccccc}\n   z_{11} &\n   z_{12} & . & . & . & z_{1N} \\\\\n        \\end{array}\\right)\\,,\\]\ndonde, por ejemplo, \\(z_{11}=e_{11}T_1(t_1)+ e_{21}T_2(t_1) + ... + e_{M1}T_M(t_1)\\,.\\) Si hacemos lo mismo para todos los otros vectores propios, obtenemos series temporales de longitud \\(N\\) para cada FEO, lo cual se denominan componentes principales de cada EOF\n\\[{\\textbf Z}={\\textbf B}^T{\\textbf T}\\,.\\] \\[M\\times N\\]\nLas CPs también son ortogonales. Si substituimos \\({\\textbf T}={\\textbf B}{\\textbf Z}\\), en \\({\\textbf B}^T{\\textbf T}{\\textbf T}^T{\\textbf B}=N{\\textbf D}\\) obtenemos\n\\[{\\textbf B}^T{\\textbf B}{\\textbf Z}({\\textbf B}{\\textbf Z})^T{\\textbf B}=N{\\textbf D}\\]\n\\[{\\textbf I}{\\textbf Z}{\\textbf Z}^T{\\textbf B}^T{\\textbf B}=\nN{\\textbf D}\\]\n\\[{\\textbf I}{\\textbf Z}{\\textbf Z}^T{\\textbf I}=N{\\textbf D}\\]\n\\[{\\textbf Z}{\\textbf Z}^T=N{\\textbf D}\\,\\,\\,\\,\\,\\,{ o}\\,\\,\\,\\,\\,\\,&lt;{\\textbf Z}{\\textbf Z}^T&gt;={\\textbf D}\\,.\\]\nPor lo tanto no solo las FEOs sino también las CPs son ortogonales.\nEquivalencia con descomposición en valores singulares (SVD)\nSupongamos la matriz de datos compuesta por \\(M\\) series temporales de temperatura de longitud \\(N\\) \\[{\\textbf T}=\\left(\\begin{array}{cccc}\n   T_{1}(t_1) & T_{1}(t_2) & ... & T_{1}(t_N)\\\\\n   T_{2}(t_1) & T_{2}(t_2) & ... & T_{2}(t_N)\\\\\n                . & . & . \\\\\n        . & . & . \\\\\n        . & . & . \\\\\n   T_{M}(t_1) & T_{M}(t_2) & ... & T_{M}(t_N)\\\\\n        \\end{array}\\right)\\,.\\]\nLa matriz de dispersión es \\[&lt;{\\textbf T}{\\textbf T}^T&gt;=\\frac{1}{N} {\\textbf T}{\\textbf T}^T\\,.\\]\nSabemos que la matriz \\({\\textbf T}\\) puede descomponerse en una matriz ortogonal \\({\\textbf U}\\), una matriz diagonal \\({\\textbf S}\\), y la transpuesta de una matriz ortogonal \\({\\textbf V}\\). Esto es \\[{\\textbf T}={\\textbf U}{\\textbf S}{\\textbf V}^T\\,,\\] donde el número de valores singulares no nulos indican el rango de \\({\\textbf T}\\). Si \\(K&lt;N\\) y el número de filas (i.e., los datos) son linealmente independientes entonces el rango será \\(K\\). Ahora la matriz de covarianza es\n\\[\\frac{1}{N} {\\textbf T}{\\textbf T}^T = \\frac{1}{N} ({\\textbf U}{\\textbf S}{\\textbf V}^T)({\\textbf U}{\\textbf S}{\\textbf V}^T)^T=\n\\frac{1}{N}{\\textbf U}{\\textbf S}{\\textbf V}^T{\\textbf V}({\\textbf U}{\\textbf S})^T=\\frac{1}{N} {\\textbf U}{\\textbf S}{\\textbf V}^T{\\textbf V}{\\textbf S}^T{\\textbf U}^T=\\frac{1}{N} {\\textbf U}{\\textbf S}{\\textbf S}^T{\\textbf U}^T\\,.\\]\nLa derecha del igual es la descomposición en valores propios de la matriz de covarianza, donde \\({\\textbf S}{\\textbf S}^T\\) es cuadrada y diagonal con los elementos igual a \\(N\\lambda_i\\); y las columnas de \\({\\textbf U}\\) son las FEOs. Las amplitudes de las FEOs vienen dadas por las filas de la matriz\n\\[{\\textbf U}^T{\\textbf T}={\\textbf S}{\\textbf V}\\,,\\]\nasociado con valores singulares no nulos.\nInterpretación de las FEOs\nComo comentario final hay que decir que las FEOs no son muy fáciles de interpretar. Matemáticamente son estructuras que representan la mayor cantidad de varianza de los datos originales y que son ortogonales entre ellas. En ocasiones estas estructuras nos dan estructuras con sentido físico en un conjunto de datos, en otras no. Las estructuras particulares encontradas dependerán de cómo hemos acomodado nuestra matriz bidimensional de datos. Algunas sugerencias para detectar si las FEOs tienen sentido físico son las siguientes:\n\n\n¿La varianza de tu FEO es mas grande que lo que esperabas si los datos originales no tenían estructura alguna?\n\n¿Existe una explicación apriori para las estructuras que has encontrado? ?`Se pueden explicar las estructuras en términos de alguna teoría? ¿Las estructuras se comportan consistentemente con la teoría apriori?\n\n¿Cuán robustas son las estructuras a la elección del dominio de la estructura? ¿Si cambias el dominio del análisis, esas estructuras cambian significantemente? ¿Si las estructuras estan definidas en un espacio geográfico, y cambias el tamaño de la región, las estructuras cambian significativamente? ¿Si las estructuras estan definidas en el espacio de parámetros y añades o eliminas un parámetros, los resultados cambian de forma suave o aleatoriamente?\n\n\n(4)¿Cuán robustas son las estructuras a los datos usados? ¿Si divides los datos originales en fracciones menores y haces el analisis de cada fracción, obtienes las mismas estructuras?\nEjemplo de descomposición en valores singulares\n  Supongamos la matriz\n\\[{\\textbf A}=\\left(\\begin{array}{ccc}\n  3 & 1 & 1\\\\\n  -1 & 3 & 1\\\\\n      \\end{array}\\right)\\,.\\]\nPara encontrar \\({\\textbf U}\\) debemos resolver el problema en vectores y valores propios de la matriz \\({\\textbf A}{\\textbf A}^T\\)\n\\[{\\textbf A}{\\textbf A}^T=\\left(\\begin{array}{ccc}\n  3 & 1 & 1\\\\\n  -1 & 3 & 1\\\\\n      \\end{array}\\right)\n      \\left(\\begin{array}{ccc}\n  3 & -1 \\\\\n  1 & 3 \\\\\n  1 & 1 \\\\\n      \\end{array}\\right)=\\left(\\begin{array}{cc}\n  11 & 1 \\\\\n  1 & 11 \\\\\n      \\end{array}\\right)\\]\nEl sistema de ecuaciones `propio’ es\n\\[\\left(\\begin{array}{cc}\n  11 & 1 \\\\\n  1 & 11 \\\\\n      \\end{array}\\right)\\left(\\begin{array}{cc}\n  x_1 \\\\\n  x_2 \\\\\n      \\end{array}\\right)=\\lambda\\left(\\begin{array}{cc}\n  x_1 \\\\\n  x_2 \\\\\n      \\end{array}\\right)\\]\nSi resolvemos para \\(\\lambda\\)\n\\[\\left|\\begin{array}{cc}\n  11-\\lambda & 1 \\\\\n  1 & 11-\\lambda \\\\\n      \\end{array}\\right|=0\\,,\\]\nlo que deja el polinomio característico\n\\[(\\lambda-10)(\\lambda-12)=0\\,,\\]\ncon raices (valores propios) \\(\\lambda_1=12\\) y \\(\\lambda_2=10\\).\nSi sustituimos en el sistema de ecuaciones `propio’ el primer valor propio \\(\\lambda_1=12\\) \\[(11-12)x_1 + x_2=0\\]\n\\[x_1=x_2\\]\nPara \\(x_1=1\\) obtenemos que \\(x_2=1\\). Entonces obtenemos el vector propio \\({\\textbf v}_1=[1,1]\\)“”\nSi sustituimos en el sistema de ecuaciones “propio” el primer valor propio \\(\\lambda_1=10\\) \\[(11-10)x_1 + x_2=0\\]\n\\[x_1=-x_2\\] Para \\(x_1=1\\)\nobtenemos que \\(x_2=-1\\). Entonces obtenemos el vector propio \\({\\textbf v}_2=[1,-1]\\).\nSi organizamos la matriz con columnas correspondientes a los vectores propios asociados a los valores propios de mayor a menor, obtenemos\n\\[\\left(\\begin{array}{cc}\n  1 & 1 \\\\\n  1 & -1 \\\\\n      \\end{array}\\right)\\]\nFinalmente, sabemos que \\({\\textbf U}\\) tiene que ser ortonormal. Vamos a usar el proceso de Gram-Schmidt para ortonormalizar las columnas de \\({\\textbf U}\\). Empezamos normalizando la primera columna\n\\[{\\textbf u}_1=\\frac{{\\textbf v}_1}{|{\\textbf v}_1|}=\\frac{[1,1]}{\\sqrt{2}}=\\left[ \\frac{1}{\\sqrt{2}}, \\frac{1}{\\sqrt{2}} \\right]\\,,\\]\ny calculamos el vector ortonormal como\n\\[{\\textbf w}_2={\\textbf v}_2-{\\textbf u}_1\\cdot{\\textbf v}_2{\\textbf u}_1=[1,-1]-\\left[ \\frac{1}{\\sqrt{2}}, \\frac{1}{\\sqrt{2}} \\right]\n\\cdot[1,-1]\\left[ \\frac{1}{\\sqrt{2}}, \\frac{1}{\\sqrt{2}} \\right]=[1,-1]-[0,0]=[1,-1]\\,.\\]\nSi lo normalizamos\n\\[{\\textbf u}_2=\\frac{{\\textbf w}_2}{|{\\textbf w}_2|}=\\left[ \\frac{1}{\\sqrt{2}}, \\frac{-1}{\\sqrt{2}} \\right]\\,,\\]\ndejando la matriz\n\\[{\\textbf U}=\\left(\\begin{array}{cc}\n  \\frac{1}{\\sqrt{2}} & \\frac{1}{\\sqrt{2}} \\\\\n  \\frac{1}{\\sqrt{2}} & -\\frac{1}{\\sqrt{2}} \\\\\n\\end{array}\\right)\\,.\\]\nSimilarmente, para calcular la matriz \\({\\textbf V}\\) debemos resolver el problema en vectores y valores propios de la matriz \\({\\textbf A}^T{\\textbf A}\\)\n\\[{\\textbf A}^T{\\textbf A}=\\left(\\begin{array}{ccc}\n  3 & -1 \\\\\n  1 & 3 \\\\\n  1 & 1 \\\\\n      \\end{array}\\right)\\left(\\begin{array}{ccc}\n  3 & 1 & 1\\\\\n  -1 & 3 & 1\\\\\n      \\end{array}\\right)=\n      \\left(\\begin{array}{ccc}\n  10 & 0 & 2 \\\\\n  0 & 10 & 4\\\\\n  2 & 4 & 2\\\\\n      \\end{array}\\right)\n      \\]\nEl sistema de ecuaciones `propio’ es\n\\[\\left(\\begin{array}{ccc}\n  10 & 0 & 2 \\\\\n  0 & 10 & 4\\\\\n  2 & 4 & 2\\\\\n      \\end{array}\\right)\\left(\\begin{array}{c}\n  x_1 \\\\\n  x_2 \\\\\n  x_3 \\\\\n      \\end{array}\\right)=\\lambda \\left(\\begin{array}{c}\n  x_1 \\\\\n  x_2 \\\\\n  x_3 \\\\\n      \\end{array}\\right)\\]\n\\[(10-\\lambda)x_1 + 2x_3=0\\] \\[(10-\\lambda)x_2 + 4x_3=0\\] \\[2x_1 + 4x_2 + (2-\\lambda)x_3=0\\]\nSi resolvemos para \\(\\lambda\\)\n\\[\\left|\\begin{array}{ccc}\n  10-\\lambda & 0 & 2 \\\\\n  0 & 10-\\lambda & 4\\\\\n  2 & 4 & 2-\\lambda\\\\\n      \\end{array}\\right|=0\\]\nlo que deja el polinomio característico\n\\[\\lambda (\\lambda-10)(\\lambda-12)=0\\,,\\]\ncon raices (valores propios) \\(\\lambda_1=12\\), \\(\\lambda_2=10\\), y \\(\\lambda_3=0\\). Substituyendo en el sistema `propio’ para \\(\\lambda_1=12\\)\n\\[(10-12)x_1 + 2x_3 = -2x_1+2x_3=0\\] \\[x_1=1;\\,\\,x_3=1\\]\n\\[(10-12)x_2 + 4x_3 = -2x_2+4x_3=0\\] \\[x_2=2x_3\\]\n\\[x_2=2\\] Entonces, para \\(\\lambda_1=12\\) obtenemos el vector propio \\({\\textbf v}_1=[1,2,1]\\).\n\nPara \\(\\lambda_2=10\\)\n\\[(10-10)x_1 + 2x_3=2x_3=0\\] \\[x_3=0\\]\n\\[2x_1+4x_2=0\\] \\[x_1=-2x_2\\]\n\\[x_1=2;\\,\\,x_2=-1\\]\ny obtenemos \\({\\textbf v}_2=[2,-1,0]\\) para \\(\\lambda_2=10\\). \\ Finalmente, para \\(\\lambda_3=0\\)\n\\[10x_1+2x_3=0\\] \\[x_3=-5\\] \\[10x_1-20=0\\]\n\\[x_2=2\\] \\[2x_1+8-10=0\\]\n\\[x_1=1\\]\nlo que implica que para \\(\\lambda_3=0\\) \\({\\textbf v}_3=[1,2,-5]\\).\n\nSi organizamos los vectores de acuerdo con el valor de los valores propios (de mayor a menor) obtenemos la matriz\n\\[\\left(\\begin{array}{ccc}\n  1 & 2 & 1 \\\\\n  2 & -1 & 2\\\\\n  1 & 0 & -5\\\\\n      \\end{array}\\right)\\]\nAhora vamos a ortonormalizarla con el proceso de Gram-schmidt\n\\[{\\textbf u}_1=\\frac{{\\textbf v}_1}{|{\\textbf v}_1|}=\\frac{[1,2,1]}{\\sqrt{6}}\\,,\\] y calculamos el vector ortonormal como\n\\[{\\textbf w}_2={\\textbf v}_2-{\\textbf u}_1\\cdot{\\textbf v}_2{\\textbf u}_1=[2,-1,0]\\]\nSi lo normalizamos\n\\[{\\textbf u}_2=\\frac{{\\textbf w}_2}{|{\\textbf w}_2|}=\\left[ \\frac{2}{\\sqrt{5}}, \\frac{-1}{\\sqrt{5}} , 0 \\right]\\]\nEl último vector ortonormal a calcular es\n\\[{\\textbf w}_3={\\textbf v}_3-{\\textbf u}_1\\cdot{\\textbf v}_3{\\textbf u}_1 - {\\textbf u}_2\\cdot{\\textbf v}_3{\\textbf u}_2=[\\frac{-2}{3},\\frac{-4}{3},\\frac{10}{3}]\\]\nSi lo normalizamos\n\\[{\\textbf u}_3=\\frac{{\\textbf w}_3}{|{\\textbf w}_3|}=\\left[ \\frac{1}{\\sqrt{30}}, \\frac{2}{\\sqrt{30}} , \\frac{-5}{\\sqrt{30}} \\right]\\]\ndejando la matriz\n\\[{\\textbf V}=\\left(\\begin{array}{ccc}\n  \\frac{1}{\\sqrt{6}} & \\frac{2}{\\sqrt{5}} & \\frac{1}{\\sqrt{30}}\\\\\n  \\frac{2}{\\sqrt{6}} & -\\frac{1}{\\sqrt{5}} & \\frac{2}{\\sqrt{30}}\\\\\n  \\frac{1}{\\sqrt{6}} & 0 & \\frac{-5}{\\sqrt{30}}\n      \\end{array}\\right)\\,.\\]\nFinalmente, lo que realmente queremos es\n\\[{\\textbf V}^T=\\left(\\begin{array}{ccc}\n  \\frac{1}{\\sqrt{6}} & \\frac{2}{\\sqrt{6}} & \\frac{1}{\\sqrt{6}}\\\\\n  \\frac{2}{\\sqrt{5}} & -\\frac{1}{\\sqrt{5}} & 0\\\\\n  \\frac{1}{\\sqrt{30}} & \\frac{2}{\\sqrt{30}} & \\frac{-5}{\\sqrt{30}}\\\\\n      \\end{array}\\right)\\,.\\]\nPara calcular \\({\\textbf S}\\) debemos tomar las raices cuadradas de los valores propios diferentes de cero (\\(\\lambda_i\\ne0\\)) y colocarlos en la diagonal principal en orden descendente. Es decir, el valor propio mayor en la posición \\(s_{11}\\), el siguiente mas grande en \\(s_{22}\\), y así sucesivamente. Los valores propios diferentes de cero son iguales para \\({\\textbf U}\\) y \\({\\textbf V}\\) con lo que no importa de cual los tomemos. Puesto que solo hay dos valores propios diferentes de cero y el orden de las matrices \\({\\textbf U}\\) y \\({\\textbf V}\\) es \\(3\\times3\\), debemos añadir una columna de ceros a \\({\\textbf S}\\)\n\\[{\\textbf S}=\\left(\\begin{array}{ccc}\n  \\sqrt{12} & 0 & 0 \\\\\n  0 & \\sqrt{10} & 0\\\\\n      \\end{array}\\right)\\,.\\]\nAhora ya tenemos todas las matrices de la descomposición en valores singulares:\n\\[{\\textbf A}={\\textbf U}{\\textbf S}{\\textbf V}^T=\\left(\\begin{array}{cc}\n  \\frac{1}{\\sqrt{2}} & \\frac{1}{\\sqrt{2}} \\\\\n  \\frac{1}{\\sqrt{2}} & -\\frac{1}{\\sqrt{2}} \\\\\n      \\end{array}\\right)\n      \\left(\\begin{array}{ccc}\n  \\sqrt{12} & 0 & 0 \\\\\n  0 & \\sqrt{10} & 0\\\\\n      \\end{array}\\right)\n      \\left(\\begin{array}{ccc}\n  \\frac{1}{\\sqrt{6}} & \\frac{2}{\\sqrt{5}} & \\frac{1}{\\sqrt{30}}\\\\\n  \\frac{2}{\\sqrt{6}} & -\\frac{1}{\\sqrt{5}} & \\frac{2}{\\sqrt{30}}\\\\\n  \\frac{1}{\\sqrt{6}} & 0 & \\frac{-5}{\\sqrt{30}}\n      \\end{array}\\right)=\\]\n\\[=\\left(\\begin{array}{ccc}\n  \\frac{12}{\\sqrt{2}} & \\frac{\\sqrt{10}}{\\sqrt{2}} & 0\\\\\n  \\frac{12}{\\sqrt{2}} & -\\frac{\\sqrt{10}}{\\sqrt{2}} & 0\\\\\n      \\end{array}\\right)\n      \\left(\\begin{array}{ccc}\n  \\frac{1}{\\sqrt{6}} & \\frac{2}{\\sqrt{5}} & \\frac{1}{\\sqrt{30}}\\\\\n  \\frac{2}{\\sqrt{6}} & -\\frac{1}{\\sqrt{5}} & \\frac{2}{\\sqrt{30}}\\\\\n  \\frac{1}{\\sqrt{6}} & 0 & \\frac{-5}{\\sqrt{30}}\n      \\end{array}\\right)=\n      \\left(\\begin{array}{ccc}\n  3 & 1 & 1 \\\\\n  -1 & 3 & 1\\\\\n      \\end{array}\\right)\\,.\\]"
  },
  {
    "objectID": "chap4.html#análisis-espectral-o-análisis-de-fourier",
    "href": "chap4.html#análisis-espectral-o-análisis-de-fourier",
    "title": "Notas del Curso Análisis de Datos",
    "section": "Análisis espectral o análisis de Fourier",
    "text": "Análisis espectral o análisis de Fourier\n\nIntroducción\nUna función periódica es aquella cuyos valores se repiten a intervalos regulares. El tiempo entre las sucesivas repeticiones se denomina periodo \\(\\tau\\). Normalmente lo definimos entre sucesivas crestas. Matemáticamente, una función es periódica si \\(f(t)=f(t+T)\\) para todo valor de \\(T\\).\nLa frecuencia de una función periódica se define como el inverso del periodo, \\(f=1/\\tau\\), es decir el número de ciclos por unidad de tiempo (si es por segundo hablamos de Hercios, Hz). Si un ciclo equivale a \\(2\\pi\\) radianes, entonces el número de radianes por segundo es lo que se concoce por la frecuencia angular fundamental: \\[\\omega=\\frac{2\\pi}{T}\\,.\\]\nLas funciones periódicas también se pueden definir en el espacio. Entonces el periodo se define como \\[\\tau=\\lambda/v_p\\,,\\] donde \\(\\lambda\\) es la longitud de onda y \\(v_p=\\lambda/\\tau=\\omega/k\\) es la velocidad de fase. La longitud de onda es una distancia entre estados de la onda que se repiten, e.j. entre dos crestas. El número de onda \\(k\\) es el número de ondas contenidas en una unidad de distancia \\[k=\\frac{2\\pi}{\\lambda}=\\frac{\\omega}{v_p}\\]\nEl valor promedio de una función periódica es: \\[f_m=\\frac{1}{\\tau}\\int\\limits^{\\tau}_0 f(t) dt\\,,\\] y su valor cuadrático medio (o RMS, en inglés) es: \\[f_{rms}=\\sqrt{\\frac{1}{\\tau}\\int\\limits^{\\tau}_0 f^2(t) dt}\\,,\\] donde las integrales se han definido en el intervalo 0,\\(\\tau\\), aunque se pueden definir en cualquier intervalo que abarque un periodo, e.j. de -\\(\\tau\\)/2 a \\(\\tau\\)/2.\nUna de las ondas periódicas mas utilizadas es la sinusoidal o cosenosoidal. \\[f(t)=A sen(\\omega t + \\theta)\\,,\\] siendo \\(A\\) la amplitud y \\(\\theta\\) su fase inicial. En este caso el valor medio es cero y su rms es \\(A/\\sqrt(2)\\). % Recordar que existen dos frecuencias básicas: (i) Frecuencia de Nyquist \\(f_N=1/(2\\Delta t)\\) (la frecuencia mas alta que podemos resolver) y (ii) Frecuencia fundamental \\(f_0=1/(\\Delta t N)=1/T\\) (la frecuencia mas baja que podemos resolver).\n\n\nSerie de Fourier\nEl principio básico del análisis de Fourier es que cualquier función periódica \\(f(t)\\) definida en el intervalo \\([0,T]\\) se puede descomponer en suma de funciones simples, sinusoidales y cosinusoidales, o series de Fourier de la forma \\[f(t)=\\bar{f(t)} + \\sum\\limits_p [A_p cos(\\omega_p t) + B_p sin (\\omega_p t)]\\,,\\] donde \\(\\overline{f(t)}\\) es el valor medio de la serie temporal, \\(A_p\\) y \\(B_p\\) son constantes denominados coeficientes de Fourier, y \\(\\omega_p=2 \\pi p f_0=2\\pi p/\\tau\\) es múltiplo de la frecuencia angular fundamental.\nSi tenemos suficientes componentes de Fourier cada valor de la serie original se puede reconstruir. La contribución que cada componente tiene sobre la varianza de la serie temporal es una medida de la importancia de una frecuencia particular en la serie original. El punto clave aqui es que el conjunto de coeficientes de Fourier con amplitudes \\(A_p\\) y \\(B_p\\) forman un espectro el cual define la contribución de cada componente oscilatoria \\(\\omega_p\\) sobre la `energía’ total de la señal original. En concreto, el spectro de potencia (Power spectrum) define la energía por unidad de banda de frecuencia de una serie temporal. Puesto que debemos definir dos amplitudes \\(A_p\\) y \\(B_p\\), hay dos grados de libertad por estimación espectral.\nComo ya hemos dicho el primer armónico (\\(p=1\\)) oscila con frecuencia fundamental \\(\\omega_1=2\\pi f_1\\). El armónico \\(N/2\\), el cual nos da la componente con la frecuencia más alta que puede ser resuelta tiene frecuencia \\(f_N=N/2/N\\Delta t=1/(2\\Delta T)\\) ciclos por unidad de tiempo y un periodo de \\(2\\Delta t\\). Esta es la frecuencia de Nyquist.\nLas series de Fourier se definen como \\[f(t)=\\frac{1}{2}A_0 +\\sum\\limits^\\infty_{p=1}[A_p cos(\\omega_p t) + B_p sen(\\omega_p t)]\\,,\\] en la cual \\[\\omega_p=2\\pi f_p=2\\pi p f_1=2\\pi p /T;\\,\\,\\,\\,p=1,2,...\\,,\\] es la frecuencia de la componente \\(p\\)ésima en radianes por unidad de tiempo (\\(f_p\\) es en ciclos por unidad de tiempo) y \\(A_0/2\\) es la media de la serie temporal.\nPara obtener los coeficientes \\(A_p\\), debemos multiplicar la expresión de la descomposición de Fourier por \\(cos(\\omega_r t)\\) e integrar sobre la serie completa. \\[\\int\\limits^{T}_{0}f(t)cos(\\omega_r t)dt=\\frac{1}{2}A_0\\int\\limits^{T}_{0} cos(\\omega_r t) dt + \\] \\[+\\sum\\limits^\\infty_{p=1} A_p \\int\\limits^{T}_{0}cos(\\omega_p t)cos(\\omega_r t) dt +\\] \\[+\\sum\\limits^\\infty_{p=1} B_p \\int\\limits^{T}_{0}sin(\\omega_p t)cos(\\omega_r t) dt\\,.\\]\nSi usamos las siguientes condiciones de ortogonalidad: \\[\\int\\limits^{T}_{0}sin(\\omega_p t)cos(\\omega_r t) dt=0\\] \\[\\int\\limits^{T}_{0}cos(\\omega_p t)cos(\\omega_r t) dt=\n\\left\\lbrace\n  \\begin{array}{l}\n     T, p=r=0 \\\\\n     T/2,p=r&gt;0 \\\\\n     0, p\\ne r \\\\\n  \\end{array}\n  \\right.\\] \\[\\int\\limits^{T}_{0}sin(\\omega_p t)sin(\\omega_r t) dt=\n\\left\\lbrace\n  \\begin{array}{l}\n     0, p=r=0 \\\\\n     T/2,p=r&gt;0 \\\\\n     0, p\\ne r \\\\\n  \\end{array}\n  \\right.\\] entonces encontramos que para \\(r=0;p\\ne r\\) la ecuación de arriba se reduce a \\[\\int\\limits^{T}_{0}f(t)dt=\\frac{A_0}{2}T\\,,\\] es decir, \\[A_0=\\frac{2}{T}\\int\\limits^{T}_{0}f(t)dt=2\\overline{f(t)}\\,,\\] dos veces el valor medio de la serie \\(f(t)\\). Es por ello que se añade el factor de \\(1/2\\) en la serie de Fourier. Es decir, para que el primer término de la serie de Fourier sea igual a la media de la serie temporal \\(\\overline{f(t)}=1/2A_0\\).\nCuando \\(p\\ne0\\) el único término no despreciable de la derecha de la expresión de arriba sucede cuando \\(r=p\\) \\[\\int\\limits^{T}_{0}f(t)cos(\\omega_p t)dt=\\frac{A_p}{2}T\\,,\\] y entonces \\[A_p=\\frac{2}{T}\\int\\limits_0^{T} f(t) cos(\\omega_p t) dt,\\,\\,\\,\\,p=1,2,...\\]\nLos otros coeficientes \\(B_p\\) son obtenidos igualmente multiplicando por \\(sen(\\omega_r t)\\) en lugar de \\(cos(\\omega_r t)\\) \\[B_p=\\frac{2}{T}\\int\\limits_0^{T} f(t) sen(\\omega_p t) dt,\\,\\,\\,\\,p=1,2,...\\,.\\]\nTambién podemos representar la serie de Fourier en notación compacta como: \\[f(t)=\\frac{1}{2}C_0 +\\sum\\limits^\\infty_{p=1}C_p cos(\\omega_p t - \\theta_p)\\,,\\] en la cual la amplitud de la \\(p\\)ésima componente es \\[C_p=\\sqrt{A_p^2+B_p^2}\\,,\\,\\,\\,\\,p=1,2,....\\] donde \\(C_0=A_0 (B_0=0)\\) es dos veces el valor promedio de la serie y \\[\\theta_p=arctg[B_p/A_p]\\,,\\,\\,\\,\\,p=1,2,...\\] es el ángulo de fase de la componente al tiempo \\(t=0\\). El ángulo de fase nos informa del “desfase” lag relativo de las componente en radianes (o grados) medido en el sentido contrario a las agujas del reloj desde el eje real definido por \\(B_p=0, A_p&gt;0\\). El correspondiente tiempo de desfase para la componente \\(p\\)ésima es \\(t_p=\\theta_p/2\\pi f_p\\) en el cual \\(\\theta_p\\) esta medida en radianes. La energía espectral se define como las amplitudes de los coeficientes de Fourier al cuadrado, lo cual representa la varianza y entonces la energía \\[C^2_p=A_p^2 + B_p^2\\,.\\] \\ \\ De igual forma con las relaciones trigonométricas de arriba se puede expresar las series de Fourier en notación compleja. Usando \\[sen\\omega_p t=\\frac{e^{i\\omega_pt}-e^{-i\\omega_pt}}{2i}\\,\\,\\,\\,\\,\\,\\,y\\,\\,\\,\\,\\,\\,\\,\ncos\\omega_p t=\\frac{e^{i\\omega_pt}+e^{-i\\omega_pt}}{2}\\,,\\] obtenemos \\[f(x)=\\frac{1}{2}A_0 +\\sum\\limits^\\infty_{p=1}[A_p cos(\\omega_p t) + B_p sen(\\omega_p t)]=\\] \\[=\\frac{1}{2}A_0 +\\sum\\limits^\\infty_{p=1}\\left[ A_p \\frac{e^{i\\omega_pt}+e^{-i\\omega_pt}}{2} + B_p \\frac{e^{i\\omega_pt}-e^{-i\\omega_pt}}{2i}\\right]=\\] \\[=\\frac{1}{2}A_0+\\sum\\limits^\\infty_{p=1}\\left[\\frac{A_p e^{i\\omega_p t}}{2} + \\frac{A_p e^{-i\\omega_p t}}{2} -\n\\frac{iB_p e^{i\\omega_p t}}{2} + \\frac{iB_p e^{-i\\omega_p t}}{2} \\right]=\\] \\[=\\frac{1}{2}A_0+\\sum\\limits^\\infty_{p=1} e^{i\\omega_p t}\\frac{A_p-iB_p}{2} +\n                  \\sum\\limits^\\infty_{p=1} e^{-i\\omega_p t}\\frac{A_p+iB_p}{2}=\\] \\[=C^*_0 + \\sum\\limits^\\infty_{p=1}C^*_pe^{i\\omega_p t} + \\sum\\limits^\\infty_{p=1}C^*_{-p} e^{-i\\omega_p t}=\n\\sum\\limits^\\infty_{p=-\\infty} C^*_p e^{i\\omega_p t}\\,,\\] donde hemos definido las siguientes relaciones entre los coeficientes de Fourier complejos y reales: \\[C^*_0=\\frac{1}{2}A_0\\,,\\] \\[C^*_p=\\frac{1}{2}(A_p-iB_p)\\,,\\] \\[C^*_{-p}=\\frac{1}{2}(A_p+iB_p)\\,.\\]\nEn resumen, podemos reconstruir la serie periódica \\(f(t)\\) con la transformada de Fourier \\[f(x)=\\sum\\limits^\\infty_{p=-\\infty} C^*_p e^{i\\omega_p t}\\,,\\] e inversamente podemos calcular los coeficientes de Fourier \\(C^*_p\\) a partir de la \\(f(t)\\) \\[C^*_p=\\frac{1}{2}(A_p-iB_p)=\\frac{1}{2}\\left(\\frac{2}{T}\\int\\limits_0^{T} f(t) cos(\\omega_p t) dt -\n                                             i \\frac{2}{T}\\int\\limits_0^{T} f(t) sen(\\omega_p t) dt \\right)=\\] \\[=\\frac{1}{T}\\int\\limits_0^{T} f(t) [cos(\\omega_p t) - isen(\\omega_p t) ]dt = \\frac{1}{T}\\int\\limits_0^{T} f(t) e^{-i \\omega_p t} dt\\,,\\] es decir, podemos pasar del espacio temporal \\(f(t)\\) al espacio espectral o de Fourier \\(C^*_p\\) e inversamente regresar al espacio temporal de nuevo.\nEl teorema de Parseval es precisamente el que demuestra que la el valor cuadrático medio de la serie de Fourier es igual al error cuadrático medio de los coeficientes de Fourier. La varianza de la serie de Fourier es \\[\\frac{1}{T}\\int\\limits^{T}_0 f^2(t) dt=\\frac{1}{T}\\int\\limits^{T}_0\n\\left(\\frac{1}{2}A_0 +\\sum\\limits^\\infty_{p=1}[A_p cos(\\omega_p t) + B_p sen(\\omega_p t)]\\right) dt=\\] \\[=\\frac{1}{T}\\frac{1}{4} A^2_0 \\int\\limits^{T}_0 dt + \\frac{1}{T}A_0 \\sum\\limits^\\infty_{p=1} A_p \\int\\limits^{T}_0 cos(\\omega_p t)dt + \\frac{1}{T}A_0\\sum\\limits^\\infty_{p=1}B_p \\int\\limits^{T}_0 sen(\\omega_p t)dt +\\] \\[+\\frac{1}{T}\\sum\\limits^\\infty_{p=1}A^2_p \\int\\limits^{T}_0 cos^2(\\omega_p t)dt +\n\\frac{1}{T}\\sum\\limits^\\infty_{p=1}B^2_p \\int\\limits^{T}_0 sen^2(\\omega_p t)dt+\\] \\[+\\frac{1}{T}2\\sum\\limits^\\infty_{p=1}A_p \\sum\\limits^\\infty_{p=1}B_p\\int\\limits^{T}_0 sen(\\omega_p t)cos(\\omega_p t)dt=\n\\frac{1}{T}\\frac{1}{4}A^2_0 T + \\frac{1}{T}\\sum\\limits^\\infty_{p=1}A^2_p\\left[ \\frac{t}{2} + \\frac{sen(2\\omega_p t)}{4\\omega_p}\\right]^T_0+\\] \\[+\\frac{1}{T}\\sum\\limits^\\infty_{p=1}B^2_p\\left[ \\frac{t}{2} - \\frac{sen(2\\omega_p t)}{4\\omega_p}\\right]^T_0=\n\\frac{1}{4}A^2_0 + \\frac{1}{2}\\sum\\limits^\\infty_{p=1}A_p^2 + \\frac{1}{2}\\sum\\limits^\\infty_{p=1}B_p^2=\\] \\[=\\frac{1}{4}A^2_0 + \\frac{1}{2}\\sum\\limits^\\infty_{p=1} A_p^2 + B_p^2\\,.\\]\nUtilizando las siguientes identidades \\[|C^*_p|^2=|C^*_{-p}|^2=\\frac{1}{4}(A_p^2 + B_p^2)\\,,\\] \\[C^*_0=\\frac{1}{2}A_0\\,,\\] el teorema de Parseval en términos de los coeficientes de Fourier complejos es \\[\\frac{1}{T}\\int\\limits^{T}_0 f^2(t) dt=(C^*_0)^2 + \\frac{1}{2}\\sum\\limits^\\infty_{p=1} 4|C^*_p|^2=\n(C^*_0)^2 + \\sum\\limits^\\infty_{p=1} |C^*_p|^2 + \\sum\\limits^\\infty_{p=1} |C^*_p|^2=\\] \\[=(C^*_0)^2 + \\sum\\limits^\\infty_{p=1} |C^*_p|^2 + \\sum\\limits^\\infty_{p=1} |C^*_{-p}|^2=\n\\sum\\limits^\\infty_{p=-\\infty} |C^*_p|^2\\]\nEsto da lugar a la relación entre la amplitud de las componentes de Fourier en el dominio espectral (de frecuencia) y la varianza de la serie en el dominio temporal.\n\nNOTA: En el análisis de Fourier es importante recalcar que debemos de eliminar la tendencia de la serie antes de calcular los coeficientes. Sino lo hacemos, el análisis de Fourier pondrá erroneamente la varianza de la tendencia en las componentes de baja frecuencia de la expansión de Fourier. En Matlab eso lo podemos hacer con el comando detrend.m o bien simplemente extrayendo el promedio temporal.\n{jemplo de cálculo de coeficientes de Fourier}\nImaginemos la siguiente onda cuadrada representada por la función \\[f(t)=\n\\left\\lbrace\n  \\begin{array}{l}\n     -1,\\text{para} -\\frac{1}{2}T\\le t&lt; 0 \\\\\n     +1,\\text{para} 0\\le t&lt; \\frac{1}{2}T  \\\\\n  \\end{array}\n  \\right.\\]\nPuesto que función de arriba es impar, es decir cumple la condición de simetría \\(-f(t)=f(-t)\\), entonces la serie de Fourier resultante solamente contendrá componentes sinusoidales. Entonces \\[B_p=\\frac{2}{T}\\int\\limits_{-T/2}^{T/2} f(t) sen(\\omega_p t) dt=\\] \\[=\\frac{2}{T}\\int\\limits_{-T/2}^{0} f(t) sen(\\omega_p t) dt + \\frac{2}{T}\\int\\limits_{0}^{T/2} f(t) sen(\\omega_p t) dt=\\] \\[\\frac{2}{T}\\left[ (-1) \\int\\limits_{-T/2}^{0} sen(\\omega_p t) dt +\n                   (1) \\int\\limits_{0}^{T/2} sen(\\omega_p t) dt \\right]=\\]\n\\[=\\frac{2}{T}\\left[ \\left|\\frac{cos(\\omega_p t)}{\\omega_p}\\right|_{-T/2}^{0}\n                     -\\left|\\frac{cos(\\omega_p t)}{\\omega_p}\\right|_{0}^{T/2} \\right]=\\] \\[=\\frac{2}{\\omega_p T}\\left[ 1 - cos(\\omega_p T/2) - cos(\\omega_p T/2) + 1  \\right]=\n\\frac{2}{\\omega_p T}\\left[2 -2cos(\\omega_p T/2)\\right]=\\] \\[=\\frac{4}{\\omega_p T}\\left[1-cos(\\omega_p T/2)\\right]=\n\\frac{2}{\\pi p}\\left[1-cos(\\pi p)\\right]\\] \\ \\ Los coeficientes son cero (\\(B_p=0\\)) si \\(p\\) es par y \\(B_p=4/\\pi p\\) si \\(p\\) es impar. Finalmente nuestra serie de Fourier es \\[f(t)=\\sum\\limits^\\infty_{p=1}B_p sen(\\omega_p t)=\\] \\[=\\sum\\limits^\\infty_{p=1}B_p sen\\left(\\frac{2\\pi p}{T} t\\right)=\nB_1 sen\\left(\\frac{2\\pi 1}{T} t\\right) + B_3 sen\\left(\\frac{2\\pi 3}{T} t\\right) +\\] \\[+B_5 sen\\left(\\frac{2\\pi 5}{T} t\\right) + ... =\\frac{4}{\\pi}sen\\left(\\frac{2\\pi 1}{T} t\\right) +\\] \\[+\\frac{4}{3\\pi}sen\\left(\\frac{2\\pi 3}{T} t\\right) +\\frac{4}{5\\pi}sen\\left(\\frac{2\\pi 5}{T} t\\right)=\\] \\[=\\frac{4}{\\pi}\\left( \\frac{sen{\\omega_1 t}}{1} + \\frac{sen{3\\omega_1 t}}{3} + \\frac{sen{5\\omega_1 t}}{5}\\right)+...\\]\n{Series de Fourier Discretas}\nEn general, vamos a muestrear de forma discreta el océano y consecuentemente las series temporales que obtenemos son discretas en el tiempo. Segun el teorema de Parseval, la varianza de estas series discretas \\[\\sigma^2=\\frac{1}{N-1}\\sum\\limits^N_{t=1}(f(t)-\\overline{f(t)})^2\\] se puede obtener sumando las contribuciones individuales de los armónicos de Fourier. La descomposición de series temporales discretas en armónicos específicos da lugar al concepto de espectro de Fourier. Para encontrar el espectro de Fourier debemos calcular los coeficientes \\(A_p,B_p\\) o, equivalentemente, las amplitudes \\(C_p\\) y el ángulo de fase \\(\\theta_p\\).\nSupongamos la serie de Fourier para un registro finito de longitud par \\(N\\) definido en los tiempos \\(t_1, t_2,....,t_N\\) \\[f(t_n)=\\frac{1}{2}A_0+\\sum\\limits^{N/2}_{p=1}[A_p cos(\\omega_p t_n) + B_p sen(\\omega_p t_n)]\\,,\\] donde ya sabemos \\(\\omega_p=2\\pi f_p=2\\pi p/T\\). Sabiendo que \\(t_n=n \\Delta t\\), esta serie se puede reescribir como \\[f(t_n)=\\frac{1}{2}A_0+\\sum\\limits^{N/2}_{p=1}[A_p cos(2\\pi p n \\Delta t / \\Delta t N) + B_p sen(2\\pi p n \\Delta t / \\Delta t N)]=\\] \\[=\\frac{1}{2}A_0+\\sum\\limits^{N/2}_{p=1}[A_p cos(2\\pi p n  / N) + B_p sen(2\\pi p n / N)]=\n\\frac{1}{2}C_0 + \\sum\\limits^{N/2}_{p=1}[C_p cos[(2\\pi p n  / N) - \\theta_p]\\,,\\] donde los términos \\(A_0/2\\) y \\(C_0/2\\) son los valores medios de toda la serie \\(f(t)\\). Los coeficientes se calculan de igual forma usando las condiciones de ortogonalidad. La única diferencia es que en lugar de tratar con integrales (serie contínua) tratamos con sumatorios (serie discreta) \\[A_p=\\frac{2}{N}\\sum\\limits_{n=1}^{N} f(t_n) cos(2\\pi p n /N),\\,\\,\\,\\,p=1,2,...,N/2\\] \\[A_0=\\frac{2}{N}\\sum\\limits_{n=1}^{N} f(t_n),\\,\\,\\,\\,B_0=0\\] \\[A_{N/2}=\\frac{1}{N}\\sum\\limits_{n=1}^{N} f(t_n)cos(n\\pi),\\,\\,\\,\\,B_{N/2}=0\\] \\[B_p=\\frac{2}{N}\\sum\\limits_{n=1}^{N} f(t_n) sen(2\\pi p n /N),\\,\\,\\,\\,p=1,2,...,N/2\\]\nEl \\(N/2\\) se debe a que es el armónico con mayor frecuencia que podemos resolver, es decir, aquel que oscila con frecuencia de Nyquist. Para \\(p&gt;N/2\\) las funciones trigonométricas simplemente darán coeficientes de Fourier repetidos ya obtenidos en el intervalo \\(1\\le p\\le N/2\\). Para calcular la serie discreta de Fourier, primero debemos calcular los argumentos de las funciones trigonométricas \\(2\\pi n p / N\\) para cada entero \\(p\\) y \\(n\\). Segundo, evaluamos las funciones \\(cos(2\\pi n p / N)\\) y \\(sen(2\\pi n p / N)\\), y sumamos para los términos \\(f(t_n)cos(2\\pi n p / N)\\) y \\(f(t_n)sen(2\\pi n p / N)\\). Por último, incrementamos \\(p\\) y repetimos los dos pasos anteriores.\n{jemplo de Series Temporales Discretas (modificado de Emery and Thompson, p387)}\nConsidera la serie temporal de temperatura promedia mensual por un periodo de tres años (ver tabla y Figura).\n\n\n\nUtilizando las expresiones de arriba podemos calcular las frecuencias \\(f_p\\), amplitudes \\(A_p\\), \\(B_p\\), \\(C_p\\), las fases \\(\\theta_p\\) y finalmente la serie de Fourier \\(f(t)\\). Los valores para las primeras 8 componentes estan reflejados en la tabla\n\n{Serie de Fourier para variables vectoriales (complejas)}\nEn este caso la transformada de Fourier se aplica a una cantidad vectorial en lugar de una cantidad escalar como temperatura, salinidad, densidad, etc. Supongamos que tenemos las dos componentes de la velocidad \\(u\\) y \\(v\\) las cuales expandemos en series de Fourier \\[u(t)=\\frac{1}{2}A_0+\\sum\\limits^{N/2}_{p=1}[A_p cos(\\omega_p t_n) + B_p sen(\\omega_p t_n)]\\] \\[v(t)=\\frac{1}{2}C_0+\\sum\\limits^{N/2}_{p=1}[C_p cos(\\omega_p t_n) + D_p sen(\\omega_p t_n)]\\,,\\] lo cual se puede escribir en versión compleja como \\[R(t)=u(t)+i v(t)=\\frac{1}{2}A_0+\\sum\\limits^{N/2}_{p=1}[A_p cos(\\omega_p t_n) + B_p sen(\\omega_p t_n)]\n+\\] \\[+ i \\left(\\frac{1}{2}C_0+\\sum\\limits^{N/2}_{p=1}[C_p cos(\\omega_p t_n) + D_p sen(\\omega_p t_n)]\\right)=\\] \\[=\\left[\\frac{1}{2}A_0 + i \\frac{1}{2}C_0\\right] + \\sum\\limits^{N/2}_{p=1} \\left[ (A_p + iC_p) cos(\\omega_p t_n) + (B_p + i D_p) sen(\\omega_p t_n) \\right]\\,,\\] donde \\(\\frac{1}{2}A_0 + i \\frac{1}{2}C_0=\\overline{u(t)} +i \\overline{v(t)}\\) es la velocidad media, \\(\\omega_p=2\\pi f_p=2\\pi p/N\\Delta t\\) la frecuencia angular, \\(t_n=n\\Delta t\\) es el eje de tiempo, y \\((A_p\\,,B_p\\,,C_p\\,,D_p)\\) y son las amplitudes y fases de cada componente de Fourier, tanto las reales como las imaginarias. A diferencia de la serie de Fourier real en este caso las componentes van de \\(p=1\\) hasta \\(p=N\\) y, por lo tanto, estamos cubriendo ambas frecuencias positivas y negativas.\nSi extraemos la velocidad media\n\\[R'(t)=R(t)-[\\overline{u(t)} +i \\overline{v(t)}]=\\sum\\limits^{N/2}_{p=1} \\left[ (A_p + iC_p) cos(\\omega_p t_n) + (B_p + i D_p) sen(\\omega_p t_n) \\right]\\,.\\]\nAhora vamos a escribir la anomalía de la serie compleja \\(R'(t)\\) en términos de dos componentes rotatorias ortogonales, es decir, una componente que gira en el sentido de las agujas del reloj con amplitud \\(R^{-}\\) y otra que gira en el sentido opuesto a las agujas del reloj y amplitud \\(R^{+}\\) \\[R'(t)=\\sum\\limits^{N/2}_{p=1} \\left[ e^{i\\omega_p t} + e^{-i\\omega_p t}\\right]=\\] \\[    =\\sum\\limits^{N/2}_{p=1}  R_p^{+}\\left[cos\\left(\\omega_p t_n\\right) + i sen\\left(\\omega_p t_n\\right )\\right] +\n       \\sum\\limits^{N/2}_{p=1}  R_p^{-}\\left[cos\\left(\\omega_p t_n\\right) - i sen\\left(\\omega_p t_n\\right))\\right]=\\] \\[=\\sum\\limits^{N/2}_{p=1}\\left[ (R_p^+ + R_p^{-})cos\\left(\\omega_p t_n\\right) + (R_p^+ - R_p^{-})i sen\\left(\\omega_p t_n\\right) \\right]\\]\nNote que \\(e^{i\\omega_p t}=cos\\left(\\omega_p t_n\\right) + i sen\\left(\\omega_p t_n\\right)\\) rota en el sentido contrario a las agujas del reloj y \\(e^{-i\\omega_p t}=cos\\left(\\omega_p t_n\\right) - i sen\\left(\\omega_p t_n\\right)\\) rota en el sentido de las agujas del reloj. Si comparamos las dos expresiones obtenemos las siguientes identidades \\[A_{p} + i C_{p}=R_p^+ + R_p^{-}\\] \\[B_{p} + i D_{p}=(R_p^+ - R_p^{-})i\\] y de ahí obtenemos que \\[R_p^+=\\frac{1}{2}\\left[ A_{p} + D_{p} +i(C_{p}-B_{p})\\right]\\] \\[R_p^-=\\frac{1}{2}\\left[ A_{p} - D_{p} +i(C_{p}+B_{p})\\right]\\,,\\] y las magnitudes de las componentes rotatorias es \\[|R_p^+|=\\frac{1}{2}\\left[ (A_{p} + D_{p})^2 +(C_{p}-B_{p})^2\\right]^{1/2}\\] \\[|R_p^-|=\\frac{1}{2}\\left[ (A_{p} - D_{p})^2 +(C_{p}+B_{p})^2\\right]^{1/2}\\,,\\] y las fases de las componentes rotatorias son \\[\\epsilon_p^+=actan\\left(\\frac{C_{p}-B_{p}}{A_{p} + D_{p}}\\right)\\,,\\] \\[\\epsilon_p^-=actan\\left(\\frac{C_{p}+B_{p}}{A_{p} - D_{p}}\\right)\\,.\\]\nLa rotación de las componentes clockwise y anticlockwise dibujan una elipse en el plano \\(u\\) vs \\(v\\). Puesto que ambas componentes rotan en sentido contrario pero con la misma frecuencia, habrán momentos que ambás apuntaran en la misma dirección (aditivas), otras ocasiones en dirección opuesta (cancelativas). Esos tiempos de adición y cancelación definen el eje mayor de la elipse \\(L_E=R_p^+ + R_p^-\\) y el eje menor de la elipse \\(L_e=R_p^+ - R_p^-\\). La orientación (inclinación) y la fase de estas elipses a \\(t=0\\) es \\[\\theta_e=\\frac{1}{2}(\\epsilon_p^+ + \\epsilon_p^-)\\,,\\] \\[\\phi_e=\\frac{1}{2}(\\epsilon_p^+ - \\epsilon_p^-)\\,.\\]\nUna propiedad interesante es el coeficiente rotatorio \\[r(\\omega)=\\frac{R^+_p - R^-_p}{R^+_p + R^-_p}\\,,\\] que toma valores entre 0 y 1. Para \\(r=-11\\) tenemos movimiento en el sentido de las agujas del reloj, para \\(r=0\\) tenemos un flujo unidireccional, y para \\(r=+1\\) tenemos movimiento en el sentido contrario de las agujas del reloj.\n{Transformada Rápida de Fourier y espectros de potencia}\nLa FFT (por sus siglas en inglés) es un algoritmo para calcular la serie de Fourier discreta de forma mas eficiente computacionalmente hablando. En este caso la FFT se debería aplicar a series temporales con longitudes múltiples de \\(2\\). En caso contrario, es útil rellenar de ceros nuestra serie para obtener longitudes múltiplos de \\(2\\). A eso se le llama `padding’. Básicamente, el algoritmo obtiene los coeficientes de la serie discreta de Fourier\n\\[f(t)=\\sum\\limits^\\infty_{p=-\\infty} C^*_p e^{i\\omega_p t_n}\\,\\,\\,\\,\\,\\,\\,;\\,\\,\\,\\,\\,\\,\\,\\omega_p=2 \\pi p f_0=2\\pi p/T\\,,\\] \\[F(p)=C^*_p=\\frac{1}{2}(A_p-iB_p)=\\frac{1}{N} \\sum\\limits^{N-1}_{n=0} f(t) e^{-i \\omega_p t_n} =\n        \\frac{1}{N} \\sum\\limits^{N-1}_{n=0}f(t) [cos(\\omega_p t_n) - isen(\\omega_p t_n) ]\\,.\\]\nLa parte real de la FFT me da las amplitudes \\(A_p\\) y la parte imaginaria me da las amplitudes \\(-B_p\\) \\[Re [F(p)]=A_p=\\frac{2}{N}\\sum\\limits_{n=0}^{N-1} f(t_n) cos(2\\pi p n /N),\\,\\,\\,\\,p=0,1,...,N/2\\] \\[Im [F(p)]=-B_p=-\\frac{2}{N}\\sum\\limits_{n=0}^{N-1} f(t_n) sen(2\\pi p n /N),\\,\\,\\,\\,p=0,1,...,N/2\\]\nEn general, debemos de normalizar las amplitudes \\(A_p\\) y \\(B_p\\) por la longitud del registro \\(N\\). Así que en Matlab la amplitud de la FFT es \\[{ abs}({ fft}(f(t)))/N\\,,\\] y la potencia de la FFT es \\[{ abs}({ fft}(f(t)).^2/N^2\\,.\\]\nLa FFT ha descompuesto una señal de \\(N\\) elementos, \\(f(t)\\), en un conjunto de \\(N/2 +1\\) ondas cosinusoidales y \\(N/2 + 1\\) ondas sinusoidales, con las frecuencias definidas por el índice \\(p=0,1,...,N/2\\), i.e. $_p= 2f_p = 2p / T = 2p / N t $. Las amplitudes de los cosenos estan contenidas en \\(Re [F(p)]\\) y las amplitudes de los senos en \\(Im [F(p)]\\). Note que las frecuencias son siempre positivas, es decir, los índices \\(k\\) siempre van de cero a \\(N/2\\). Las frecuencias entre \\(N/2\\) y \\(N-1\\) son negativas. Recuerda que el espectro frecuencial de una señal discreta es periódico, y entonces las frecuencias son negativas entre \\(N/2\\) y \\(N-1\\) al igual que en el intervalo \\(-N/2\\) y \\(-1\\). Los puntos \\(0\\) y \\(N/2\\) separan las frecuencias negativas de las positivas. Es por ello que, generalmente, solamente centramos nuestra atención en la parte positiva del espectro. La magnitud (o norma) de la transformada de Fourier discreta es\n\\[{ Magnitud}=|F(p)|=\\sqrt{Re [F(p)]^2 + Im[F(p)]^2}\\,,\\]\ny la fase es\n\\[Phase=tan^{-1}\\left( \\frac{Im[F(p)]}{Re [F(p)]} \\right)\\,.\\]\nLa FFT organiza los coeficientes de Fourier (imaginarios y reales) en frecuencias negativas y positivas y reparte la varianza de la señal equitativamente entre ellas. En \\(p=0\\) tenemos la media de la serie temporal, aunque debido a que hemos eliminado la media y la tendencia de la serie temporal no debemos de preocuparnos por ella. Entre \\(p=1,...,N/2\\) tenemos los valores de los coeficientes de Fourier reales y entre \\(p=N/2+1,...,N-1\\) tenemos los complejos conjugados de los primeros \\(N/2\\) coeficientes. Si calculamos el valor absoluto de la transformada de Fourier (en Matlab \\({ abs}({ fft}(f(t)))/N\\)) estamos calculando \\(A_p^2 + B_p^2\\) y si solo nos quedamos con los primeros \\(N/2\\) elementos de la FFT, debemos de multiplicar por un factor de \\(2\\) para conservar la energía espectral.\n\nEstimaciones espectrales o autoespectros\n\nEspectro de amplitud\n\nLa gráfica de la magnitud de los coeficientes complejos \\(|C_p^*|\\) de la serie de Fourier \\[f(t)=\\sum\\limits^\\infty_{p=-\\infty} C^*_p e^{i\\omega_p t_n}\\] frente a (versus) la frecuencia \\(\\omega_p\\) se denomina espectro de amplitud de la función periódica \\(f(t)\\). En Matlab (para las primeras \\(N/2\\) componentes), \\[{ Amplitud}=2*|C^*_p|=abs({ fft}(f(t)))/N\\,.\\]\n\nEspectro de densidad de potencia (Power Spectral Density, PSD)\n\nEl espectro de densidad de potencia (PSD, por sus siglas en inglés) es la potencia de la FFT por unidad de frecuencia \\[{\\it PSD}(p)=2*|C^*_p|^2/\\Delta f\\] donde \\(\\Delta f=1/N\\Delta t\\) es la frecuencia fundamental.\nLa gráfica de \\({ PSD}(p)\\) de la frente a (versus) la frecuencia \\(\\omega_p\\) se denomina espectro de densidad de potencia de la función periódica \\(f(t)\\). Si solo nos quedamos con las \\(N/2\\) primeras componentes en Matlab se escribe \\[{\\it PSD}(p)=2*abs({ fft}(f(t)))^2/N^2/\\Delta f\\,.\\]\nEsta normalización tiene su fundamento en el cumplimiento del teorema de Parseval, de tal forma que la energía total de la señal en el dominio temporal \\(f(t)\\) (por unidad de tiempo) sea igual a la energía total de la señal en el dominio frecuencial definido por \\(C^*_p\\): \\[\\frac{1}{T}\\sum\\limits^N_{n=1}|f(t_n)|^2 \\Delta t =\\frac{1}{N}\\sum\\limits^N_{n=1}|f(t_n)|^2 =\nvar(f(t))=\\sum\\limits^{\\infty}_{p=-\\infty} |C^*_p|^2=\\sum\\limits^{N/2}_{p=0}PSD(p)*\\Delta f\\,.\\]\n\nEste teorema de conservacón de energía nos informa de que la integral bajo la curva espectral \\({ PSD}(p)\\) debe ser igual a la varianza total de la serie temporal.\nEfectos de los extremos en estimaciones espectrales\nEn general, para calcular un espectro promedio debemos fragmentar nuestra serie temporal en bloques de igual tamaño que contengan las frecuencias de interés, realizar espectros individuales de dichos fragmentos, y promediar todos ellos. Este método se le conoce como Welch. Los fragmentos pueden ser únicos, es decir, sin superposición o bien pueden ser recursivos, es decir, cuando utilizamos superposición de fragmentos. Por ejemplo, una superposición del 50% significa que cada fragmento empieza en la mitad del fragmento anterior. Además de eliminar el ruido, el método de Welch también reduce la transferencia de energía de las frecuencias pico hacia frecuencias colindantes (leakage' en inglés). El método de Welch reduce el ruido causado por el uso de datos imperfectos y por el efectoleakage’. Aqui les muestro un ejemplo de como promediar un espectro de densidad espectral con fregmentos de tamaño \\(M\\):\nEste problema de transferencia de energía o `leakage’ es intrínseco al problema de que las series temporales oceanográficas son finitas y, por lo tanto, no son necesariamenteperiódicas, condición necesaria en el análisis de Fourier. Veamos esto con un ejemplo de una onda cosinusoidal periódica y no-periódica.\nPodemos observar como en el caso de la serie no-periódica existe una transferencia de energía del pico espectral hacia las frecuencias colindantes de forma que se reduce la amplitud del pico de interés.\nCorrelación\ny \\(s_y\\) son las varianzas de las variables \\(x\\) e \\(y\\), respectivamente. \\end{framed}\nKundu (1976b) define la función de correlación cruzada desfasada entre dos series de velocidad en las profundidades 1 y 2 como \\[\\rho_{\\tau}=\\frac{\\overline{u_1'(t)u_2'(t-\\tau)}}\n{\\left[ \\overline{u_1'(t)^2}\\,\\, \\overline{u_2'(t)^2} \\right]^{1/2}}\\,,\\] donde las primas \\('\\) indican anomalias. Esta función fue utilizada para estudiar la propagación vertical de ondas inercio-gravitatorias y la velocidad de fase de estas ondas \\(c=\\Delta_{12}/\\tau\\). Kundu (1976a) introduce el coeficiente de correlación complejo\n\\[\\rho=\\frac{\\overline{w_1^*(t)w_2(t)}}\n{\\left[ \\overline{w_1^*(t)w_1^*(t)}\\,\\, \\overline{w_2^*(t)w_2(t)} \\right]^{1/2}}\\,,\\]\ndonde \\(w=u+i v\\), los asteriscos \\(*\\) indican complejo conjugados, y los subíndices \\(1\\) y \\(2\\) se refieren a dos estaciones de medida. La cantidad \\(\\rho\\) es un número complejo cuya magnitud (\\(\\le1\\)) nos da una medida de correlación promedia y cuyo ángulo de fase da el angulo promedio, medido en el sentido contrario a las agujas del reloj, del segundo vector con respecto del primero. Por ejemplo, un ángulo de fase negativo entre las profundidades \\(50\\) y \\(100\\,{ m}\\) implica que la señal llega primero a \\(z=-100\\,{ m}\\) y luego a \\(z=-50\\,{ m}\\), es decir, podría tratarse de una onda interna cuyas fases se propagan hacia arriba.\n\n\nEspectro cruzado\nCon análisis de espectros cruzados pretendemos comprender la relación entre dos series temporales en función de la frecuencia. Por ejemplo, observamos en dos localizaciones espectros con picos en las mismas frecuencias y queremos saber si dichos armónicos estan relacionados.\nSupongamos dos series de Fourier \\(x(t)\\) e \\(y(t)\\) \\[x(t)=\\bar{x}+\\sum\\limits^{N/2}_{p=1} A_{xk} cos(\\omega_p t_n) + B_{xp}sen(\\omega_p t_n)\\,,\\] \\[y(t)=\\bar{y}+\\sum\\limits^{N/2}_{p=1} A_{yk}cos(\\omega_p t_n) + B_{yp}sen(\\omega_p t_n)\\,.\\] Utilizando las condiciones ortogonalidad entre las funciones sinusoidales y cosinusoidal, la covarianza entre las variables \\(x\\) e \\(y\\) es \\[\\overline{x'y'}=\\sum\\limits^{N/2}_{p=1} \\frac{1}{2}( A_{xp}A_{yp}  + B_{xp}B_{yp})=\\sum\\limits^{N/2}_{p=1}Co(p)\\,,\\] donde \\(Co(p)\\) es el co-espectro de \\(x\\) e \\(y\\).\nSupongamos dos series de Fourier \\(x(t)\\) e \\(y(t)\\) definidas en la forma compleja (el asterisco ha sido eliminado en esta notación) \\[x(t)=\\bar{x}+\\sum\\limits^{N/2}_{p=1}\n  C_{xp} e^{i\\omega_p t_n}=\\bar{x}+\n  \\sum\\limits^{N/2}_{p=1}\\frac{1}{2}\\left(A_{xp} - iB_{xp} \\right)e^{i\\omega_p t_n}=\\bar{x}+\n  \\sum\\limits^{N/2}_{p=1}F_x(p)\\] \\[y(t)=\\bar{y}+\\sum\\limits^{N/2}_{p=1}\n  C_{yk} e^{i\\omega_p t_n}=\\bar{y}+\n  \\sum\\limits^{N/2}_{p=1}\\frac{1}{2}\\left(A_{yp} - iB_{yp} \\right)e^{i\\omega_p t_n}=\\bar{y}+\n  \\sum\\limits^{N/2}_{p=1}F_y(p)\\,.\\]\nSi ahora calculamos las varianzas \\[\\overline{x'^2}=\\sum\\limits^{N/2}_{p=-N/2}F_{xx}(p)\\,,\\]\ndonde \\[F_{xx}(p)=2\\frac{1}{2}\\left(A_{xp} - iB_{xp} \\right)e^{i\\omega_p t_n}\\frac{1}{2}\\left(A_{xp} + iB_{xp} \\right)e^{-i\\omega_p t_n}\n=2F_x(p) F^*_x(p)=|C_{xp}|^2\\,,\\]\ny el asterisco indica complejo conjugado. Para la variable \\(y\\) de igual forma obtenemos:\n\\[\\overline{y'^2}=\\sum\\limits^{N/2}_{p=-N/2}F_{yy}\\,;\\,\\,\\,\\,\\,F_{yy}(p)=2F_y(p) F^*_y(p)=|C_{yp}|^2\\]\nDe las expresiones anteriores se deduce que covarianza se puede calcular en el espacio espectral como\n\\[\\overline{x'y'}=Re\\left[\\sum\\limits^{N/2}_{p=-N/2}F_{xy}(p)\\right]\\,,\\] donde \\[F_{xy}(p)=2\\frac{1}{2}\\left(A_{xp} - iB_{xp} \\right)e^{i\\omega_p t_n}\\frac{1}{2}\\left(A_{yp} + iB_{yp} \\right)e^{-i\\omega_p t_n}\n=2F_x(p) F^*_y(p)=|C_{xp}||C_{yp}|e^{i(\\theta_{xp}-\\theta_{yp})}\\,.\\]\nEl factor \\(e^{i(\\theta_{xp}-\\theta_{yp})}\\) aparece para considerar que ambas series periódicas no estan en fase.\n\nEspectro cruzado complejo\n\nSi escribimos \\(F_{xy}(p)\\) en términos de los coeficientes de Fourier reales \\[F_x(p) F^*_y(p)=\\frac{1}{2}\\left(A_{xp} - iB_{xp} \\right)e^{i\\omega_p t_n}\\frac{1}{2}\\left(A_{yp} + iB_{yp} \\right)e^{-i\\omega_p t_n}=\\] \\[=\\frac{1}{4}\\left[A_{xk}A_{yk}  + B_{xk}B_{yp} + i\\left(A_{xp}B_{yp} - A_{yp}B_{xp}\\right)\\right]\\,.\\] Para el caso de series \\(x(t)\\) e \\(y(t)\\) reales sabemos que las frecuencias negativas son los complejos conjugados de las frecuencias positivas y entonces \\[A_k=A_{-k}\\,\\,\\,\\,\\,{ y}\\,\\,\\,\\,\\,B_k=B_{-k}\\,,\\] y \\[F_x(p)F^*_y(p)=F_x(-p)F_y^*(-p)\\,,\\] y como conclusión\n\\[F_{xy}(p)+F_{xy}(-p)=\\frac{1}{2}\\left[ A_{xp}A_{yp}  + B_{xp}B_{yp} + i\\left(A_{xp}B_{yp} - A_{yp}B_{xp}\\right)\\right]\\,,\\]\nque es espectro cruzado de \\(x\\) e \\(y\\) para el armónico \\(p\\). De esta expresión encontramos que\n\\[F_{xy}(p)+F_{xy}(-p)=2F_{xy}(p)=Co(p) + i Q(p)\\,,\\] donde \\(Co(p)=\\frac{1}{2}( A_{xp}A_{yp} + B_{xp}B_{yp})\\) es el co-espectro del armónico p\ny \\(Q(p)=\\frac{1}{2}(A_{xp}B_{yp} - A_{yp}B_{xp})\\) es el espectro de cuadratura del armónico \\(k\\).\nEn notación compleja el espectro cruzado \\[F_{xy}(p)=C_{xp}C_{yp}e^{i(\\theta_{xp}-\\theta_{yp})}=C_{xp}C_{yp}\\left(cos(\\theta_{xp}-\\theta_{yp})+ isen(\\theta_{xp}-\\theta_{yp}) \\right)\\,.\\]\n\\[\\theta_{xp}=\\theta_{yp}\\,\\,\\,\\,\\,\\text{entonces}\\,\\,\\,\\,F_{xy}(p) \\text{es real}\\] \\[\\theta_{xp}\\ne\\theta_{yp}=\\pm\\frac{\\pi}{2}\\,\\,\\,\\,\\,\\text{entonces}\\,\\,\\,\\,F_{xy}(p) \\text{es complejo}\\]\nEntonces el co-espectro (la parte real del espectro cruzado) esta en fase con la señal y el espectro de cuadratura esta totalmente desfasado.\n\nEspectro de coherencia\n\nPara una única componente \\(p\\), el espectro de coherencia al cuadrado entre dos series \\(x\\) e \\(y\\) se define\n\\[Coh^2(p)=\\frac{|F_{xy}(p)|^2}{F_{xx}F_{yy}}=\\frac{|C_{xp}C_{yp}|^2}{C_{xp}^2C_{yp}^2}\\,,\\]\ndonde \\(|Coh^2(p)|^{1/2}\\) es su magnitud y \\(\\phi_{xy}(p)\\) es el ángulo de desfase entre las dos componentes \\(p\\) de \\(x\\) e \\(y\\).\nEl espectro de coherencia al cuadrado nos indica el grado de correlación existente entre dos señales. Dos señales estan altamente correlacionadas si la magnitud del espectro de coherencia al cuadrado es \\(\\simeq 1\\) y su fase es \\(\\phi_{xy}(p)\\simeq 0\\)."
  },
  {
    "objectID": "chap4.html#métodos-de-filtrado-y-suavizado",
    "href": "chap4.html#métodos-de-filtrado-y-suavizado",
    "title": "Notas del Curso Análisis de Datos",
    "section": "Métodos de filtrado y suavizado",
    "text": "Métodos de filtrado y suavizado\n\nConvolución y funciones respuesta (ventanas espectrales)\nLa convolución de dos funciones \\(f(t)\\) y \\(g(t)\\) sobre un registro finito \\([0,T]\\) se define como\n\\[[f*g](t)=\\frac{1}{T}\\int\\limits^{T}_{0}f(\\tau)g(t-\\tau) d\\tau\\,.\\]\nO también se puede expresar sobre un registro infinito como\n\\[[f*g](t)=\\int\\limits^{\\infty}_{-\\infty}f(\\tau)g(t-\\tau) d\\tau = \\int\\limits^{\\infty}_{-\\infty}g(\\tau)f(t-\\tau) d\\tau\\,.\\]\nLa convolución satisface las siguientes propiedades\n\n\\[f*g=g*f\\]\n\\[f*(g*h)=(f*g)*h\\]\n\\[f*(g+h)=(f*g)+(f*h)\\]\n\nAhora retomemos las definiciones de serie de Fourier y la transformada de Fourier:\nVamos ahora a deducir el teorema de la convolución. Para ello vamos a partir de la definición de convolución:\n\\[f*g=\\frac{1}{T}\\int\\limits^{T}_{0}f(\\tau)g(t-\\tau) d\\tau=\n      \\frac{1}{T}\\int\\limits^{T}_{0}f(\\tau)\\sum\\limits^{\\infty}_{p=-\\infty} G(\\omega_p) e^{i\\omega_p (t-\\tau)} d\\tau=\\]\n\\[=\\sum\\limits^{\\infty}_{p=-\\infty} G(\\omega_p) \\left[\\frac{1}{T}\\int\\limits^{T}_{0}f(\\tau)e^{-i\\omega_p \\tau} d\\tau \\right]\ne^{i\\omega_p t}=\\sum\\limits^{\\infty}_{p=-\\infty} G(\\omega_p)F(\\omega_p)\ne^{i\\omega_p t}={\\cal F}^{-1}[G(\\omega)F(\\omega)](t)\\,.\\]\nSi aplicamos transformada de Fourier a ambos lados del igual obtenemos:\n\\[{\\cal F}(f*g)=G(\\omega_p)F(\\omega_p)={\\cal F}[g(t)]{\\cal F}[f(t)]\\,,\\]\nes decir, la transformada de Fourier de la convolución de \\(f\\) y \\(g\\) es equivalente a multiplicar en el espacio espectral las transformadas de Fourier de las funciones individuales. La correlación cruzada desfasada de \\(f(t)\\) y \\(g(t)\\) en forma integral se puede definir como\n\\[C_{fg}(\\tau)=\\frac{1}{T}\\int\\limits^{T}_{0} f(\\tau) g(t+\\tau) d\\tau={\\cal F}^{-1}[G(\\omega)F(-\\omega)](t)=\n{\\cal F}^{-1}[G(\\omega)F^*(\\omega)](t)\\,,\\]\nes decir, si multiplicamos la transformada de Fourier de una función por el complejo conjugado de la transformada de Fourier de otra función es equivalente a la transformada de Foruier de la correlación cruzada desfasada entre ellas. Este se le conoce por el teorema de correlación. Para el caso particular que sea la misma función \\(g(t)\\) la que se correlaciona, entonces:\n\\[{\\cal F}[C_{gg}(\\tau)]=G(\\omega)G^*(\\omega)=|G(\\omega)|^2\\,,\\]\nes decir, la transformada de Fourier de la autocorrelación es igual al espectro de potencia de la función \\(g(t)\\). Este se denomina el teorema de Weiner-Khinchin.\nEl concepto de convolución es útil en el filtrado de señales periódicas. En general vamos a convolucionar nuestra señal \\(f(t)\\) con la denominada función respuesta \\(r(t)\\). La función \\(r(t)\\) es típicamente una función pico que cae a cero en ambas direcciones desde el máximo (o pico).\nPuesto que la función respuesta es mas ancha que algunas estructuras de pequeña escala de nuestra señal original, estas serán suavizadas tras realizar la convolución. \\ \\ {OTA:} Por el teorema de convolución filtrar en el dominio temporal convolucionando es equivalente a multiplicar la transformada de Fourier de la señal con la transformada de Fourier de la función respuesta.\nLas ventanas mas comunes para suavizar señales son las de: \n\n`Boxcar’ \\[\\begin{equation*}\n  r(t)=\n  \\left\\lbrace\n  \\begin{array}{l}\n1 \\text{ if } 0\\le t\\le T \\\\\n0 \\text{ if } t&gt;T \\\\\n  \\end{array}\n  \\right.\n\\end{equation*}\\]\n\nLa transformadad de Fourier es la función sinc\n\\[R(\\omega)={\\it sinc}=\\frac{sin\\left(\\frac{\\omega T}{2}\\right)}{\\frac{\\omega T}{2}}\\,.\\]\nEsta función respuesta tiende a cero cuando \\(\\omega T / 2\\) se acerca a cero, es decir, para \\(\\omega T=2 n \\pi;\\,\\,\\text{para}\\,\\,n=1,2,3,...\\). Esta no es una ventana o función respuesta recomendable debido a los lóbulos de menor amplitud alrededor del pico. En general respuestas tipo ondas sinusoidales o cosinusoidales amortiguadas a ambos lados del pico no son deseables.\n\nHanning\n\n\\[\\begin{equation*}\n  r(t)=\\left\\lbrace\n  \\begin{array}{l}\n     \\frac{1}{2}\\left( 1-cos\\frac{2\\pi t}{T}\\right)\n      \\text{ if } -T/2 \\le t \\le T/2 \\\\\n     0 \\text{ if } { contrario} \\\\\n  \\end{array}\n  \\right.\n  \\end{equation*}\\]   (3) Hamming\n\\[\\begin{equation*}\n  r(t)=\\left\\lbrace\n  \\begin{array}{l}\n     \\left(0.54  + 0.46 cos(\\frac{\\pi t}{T}) \\right) \\text{ if } -T/2 \\le t \\le T/2 \\\\\n     0 \\text{ if } { contrario} \\\\\n  \\end{array}\n  \\right.\n  \\end{equation*}\\]\nEs evidente que este tipo de filtro es un suavizado o filtro pasa bajo. Sin embargo, siempre podemos recuperar facilmente la señal de alta frecuencia (filtro pasa altas) restando a la señal original la serie suavizada con convolución.\nImaginemos que tenemos una serie temporal \\(u(t)\\) con un paso temporal de \\(dt=1\\,{ h}\\). Entonces, para suavizar \\(u(t)\\) de tal forma que se eliminen las señales con periodos menores de \\(T=48\\,{ h}\\), es decir, un filtro pasa baja con frecuencia de corte \\(1/48\\,{ h}^{-1}\\) debemos convolucionar \\(u(t)\\) con una función de resouesta \\(r(t)\\) o ventana. En este ejemplo Matlab se muestra como programar un suavizado\nYa hemos visto que podemos suavizar una señal simplemente con la convolución en el dominio temporal de la señal con una ventana o función respuesta (Boxcar, Hanning, Hamming,etc.). Suavizar una señal es comparable a un filtro de pasa baja, es decir, un filtro que solamente deja pasar las frecuencias bajas y elimina (pone a cero) las altas frecuencias. De forma ideal los filtros en el dominio frecuencial los representamos como: \\\n\nPasa baja (filtra las altas frecuencias), \\[\\begin{equation*}\n|R(\\omega)|=\n  \\left\n  \\lbrace\n  \\begin{array}{l}\n1 \\text{ si } |\\omega| \\le \\omega_c \\\\\n0 \\text{ si } \\omega_c \\le \\omega   \\\\\n  \\end{array}\n  \\right.\n  \\end{equation*}\\] \\\nPasa banda (filtra las frecuencias fuera de la banda) \\[\\begin{equation*}\n|R(\\omega)|=\\left\\lbrace\n  \\begin{array}{l}\n1 \\text{ si } \\omega_{c1}\\le|\\omega|\\le \\omega_{c2} \\\\\n0 \\text{ si } \\text({ lo}\\,\\,\\,{ contrario}) \\\\\n  \\end{array}\n  \\right.\n  \\end{equation*}\\] \\\nPasa alta (filtra las bajas frecuencias) \\[\\begin{equation*}\n|R(\\omega)|=\\left\\lbrace\n  \\begin{array}{l}\n0 \\text{ si } |\\omega|\\le \\omega_c \\\\\n1 \\text{ si } \\omega_c\\le\\omega \\\\\n  \\end{array}\n  \\right.\n  \\end{equation*}\\]\n\n\n\nPromedio corrido\nVeamos primero un ejemplo sencillo de filtrado paso bajo con un promedio corrido de dos puntos. Este sería el caso de filtrar utilizando la función smooth.m de Matlab. Para dos puntos sería \\(&gt;&gt; f_{suavizada}=smooth(f,2);\\). \\ Promedio corrido con dos puntos es simplemente el valor promedio \\[y(n)=\\frac{s(n) + s(n-1)}{2}\\,,\\] donde \\(s(n)\\) es una señal periódica, \\(s(-1)=s(N)\\), \\(N\\ge 2\\). La función \\(y(n)\\) es una versión suavizada con altas frecuencias eliminadas y bajas frecuencias mantenidas. Para ver esto definimos \\(s(n)=sen(2\\pi f n/N)\\), y entonces \\[y(n)=\\frac{1}{2} s(n) + \\frac{1}{2} s(n-1)=\\frac{1}{2} sen(2\\pi f n/N) + \\frac{1}{2} sen(2\\pi f (n-1)/N)=\\] \\[=\\frac{1}{2}sen(2\\pi f n/N) + \\frac{1}{2}\\left[ sen(2\\pi f n/N)cos(2\\pi f/N) - cos(2\\pi f n/N)sen(2\\pi f/N)\\right]=\\] \\[=\\frac{1}{2}\\left[ 1+cos(2\\pi f/N)\\right]sen(2\\pi f n/N) - \\frac{1}{2} cos(2\\pi f n/N)sen(2\\pi f/N)=\\] \\[=A_1sen(2\\pi f n/N) - A_2cos(2\\pi f n/N)\\,,\\] donde \\[A_1=\\frac{1}{2}\\left[ 1+cos(2\\pi f/N)\\right]\\,\\,;\\,\\,\\,\\,\\,A_2=sen(2\\pi f/N)\\,.\\]\nPara bajas frecuencias, es decir, \\(f\\sim 0\\) se cumple que \\(A_1\\sim 1\\) y \\(A_2 \\sim 0\\) y entonces \\[y(n)\\sim sen(2\\pi f n/N)=s(n)\\] y las bajas frecuencias son prácticamente mantenidas. Por el contrario para altas frecuencias, es decir, \\(f\\sim N/2\\), \\(A_1\\sim 0\\), \\(A_2\\sim0\\), y entonces \\(y(n)\\sim0\\) y consecuentemente las altas frecuencias son prácticamente eliminadas.\nLa fórmula general para el promedio corrido es \\[y(n)=\\frac{1}{M}\\sum\\limits^{(M-1)/2}_{p=-(M-1)/2} s(n+p)\\,,\\] donde \\(y( )\\) es el valor de la serie filtrada y \\(s( )\\) es la serie original sin filtrar, \\(M\\) es el número de puntos usados en el promedio. Por ejemplo, en un promedio corrido de \\(5\\) puntos, el valor en el punto \\(30\\) será \\[y(30)=\\frac{s(28)+s(29)+s(30)+s(31)+s(32)}{5}\\,.\\]\nEs evidente que este tipo de filtro es un suavizado o filtro pasa bajo. Sin embargo, podemos recuperar facilmente la señal de alta frecuencia restando a la señal original la serie filtrada.\n\n\nFiltros generales coseno\nSupongamos un simple filtro simétrico obtenido como la convolución entre una función de pesos \\(r(t)\\) y la señal \\(x(t)\\)\n\\[y_n=\\sum\\limits^{\\infty}_{p=-\\infty} r_p x_{n-p}\\,\\,\\,\\,{ donde}\\,\\,\\,\\,r_p=r_{-p}\\,,\\]\nson pesos elegidos adecuadamente. El efecto de filtrado se observa mejor en el dominio frecuencial. Queremos calcular la transformada de Fourier de una serie temporal \\(f(t)\\), la cual ha sido desfasada un tiempo \\(\\Delta t = a\\):\n\\[f(t\\pm a)=\\int\\limits^{\\infty}_{-\\infty} F(\\omega)e^{i\\omega (t\\pm a)} d{\\omega}=\n\\int\\limits^{\\infty}_{-\\infty}  \\left[F(\\omega) e^{i \\omega t}\\right]e^{\\pm i \\omega a} d{\\omega}\\]\nDe esta expresión deducimos que la transformada de Fourier de una serie desfasada por un intervalo de tiempo \\(\\Delta t\\) es igual a la transformada de Fourier de la serie no desfasada multiplicada por un factor \\[e^{\\pm i\\omega \\Delta t}\\,.\\] Usando este resultado, la transformada de Fourier de \\(y_n\\) se puede escribir como \\[Y(\\omega)={\\cal F}[y_n]=\\sum\\limits^{\\infty}_{p=-\\infty} r_p e^{-i\\omega_p \\Delta t} X(\\omega)\\,,\\] donde \\(X(\\omega)\\) y \\(Y(\\omega)\\) son la transformada de Fourier de \\(y(t)\\) y \\(x(t)\\), y la función respuesta en el dominio frecuencial es\n\\[R(\\omega)=\\frac{Y(\\omega)}{X(\\omega)}=\\sum\\limits^{\\infty}_{p=-\\infty} r_p e^{-i\\omega_p \\Delta t}\\,.\\]\n{}Puesto que \\(r_p=r_{-p}\\) y\n\\[{ cos}(x)=\\frac{e^{ix} + e^{-ix}}{2}\\,,\\]\npodemos escribir la función respuesta del filtrado deseado como\n\\[R(\\omega)=\\sum\\limits^{\\infty}_{p=-\\infty} r_p e^{-i\\omega_p \\Delta t}=\nr_0 + \\sum\\limits^{\\infty}_{p=1} r_p e^{i\\omega_p \\Delta t}\n+ \\sum\\limits^{\\infty}_{p=1} r_{-p} e^{-i\\omega_p \\Delta t} =\nr_0 + \\sum\\limits^{\\infty}_{p=1} r_p \\left[ e^{i\\omega_p \\Delta t} + e^{-i\\omega_p \\Delta t}\\right]=\n\\] \\[=r_0 + 2\\sum\\limits^{\\infty}_{p=1} r_p \\left[ \\frac{e^{i\\omega_p \\Delta t} + e^{-i\\omega_p t}}{2}\\right]=\nr_0 + 2\\sum\\limits^{\\infty}_{p=1}r_p cos(\\omega_p \\Delta t)\\]\nEn general los pesos \\(r_p\\) se van a calcular utilizando la siguiente expresión: \\[r_p=\\frac{1}{\\omega_N}\\int\\limits^{\\omega_N}_0 R(\\omega_p) cos(\\omega_p \\Delta t\n)d{\\omega}\\]\nPor ejemplo para un filtro pasa bajo \\(R(\\omega)=1\\) para \\(0&lt;|\\omega_p|\\le \\omega_c\\) y la integral para calcular los pesos queda \\[r_p=\\frac{1}{\\omega_N}\\int\\limits^{\\omega_c}_0 R(\\omega_p) cos(\\omega_p \\Delta t\n)d{\\omega}=\\frac{\\omega_c}{\\omega_N}\\frac{sen(\\omega_c p \\Delta t)}{\\omega_c p \\Delta t}=\n      \\frac{1}{\\omega_N}\\frac{sen(\\pi p \\omega_c / \\omega_N)}{\\pi p / \\omega_N}=\\] \\[=\\frac{sen(\\pi p \\omega_c / \\omega_N)}{\\pi p}\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,p=1,2,...,N\\]\nPara \\(p=0\\) entonces\n\\[r_0=\\frac{1}{\\omega_N}\\int\\limits^{\\omega_c}_0 R(\\omega_p) cos(\\omega_p \\Delta t\n)d{\\omega}=\\frac{1}{\\omega_N}\\int\\limits^{\\omega_c}_0 d{\\omega}=\\frac{\\omega_c}{\\omega_N}\\,.\\]\nY la función respuesta es\n\\[R(\\omega)=\\frac{\\omega_c}{\\omega_N} + 2\\sum\\limits^{\\infty}_{p=1}\n\\frac{sen(\\pi p \\omega_c / \\omega_N)}{\\pi p}cos(\\pi p \\omega / \\omega_N)\\]\nVeamos que forma tiene este filtro en el dominio frecuencial, asumiendo un número finito de \\(N\\) coeficientes de Fourier, i.e., \\(p=1,2,...,N\\), frecuencia de Nyquist \\(f_N=1\\) y frecuencia de corte \\(f_c=1\\):\nEn la figura observamos oscilaciones con longitud de onda \\[\\lambda=\\frac{4f_N}{2N+1}\\,,\\]\nEsta longitud de onda coincide con el ancho de banda de transición del filtro, es decir, del pico hasta la base indicado en la figura por las líneas rojas. Para filtrar únicamente debemos:  \n\nmultiplicar la respuesta espectral \\(R(f)\\) por la transformada de Fourier de la señal y regresar con la transformada inversa\n\n\\[x(t)[filtrado]={\\cal F}^{-1}[R(f)X(f)]\\,,\\]\n(ii) convolucionar la respuesta en el dominio temporal \\(r(t)\\) por la serie temporal. \\[x(t)[{ filtrado}]=[r(t)*x](t)\\,.\\]\n\n{}Si queremos un filtro pasa alta, usamos \\(r_p({ pasa}\\,\\,\\,{ alto})=1-r_p\\). Y la función respuesta sería \\[R(\\omega)[{ pasa}\\,\\,\\,{ alto}]=1-R(\\omega)=1-\\frac{\\omega_c}{\\omega_N} - 2\\sum\\limits^{\\infty}_{p=1}\n\\frac{sen(\\pi p \\omega_c / \\omega_N)}{\\pi p}cos(\\pi p \\omega / \\omega_N)\\]\nVeamos ahora de nuevo el promedio corrido pero esta vez usando el método de Fourier. De nuevo decir que el promedio corrido reemplaza el valor central de la ventana por el promedio de los valores que rodean a ese punto. Para este ejemplo los pesos son siempre iguales \\(r_p=1/T\\) para el intervalo \\(-N&lt;p&lt;N\\), donde \\(T=1/(2N+1)\\) es el tamaño de la ventana `boxcar’. De esta forma  \n\\[T=2N+1=3\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\nR(\\omega)=\\frac{1}{3} + \\frac{2}{3}cos(\\omega \\Delta t)\n\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,0&lt;\\omega&lt;\\pi/\\Delta t\\]\n\\[T=2N+1=5\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\nR(\\omega)=\\frac{1}{5} + \\frac{2}{5}cos(\\omega \\Delta t)+ \\frac{2}{5}cos(2\\omega \\Delta t)\n\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,0&lt;\\omega&lt;\\pi/\\Delta t\\]\n\\[T=2N+1=7\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\nR(\\omega)=\\frac{1}{7} + \\frac{2}{7}cos(\\omega \\Delta t)+ \\frac{2}{7}cos(2\\omega \\Delta t)+ \\frac{2}{7}cos(3\\omega \\Delta t)\n\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,0&lt;\\omega&lt;\\pi/\\Delta t\\]\nComo volvemos a observar, las transformadas de Fourier de funciones de peso cuadradas (`boxcar’) no son adecuadas debido a las oscilaciones o lóbulos menores.\n\n\nFiltro Lanczos pasabaja\nSea \\(R(f)\\) la función respuesta de un filtro pasa baja, en donde \\(f\\) corresponde a la frecuencia en ciclos por unidad de tiempo, \\(f_N\\) es la frecuencia de Nyquist, y \\(fc\\) es la frecuencia de corte.\nLanzcos se dio cuenta que las oscilaciones de los filtros cosinusoidales con longitud de onda \\(\\lambda(f)=4f_N/2N+1\\) podían ser reducidas si se realiza un suavizado de la fucnión respuesta \\(H(f)\\). Para ello realizó un promedio corrido de tamaño igual a la longitud de onda de las oscilaciones, es decir, \\(\\lambda\\). Esto se puede escribir como \\[\\widetilde{R}(f)=\\frac{1}{\\lambda(f)}\\int\\limits^{f+\\lambda/2}_{f-\\lambda/2} R(f) d{f}\\,,\\] donde ya hemos visto que \\[R(f)=\\frac{f_c}{f_N} + 2\\sum\\limits^{N}_{p=1}\nr_p cos(\\pi p f / f_N)\\,.\\]\nUn filtro de media corrida no tiene efecto en el promedio, entonces \\[\\widetilde{R}(f)=\\frac{f_c}{f_N} + \\frac{1}{\\lambda}\n\\int\\limits^{f+\\lambda/2}_{f-\\lambda/2} 2 \\sum\\limits^{N}_{p=1}\nr_pcos(\\pi p f / f_N)d{f}=\\]\n\\[=\\frac{f_c}{f_N} + \\frac{2}{\\lambda}\\sum\\limits^{N}_{p=1} r_p\n\\left[\\frac{1}{\\pi p /f_N} sen\\left( \\frac{\\pi p\nf}{f_N}\\right)\\right]^{f+\\lambda/2}_{f-\\lambda/2}=\\]\n\\[=\\frac{f_c}{f_N}+ \\frac{2}{\\lambda}\\sum\\limits^{N}_{p=1}\nr_p\\frac{f_N}{\\pi p}\\left[sen\\left( \\frac{\\pi p\n(f+\\lambda/2)}{f_N}\\right)-sen\\left( \\frac{\\pi p\n(f-\\lambda/2)}{f_N}\\right) \\right]=\\]\n\\[=\\frac{f_c}{f_N}+ \\frac{2}{\\lambda}\\sum\\limits^{N}_{p=1}\nr_p\\frac{f_N}{\\pi p}\\left[2cos\\left( \\frac{\\pi p\nf}{f_N}\\right)sen\\left( \\frac{\\lambda \\pi p}{2f_N}\\right) \\right]=\n\\frac{f_c}{f_N}+ \\frac{2}{\\lambda}\\sum\\limits^{N}_{p=1}\nr_p\\frac{f_N}{\\pi p}\\left[2cos\\left( \\frac{\\pi p\nf}{f_N}\\right)sen\\left( \\frac{2 \\pi p}{2N+1}\\right) \\right]=\\]\n\\[=\\frac{f_c}{f_N}+ \\frac{2(2N+1)}{4f_N}\\sum\\limits^{N}_{p=1}\nr_p\\frac{f_N}{\\pi p}\\left[2cos\\left( \\frac{\\pi p\nf}{f_N}\\right)sen\\left( \\frac{2 \\pi p}{2N+1}\\right) \\right]=\\]\n\\[=\\frac{f_c}{f_N}+ 2\\sum\\limits^{N}_{p=1}\nr_p\\frac{2N+1}{2 \\pi p}\\left[cos\\left( \\frac{\\pi p\nf}{f_N}\\right)sen\\left( \\frac{2 \\pi p}{2N+1}\\right) \\right]=\\]\n\\[=\\frac{f_c}{f_N}+ 2\\sum\\limits^{N}_{p=1}\nr_p\\frac{1}{\\frac{2 \\pi p}{2N+1}}\\left[cos\\left( \\frac{\\pi p\nf}{f_N}\\right)sen\\left( \\frac{2 \\pi p}{2N+1}\\right) \\right]=\n\\frac{f_c}{f_N}+ 2\\sum\\limits^{N}_{p=1}\nr_p \\sigma_p cos\\left( \\frac{\\pi p\nf}{f_N}\\right)\\,,\\]\ndonde\n\\[\\sigma_p=\\frac{sen\\left( \\frac{2 \\pi p}{2N+1}\\right)}{\\frac{2 \\pi p}{2N+1}}= sinc\\left(\\frac{2 \\pi p}{2N+1} \\right)\\,,\\]\nes una función sinc como la respuesta espectral de un filtro rectangular o boxcar. A este factor de suavizado se le suele llamar peso sigma.\nEn la figura se observa que la frecuencia de corte es la la frecuencia que corta el 50% de la magnitud de la respuesta. La frecuencia efectiva es \\(f=f_c+\\lambda/2\\).\n\n\nFiltro Lanczos pasabanda\nEn el dominio de las frecuencias, el filtro de pasa-banda se obtiene convolucionando el filtro pasa-bajas con la transformada de Fourier de la función coseno: \\[\\widetilde{R}_b(f)=\\widetilde{R}(f)*\\left[\\delta(f-f_o) + \\delta(f+f_0) \\right]\\,,\\] donde * significa convolución, \\[{\\cal F}[cos(2\\pi f_0 x)](f)=\\int\\limits^{\\infty}_{-\\infty} e^{-2\\pi i f x} cos(2\\pi\nf_0 x) dx=\\int\\limits^{\\infty}_{-\\infty} e^{-2\\pi i f x}\n\\left(\\frac{e^{2\\pi i f_0 x} +e^{-2\\pi i f_0 x}}{2}\\right) dx = \\] \\[=\\frac{1}{2}\\int\\limits^{\\infty}_{-\\infty}\\left[e^{-2\\pi i (f-f_0) x} + e^{-2\\pi i (f+f_0) x} \\right] dx\\] \\[=\\frac{1}{2}\\left[ \\delta(f-f_0) + \\delta(f+f_0)\\right]\\,,\\] y la delta de dirac se define como \\[\\delta(x)={\\cal F}[1](f\\pm f_0)=\\int\\limits^{\\infty}_{-\\infty} e^{-2\\pi i (f\\pm f_0) x} dx\\]\nVemos que la transformada de Fourier del coseno se ha multiplicado por un factor de 2 para que la respuesta del filtro sea unitaria (normalización). El resultado de la convolución es\n\\[\\widetilde{R}_b(f)=\\widetilde{R}(f)*\\left[\\delta(f-f_o) + \\delta(f+f_0) \\right]=\n\\widetilde{R}(f-f_0)+\\widetilde{R}(f+f_0)=\\]\n\\[=\\frac{f_c}{f_N}+ 2\\sum\\limits^{N}_{p=1}\n{r_p} \\sigma_p cos\\left( \\frac{\\pi p (f-f_0)}{f_N}\\right) +\n\\frac{f_c}{f_N}+ 2\\sum\\limits^{N}_{p=1}\n{r_p} \\sigma_p cos\\left( \\frac{\\pi p (f+f_0)}{f_N}\\right)=\\]\n\\[=2\\frac{f_c}{f_N}+ 2\\sum\\limits^{N}_{p=1}\n{r_p} \\sigma_p \\left[ cos\\left( \\frac{\\pi p f}{f_N} - \\frac{\\pi p f_0}{f_N}\\right) +\ncos\\left( \\frac{\\pi p f}{f_N} + \\frac{\\pi p f_0}{f_N}\\right)\\right]=\\]\n\\[=2\\frac{f_c}{f_N}+ 2\\sum\\limits^{N}_{p=1} r_p \\sigma_p\\left[\n2cos\\left( \\frac{\\pi p f}{f_N}\\right) cos\\left( \\frac{\\pi p f_0}{f_N}\\right)\\right]=\\]\n\\[=2\\frac{f_c}{f_N}+ 4\\sum\\limits^{N}_{p=1} r_p \\sigma_p\\left[\ncos\\left( \\frac{\\pi p f}{f_N}\\right) cos\\left( \\frac{\\pi p f_0}{f_N}\\right)\\right]\\]\nPara el filtro pasa bajas: \\[x(t)[{ filtrado}]=[r_p \\sigma_p * x](t)\\,,\\] o \\[x(t)[{ filtrado}]={\\cal F}^{-1}[\\widetilde{R}(f)X(f)]\\,.\\]\nPara el filtro pasa-banda: \\[x(t)[{ filtrado}]=[r_p \\sigma_p cos\\left( \\frac{\\pi p f_0}{f_N}\\right)*x](t)\\,,\\] o \\[x(t)[{ filtrado}]={\\cal F}^{-1}[\\widetilde{R}_b(f)X(f)]\\,.\\]\nTodo esto se puede programar facilmente en Matlab como se muestra en el siguiente:\n% OUTPUT\\\nY una vez tenemos los coeficientes solamente tenemos que convolucionar o multiplicar:\nAqui se muestra un ejemplo sintético del uso de las subrutinas presentadas."
  },
  {
    "objectID": "chap4.html#temas-selectos",
    "href": "chap4.html#temas-selectos",
    "title": "Notas del Curso Análisis de Datos",
    "section": "Temas selectos",
    "text": "Temas selectos\n\nAnálisis Armónico\nSe trata de un ajuste por mínimos cuadrados de una serie temporal dominada por armónicos específicos. Por ejemplo, en el caso del océano, es muy comúnencontrar en series temporales de temperatura, salinidad, velocidad, etc.. señales de las mareas que no son nada mas que corrientes periódicas generadas por fuerzas astronómicas con una frecuencia de oscilación determinada (24h, 12h, etc…).\nEl método consiste en elejir las frecuencias de los armónicos y usar cuadrados mínimos para ajustarlos a la serie temporal. Supongamos \\(M\\) armónicos a ajustar\n\\[y(t_n) = \\overline{y(t)} + \\sum\\limits^{M}_{q=1}C_q cos(\\omega_q t_n -\\theta_q)+y_r(t_n)\\,,\\]\ndonde \\(\\overline{y(t)}\\) es el promedio de la serie, y \\(y_r(t_n)\\) es el residuo de la serie temporal (donde hay el resto de armónicos presentes en la serie), \\(\\omega_q=2\\pi q/N\\Delta t\\). En términos de las amplitudes \\(A_q\\) y \\(B_q\\)\n\\[y(t_n) = \\overline{f(t)} + \\sum\\limits^{M}_{q=1}[A_q cos(\\omega_q t_n)+B_q sen(\\omega_q t_n)] +y_r(t_n)\\,,\\] donde \\[C_q=\\sqrt{A_q^2+B_q^2}\\,,\\,\\,\\,\\,q=1,2,....\\] \\[\\theta_q=arctg[B_q/A_q]\\,,\\,\\,\\,\\,q=1,2,...\\]\nAntes de empezar el análisis debemos de extraer la media, \\(\\overline{y}\\), a la serie temporal. El método de mínimos cuadrados consiste en minimizar la suma de los errores cuadrados \\(SEC\\), es decir, \\[SEC= \\sum\\limits^N_{n=1} y_r^2(t_n) = \\sum\\limits^N_{n=1}  \\left( y(t_n) - \\left[ \\overline{y(t)} +\n\\sum\\limits^{M}_{q=1} A_q cos(\\omega_q t_n)+B_q sen(\\omega_q t_n) \\right] \\right)^2 =\\] \\[=\\sum\\limits^N_{n=1}  \\left(y(t_n) - \\left[ \\overline{y(t)} +\n\\sum\\limits^{M}_{q=1} A_q cos(2\\pi q n/ N)+B_q sen(2\\pi q n/ N) \\right] \\right)^2\\]\nComo siempre derivamos respecto los coeficientes e igualamos a cero para obtener un sistema de \\(2M+1\\) equaciones \\[\\frac{\\partial{SEC}}{\\partial{A_q}}=0=2\\sum\\limits^N_{n=1} \\left(y(t_n) - \\left[ \\overline{y(t)} +\n\\sum\\limits^{M}_{q=1} A_q cos(2\\pi q n/ N)+B_q sen(2\\pi q n/ N) \\right]-cos(2\\pi q n/ N) \\right)\\] \\[\\frac{\\partial{SEC}}{\\partial{B_q}}=0=2\\sum\\limits^N_{n=1} \\left(y(t_n) - \\left[ \\overline{y(t)} +\n\\sum\\limits^{M}_{q=1} A_q cos(2\\pi q n/ N)+B_q sen(2\\pi q n/ N) \\right]-sen(2\\pi q n/ N) \\right)\\]\nSoluciones del sistema requiere una equación matricial de la forma \\({\\textbf D}{\\textbf z}={\\textbf y}\\), donde \\[{\\textbf D}=\\left( \\begin{array}{cccccccccccccc}\n  N & c_1 & c_2 & ... & c_M & s_1 & s_2 & ... & s_M \\\\\n  c_1 & cc_{11} & cc_{12} & ... & cc_{1M} & cs_{11} & cs_{12} & ... & cs_{1M} \\\\\n  c_2 & cc_{21} & cc_{22} & ... & cc_{2M} & cs_{21} & cs_{22} & ... & cs_{2M} \\\\\n  ... & ... & ... & ... & ... & ... & ... & ... & ... \\\\\n  ... & ... & ... & ... & ... & ... & ... & ... & ... \\\\\n  c_M & cc_{M1} & cc_{M2} & ... & cc_{MM} & cs_{M1} & cs_{M2} & ... & cs_{MM} \\\\\n  s_1 & sc_{11} & sc_{12} & ... & sc_{1M} & ss_{11} & ss_{12} & ... & ss_{1M} \\\\\n  s_2 & sc_{21} & sc_{22} & ... & sc_{2M} & ss_{21} & ss_{22} & ... & ss_{2M} \\\\\n  ... & ... & ... & ... & ... & ... & ... & ... & ... \\\\\n  ... & ... & ... & ... & ... & ... & ... & ... & ... \\\\\n  s_M & sc_{M1} & sc_{M2} & ... & sc_{MM} & ss_{M1} & ss_{M2} & ... & ss_{MM} \\\\\n      \\end{array} \\right)\\] y\n\\[{\\textbf y}=\\left( \\begin{array}{ccc}\nyc_0 \\\\\nyc_1 \\\\\nyc_2 \\\\\n... \\\\\n... \\\\\nyc_M \\\\\nys_1 \\\\\nys_2 \\\\\n... \\\\\n... \\\\\nys_M \\\\\n\\end{array} \\right)\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\n{\\textbf z}=\\left( \\begin{array}{ccc}\nA_0\\\\\nA_1\\\\\nA_2\\\\\n...\\\\\n...\\\\\nA_M\\\\\nB_1\\\\\nB_2\\\\\n...\\\\\n...\\\\\nB_M\n\\end{array} \\right)\\]\nLos coeficientes de las matrices son:\n\\[yc_i=\\sum\\limits^N_{n=1}y(t_n)cos(\\omega_i t_n)\\,\\,\\,\\,\\,,\\,\\,\\,\\,\\,ys_i=\\sum\\limits^N_{n=1}y(t_n)sen(\\omega_i t_n)\\] \\[c_i=\\sum\\limits^N_{n=1}cos(\\omega_i t_n)\\,\\,\\,\\,\\,,\\,\\,\\,\\,\\,s_i=\\sum\\limits^N_{n=1}sen(\\omega_i t_n)\\] \\[cc_{ij}=cc_{ji}=\\sum\\limits^N_{n=1}[cos(\\omega_i t_n)cos(\\omega_j t_n)]\\] \\[ss_{ij}=ss_{ji}=\\sum\\limits^N_{n=1}[sen(\\omega_i t_n)sen(\\omega_j t_n)]\\] \\[cs_{ij}=sc_{ji}=\\sum\\limits^N_{n=1}[cos(\\omega_i t_n)sen(\\omega_j t_n)]\\,,\\] donde \\(t_n= n \\Delta\\), \\(\\omega_i = 2\\pi f_i\\) es la frecuencia angular de las componentes de interés \\(i\\), y \\(\\phi_i(n)=\\omega_i t_n\\) es el argumento de las funciones de Fourier.\n{Ejemplo de Ajuste de armónicos (Emery and Thompson, p395)}\nAsumamos la siguiente serie temporal de temperatura promedia mensual. \\\n\\ \\\nDeseamos encontrar las componentes mareales dominantes en la serie temporal de temperatura. A simple vista podemos ver que existe una frecuencia dominante semianual. Por tanto, vamos buscar las amplitudes y frecuencias de interés, es decir, de las componentes anual y semianual que tienen unas frecuencias de \\(f_1=1/12\\) meses (\\(=0.0833\\,{ cpm}\\)) y \\(f_2=1/24\\) meses (\\(=0.1667\\,{ cpm}\\)). Los argumentos de las funciones de Fourier son \\(\\phi_1(n)=\\omega_1 t_n=2\\pi(1/12)*n*\\Delta t=(\\pi/6)*n*1=n\\pi/6\\) y \\(\\phi_2(n)=\\omega_2 t_n=2\\pi(1/6)*n*\\Delta t=(\\pi/3)*n*1=n\\pi/3\\) Para este problema las matrices son \\\n\\[{\\textbf D}=\\left( \\begin{array}{ccccc}\n  N & c_1 & c_2 & s_1 & s_2 \\\\\n  c_1 & cc_{11} & cc_{12} & cs_{11} & cs_{12}\\\\\n  c_2 & cc_{21} & cc_{22} & cs_{21} & cs_{22} \\\\\n  s_1 & sc_{11} & sc_{12} & ss_{11} & ss_{12} \\\\\n  s_2 & sc_{21} & sc_{22} & ss_{21} & ss_{22} \\\\\n      \\end{array} \\right)=\\] \\[=\\scriptsize\n\\left( \\begin{array}{ccccc}\nN & c_1 & c_2 & s_1 & s_2 \\\\\n  c_1 & \\sum\\limits^N_{n=1}[cos(\\phi_1(n))cos(\\phi_1(n))] & cc_{12} & cs_{11} & cs_{12}\\\\\n  c_2 & cc_{21} & \\sum\\limits^N_{n=1}[cos(\\phi_1(n))cos(\\phi_2(n))] & cs_{21} & cs_{22} \\\\\n  s_1 & sc_{11} & sc_{12} & \\sum\\limits^N_{n=1}[sen(\\phi_1(n))sen(\\phi_1(n))] & ss_{12} \\\\\n  s_2 & sc_{21} & sc_{22} & ss_{21} & \\sum\\limits^N_{n=1}[sen(\\phi_2(n))sen(\\phi_2(n))] \\\\\n      \\end{array} \\right) =\n      \\] \\[\n=\n\\left( \\begin{array}{ccccc}\n  24 & 0 & 0 & 0 & 0 \\\\\n  0 & 12 & 0 & 0 & 0\\\\\n  0 & 0 & 12 & 0 & 0 \\\\\n  0 & 0 & 0 & 12 & 0 \\\\\n  0 & 0 & 0 & 0 & 12 \\\\\n      \\end{array} \\right)\n\\]\nLa matriz \\[{\\textbf y}=\\left( \\begin{array}{ccc}\n\\sum\\limits^N_{n=1}y(t_n)cos(\\omega_0 t_n) \\\\\n\\sum\\limits^N_{n=1}y(t_n)cos(\\omega_1 t_n) \\\\\n\\sum\\limits^N_{n=1}y(t_n)cos(\\omega_2 t_n) \\\\\n\\sum\\limits^N_{n=1}y(t_n)sen(\\omega_1 t_n) \\\\\n\\sum\\limits^N_{n=1}y(t_n)sen(\\omega_2 t_n) \\\\\n\\end{array} \\right)=\n\\left( \\begin{array}{ccc}\n262.5 \\\\\n-21.45 \\\\\n-5.4 \\\\\n-23.76 \\\\\n-0.51 \\\\\n\\end{array} \\right)\\,\\,^\\circ{C}\\]\nFinalmente encontramos las amplitudes de los armónicos resolviendo el sistema \\[{\\textbf z}=({\\textbf D}^T{\\textbf D})^{-1}{\\textbf D}^{T}{\\textbf y}={\\textbf D}^{-1}{\\textbf y}=\\left( \\begin{array}{ccc}\n10.93\\\\\n-1.78\\\\\n-0.45\\\\\n-1.98\\\\\n-0.04\n\\end{array} \\right)\\]\nEl coeficiente de correlación entre la señal original y la serie de Fourier con 2 armónicos es \\(r^2=0.92\\), es decir, solamente con 2 armónicos podemos explicar el 92% de la varianza total.\n\n\nDemodulación compleja\nEste método es utilizado para conocer el comportamiento de una componente o armónico con frecuencia particular \\(\\omega\\), tal como la marea diurna, o semidiurna, o las ondas inerciales. Aqui vamos a mostrar la forma clásica de demodular que consiste en ajustar por fragmentos de la serie un armónico teórico utilizando mínimos cuadrados. Cada fragmento de la serie debe, como mínimo, contener un ciclo del armónico a demodular. Para cada segmento, la anomalía de la componente de velocidad a la frecuencia de interés \\(\\omega\\) es \\[{\\textbf u} - \\overline{\\textbf u}=[u(t)-\\overline{u(t)} +iv(t)-\\overline{v(t)}]=\\] \\[R^+ e^{i(\\omega t + \\epsilon^+)} + R^- e^{-i(\\omega t + \\epsilon^-)}\\,,\\] donde \\(\\overline{u(t)}\\), \\(\\overline{v(t)}\\) son las componentes de la velocidad promedio, \\(R^+,\\,\\,R^-\\) y \\(\\epsilon^+\\,\\,\\epsilon^-\\) son las amplitudes y fases de las componentes rotatorias que giran en el sentido de las agujas del reloj (+) y en el sentido contrario (-). La serie temporal esta definida para cada \\(t_k\\,(k=1,2,....,N)\\) y las soluciones son encontradas resolviendo el sistema de ecuaciones \\[{\\textbf z}={\\textbf D}^{-1}{\\textbf y}\\,,\\] donde \\[{\\textbf y}=\\left( \\begin{array}{c}\nu(t_1) \\\\\nu(t_2) \\\\\n... \\\\\nu(t_n) \\\\\nv(t_1) \\\\\nv(t_2) \\\\\n... \\\\\nv(t_n) \\\\\n\\end{array} \\right)\\,;\\,\\,\\,\n{\\textbf z}=\\left( \\begin{array}{c}\nR^+cos(\\epsilon^+) \\\\\nR^+sen(\\epsilon^+)\\\\\nR^-cos(\\epsilon^-) \\\\\nR^-sen(\\epsilon^-)\\\\\n\\end{array} \\right)=\n\\left( \\begin{array}{c}\nACP \\\\\nASP\\\\\nACM \\\\\nASM\\\\\n\\end{array} \\right)\\,,\\] y la matriz \\({\\textbf D}\\) es \\[{\\textbf D}=\\left( \\begin{array}{cccc}\ncos(\\omega t_1) & -sen(\\omega t_1) & cos(\\omega t_1) & sen(\\omega t_1) \\\\\ncos(\\omega t_2) & -sen(\\omega t_2) & cos(\\omega t_2) & sen(\\omega t_2) \\\\\n... \\\\\ncos(\\omega t_n) & -sen(\\omega t_n) & cos(\\omega t_n) & sen(\\omega t_n) \\\\\nsen(\\omega t_1) & cos(\\omega t_1) & -sen(\\omega t_1) & cos(\\omega t_1) \\\\\nsen(\\omega t_2) & cos(\\omega t_2) & -sen(\\omega t_2) & cos(\\omega t_2) \\\\\n... \\\\\nsen(\\omega t_n) & cos(\\omega t_n) & -sen(\\omega t_n) & cos(\\omega t_n) \\\\\n\\end{array} \\right)\\,.\\]\nUna vez los valores de \\({\\textbf z}\\) son encontrados a partir de la solución de mínimos cuadrados de arriba, podemos encontrar los parámetros de la elipse como: \\[R^+=\\sqrt{\\left( ASP^2 + ACP^2\\right)}\\,;\\,\\,\\,\\,\\,R^-=\\sqrt{\\left( ASM^2 + ACM^2\\right)}\\] \\[\\epsilon^+=tan^{-1}\\left(\\frac{ASP}{ACP}\\right)\\,;\\,\\,\\,\\,\\,\\epsilon^-=tan^{-1}\\left(\\frac{ASM}{ACM}\\right)\\]\nPor ejemplo, si queremos demodular la amplitud y fase de las ondas inerciales observadas en un anclaje situado en latitudes medias, debemos de usar una frecuencia \\(\\omega=2\\Omega sen\\phi\\) y ajustar por mínimos cuadrados segmentos de \\(24\\,{ h}\\) sin superposición. La serie temporal medida por el anclaje debería de ser horaria para que existan mas datos por segmento que parámetros a ajustar.\nOtra forma, tal vez mas sencilla, es la siguiente. Imaginemos que la serie original es \\(X(t)\\) y se asume como una señal periódica con frecuencia igual a la de interés mas otras cosas que llamamos \\(Z(t)\\) \\[X(t)=A(t)cos\\left(\\omega t + \\varphi(t)\\right) + Z(t)= \\frac{1}{2}A(t)\n       \\left[e^{i(\\omega t + \\varphi(t))} + e^{-i(\\omega t + \\varphi(t))} \\right] + Z(t)\\,,\\] donde la amplitud \\(A(t)\\) y la fase \\(\\varphi(t)\\) de la señal periódica se asumen que dependen del tiempo pero que varían “lentamente” en comparación a la frecuencia \\(\\omega\\).\nPara demodular tenemos que: \\\n\nMultiplicar \\(X(t)\\) por \\(e^{-i\\omega t}\\): \\[Y(t)=X(t)e^{-i\\omega t}=\\frac{1}{2}A(t)\n   \\left[e^{i(\\omega t + \\varphi(t))} + e^{-i(\\omega t + \\varphi(t))} \\right]e^{-i\\omega t} + Z(t)e^{-i\\omega t}=\\] \\[=\\frac{1}{2}A(t)e^{i(\\omega t + \\varphi(t))}e^{-i\\omega t} + \\frac{1}{2}A(t)e^{-i(\\omega t + \\varphi(t))}e^{-i\\omega t} +\nZ(t)e^{-i\\omega t}=\\] \\[=\\underbrace{\\frac{1}{2}A(t)e^{i\\varphi(t)}}_{(a)} + \\underbrace{\\frac{1}{2}A(t)e^{-i(2\\omega t + \\varphi(t))}}_{(b)} +\n\\underbrace{Z(t)e^{-i\\omega t}}_{(c)}\\,.\\]\n\nEl término (a) varía lentamente ya que \\(\\varphi(t)\\) también lo hace y no tiene energía (potencia espectral) a la frecuencia de demodulación \\(\\omega\\) o arriba de ella. El término (b) oscila a dos veces la frecuencia de demodulación, i.e., \\(2\\omega\\). El término (c) varía a la frecuencia \\(\\omega\\). Debido a que \\(Z(t)\\) no tiene energía a la frecuencia \\(\\omega\\), entonces el término (c) no tendrá tampoco energía en la frecuencia cero, i.e., \\(\\omega=0\\).\\\n\nFiltro pasa-bajas de la serie \\(Y(t)\\) para eliminar las ondas con frecuencia \\(\\omega\\) o por encima de \\(\\omega\\). Esto eliminará prácticamente los términos (b) y (c), y suavizará (a). El resultado es \\[Y_s(t)=\\frac{1}{2}A_s(t)e^{i\\varphi_s(t)}\\,,\\] donde el subíndice \\(_s\\) significa suavizado o filtro pasa-bajas. \\\nExtraer \\(A_s(t)\\) y \\(\\varphi_s(t)\\): \\[\\frac{1}{2}A_s(t)=|Y_s(t)|=2\\left( Re[Y_s]^2 + Im[Y_s]^2\\right)^{1/2}\\] \\[e^{i\\varphi(t)}=2\\frac{Y_s(t)}{A_s(t)}\\,;\\,\\,\\,\\,\\,\\varphi_s(t)=tan^{-1}\\left(\\frac{Im[Y_s]}{Re[Y_s]}\\right)\\]\n\nAl suavizar hacemos dos cosas. Primero, eliminamos los términos no deseados (a) y (b). El tipo de filtrado o suavizado determina la anchura de la banda de frecuencias de las oscilaciones retenidas. Por ejemplo, si usamos un triángulo (ventana triangular) de longitud \\(2T-1\\) donde \\(T=2\\pi/\\omega\\) es el periodo de demodulación, entonces para la banda para la potencia-media (\\(3\\,{ dB}\\) desde el pico) será \\(\\omega \\in [T/(1+0.44295),T/(1-0.44295)]\\). Potencia media se refiere a la frecuencia a la cual la potencia se ha reducido a la mitad de su valor medio de la banda. Segundo, el filtrado suaviza las series de amplitud y la fase. \\\n\nLa elección de la frecuencia de demodulación \\(\\omega\\) se puede validar ajustando localmente una línea a la fase,\\(\\varphi\\simeq a + bt\\). Típicamente esto lo haremos en fragmentos de longiutd \\(T\\). De esta forma si seleccionamos el origen en el tiempo central de cada fragmento (tal que \\(a\\simeq 0\\)) obtenemos que \\(cos(\\omega t + \\varphi)\\simeq cos(\\omega t + bt)=cos(\\hat{\\omega} t)\\).\n\nLa frecuencia ajustada \\(\\hat{\\omega}=\\omega + a\\) es una validación de la elección inicial de nuestra frecuencia de demodulación \\(\\omega\\)."
  },
  {
    "objectID": "chap4.html#acknowledgments",
    "href": "chap4.html#acknowledgments",
    "title": "Notas del Curso Análisis de Datos",
    "section": "Acknowledgments",
    "text": "Acknowledgments\nAyuda de Muchos"
  },
  {
    "objectID": "chap4.html#open-research",
    "href": "chap4.html#open-research",
    "title": "Notas del Curso Análisis de Datos",
    "section": "Open research",
    "text": "Open research\nDisponible para todos"
  },
  {
    "objectID": "chap4.html#references",
    "href": "chap4.html#references",
    "title": "Notas del Curso Análisis de Datos",
    "section": "References",
    "text": "References"
  }
]